<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><front><journal-meta><journal-id journal-id-type="nlm-ta">Anat Res Int</journal-id><journal-id journal-id-type="iso-abbrev">Anat Res Int</journal-id><journal-id journal-id-type="publisher-id">ARI</journal-id><journal-title-group><journal-title>Anatomy Research International</journal-title></journal-title-group><issn pub-type="ppub">2090-2743</issn><issn pub-type="epub">2090-2751</issn><publisher><publisher-name>Hindawi</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28656109</article-id><article-id pub-id-type="pmc">5471569</article-id><article-id pub-id-type="doi">10.1155/2017/1493135</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Stereopsis, Visuospatial Ability, and Virtual Reality in Anatomy Learning</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0897-585X</contrib-id><name><surname>Luursema</surname><given-names>Jan-Maarten</given-names></name><xref ref-type="aff" rid="I1"></xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Vorstenbosch</surname><given-names>Marc</given-names></name><xref ref-type="aff" rid="I1"></xref></contrib><contrib contrib-type="author"><name><surname>Kooloos</surname><given-names>Jan</given-names></name><xref ref-type="aff" rid="I1"></xref></contrib></contrib-group><aff id="I1">Department of Anatomy, Radboud University Medical Center, 6525 GA Nijmegen, Netherlands</aff><author-notes><corresp id="cor1">*Jan-Maarten Luursema: <email>jan-maarten.luursema@radboudumc.nl</email></corresp><fn fn-type="other"><p>Academic Editor: Udo Schumacher</p></fn></author-notes><pub-date pub-type="ppub"><year>2017</year></pub-date><pub-date pub-type="epub"><day>1</day><month>6</month><year>2017</year></pub-date><volume>2017</volume><elocation-id>1493135</elocation-id><history><date date-type="received"><day>16</day><month>12</month><year>2016</year></date><date date-type="rev-recd"><day>9</day><month>3</month><year>2017</year></date><date date-type="accepted"><day>30</day><month>4</month><year>2017</year></date></history><permissions><copyright-statement>Copyright © 2017 Jan-Maarten Luursema et al.</copyright-statement><copyright-year>2017</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p><offsets xml_i="2914" xml_f="3777" txt_i="11" txt_f="874">A new wave of virtual reality headsets has become available. A potential benefit for the study of human anatomy is the reintroduction of stereopsis and absolute size. We report a randomized controlled trial to assess the contribution of stereopsis to anatomy learning, for students of different visuospatial ability. Sixty-three participants engaged in a one-hour session including a study phase and posttest. One group studied 3D models of the anatomy of the deep neck in full stereoptic virtual reality; one group studied those structures in virtual reality without stereoptic depth. The control group experienced an unrelated virtual reality environment. A post hoc questionnaire explored cognitive load and problem solving strategies of the participants. We found no effect of condition on learning. Visuospatial ability however did impact correct answers at </offsets><italic><offsets xml_i="3785" xml_f="3786" txt_i="874" txt_f="875">F</offsets></italic><offsets xml_i="3795" xml_f="3810" txt_i="875" txt_f="890">(1) = 5.63 and </offsets><italic><offsets xml_i="3818" xml_f="3819" txt_i="890" txt_f="891">p</offsets></italic><offsets xml_i="3828" xml_f="4231" txt_i="891" txt_f="1294"> = .02. No evidence was found for an impact of cognitive load on performance. Possibly, participants were able to solve the posttest items based on visuospatial information contained in the test items themselves. Additionally, the virtual anatomy may have been complex enough to discourage memory based strategies. It is important to control the amount of visuospatial information present in test items.</offsets></p></abstract></article-meta></front><body><sec id="sec1"><title><offsets xml_i="4297" xml_f="4312" txt_i="1302" txt_f="1317">1. Introduction</offsets></title><sec id="sec1.1"><title><offsets xml_i="4344" xml_f="4385" txt_i="1318" txt_f="1359">1.1. Virtual Reality for Anatomy Learning</offsets></title><p><offsets xml_i="4396" xml_f="4709" txt_i="1360" txt_f="1673">Virtual reality (VR) aims to provide its users with an illusory environment by completely replacing direct sensory stimuli for artificially generated (or mediated) sensory stimuli. Complete immersion in a virtual environment would enable interaction with virtual objects similar to interaction with real objects [</offsets><xref rid="B27" ref-type="bibr"><offsets xml_i="4741" xml_f="4742" txt_i="1673" txt_f="1674">1</offsets></xref><offsets xml_i="4749" xml_f="5075" txt_i="1674" txt_f="2000">]. In addition to this, VR allows the development of interaction modalities for which no real-world analog exists. For the study of human anatomy, this promises extraordinary versatility and flexibility in the presentation and exploration of anatomical objects, at a fraction of the cost of maintaining dissection facilities [</offsets><xref rid="B31" ref-type="bibr"><offsets xml_i="5107" xml_f="5108" txt_i="2000" txt_f="2001">2</offsets></xref><offsets xml_i="5115" xml_f="5116" txt_i="2001" txt_f="2002">–</offsets><xref rid="B28" ref-type="bibr"><offsets xml_i="5148" xml_f="5149" txt_i="2002" txt_f="2003">4</offsets></xref><offsets xml_i="5156" xml_f="5421" txt_i="2003" txt_f="2268">]. However, full VR is technologically challenging and has not yet been implemented. The ongoing development of virtual reality is reflected in the lack of consensus in the literature as to the effectiveness of digital 3D representations for human anatomy learning.</offsets></p><p><offsets xml_i="5428" xml_f="5494" txt_i="2269" txt_f="2335">Older studies often report ambiguous or negative findings (e.g., [</offsets><xref rid="B7" ref-type="bibr"><offsets xml_i="5525" xml_f="5526" txt_i="2335" txt_f="2336">5</offsets></xref><offsets xml_i="5533" xml_f="5535" txt_i="2336" txt_f="2338">, </offsets><xref rid="B17" ref-type="bibr"><offsets xml_i="5567" xml_f="5568" txt_i="2338" txt_f="2339">6</offsets></xref><offsets xml_i="5575" xml_f="6288" txt_i="2339" txt_f="3052">]). These studies however do not utilize the full potential of virtual reality, using 3D computer models in a “desktop VR” setting. In desktop VR, sources of spatial information such as physical size and tactile/force feedback are lost, and there is no direct interaction or sense of sharing space with virtual anatomical objects. Navigating such an environment might add to the cognitive load of the participants experienced during those experiments, which in turn could have biased the results of those studies. This is corroborated by studies working with a more complete implementation of virtual reality principles, which report a positive contribution of virtual reality for anatomy learning (the study of [</offsets><xref rid="B31" ref-type="bibr"><offsets xml_i="6320" xml_f="6321" txt_i="3052" txt_f="3053">2</offsets></xref><offsets xml_i="6328" xml_f="6368" txt_i="3053" txt_f="3093">] provides a review and meta-analysis; [</offsets><xref rid="B20" ref-type="bibr"><offsets xml_i="6400" xml_f="6401" txt_i="3093" txt_f="3094">7</offsets></xref><offsets xml_i="6408" xml_f="6410" txt_i="3094" txt_f="3096">, </offsets><xref rid="B23" ref-type="bibr"><offsets xml_i="6442" xml_f="6443" txt_i="3096" txt_f="3097">8</offsets></xref><offsets xml_i="6450" xml_f="6576" txt_i="3097" txt_f="3223">]). Despite uncertainty as to its effectiveness, virtual human anatomy has become an integral part of the medical curriculum [</offsets><xref rid="B13" ref-type="bibr"><offsets xml_i="6608" xml_f="6609" txt_i="3223" txt_f="3224">9</offsets></xref><offsets xml_i="6616" xml_f="6618" txt_i="3224" txt_f="3226">].</offsets></p></sec><sec id="sec1.2"><title><offsets xml_i="6652" xml_f="6695" txt_i="3228" txt_f="3271">1.2. Stereopsis in Digital Anatomy Learning</offsets></title><p><offsets xml_i="6706" xml_f="6863" txt_i="3272" txt_f="3429">Besides the loss of tactile and force feedback, desktop VR anatomy comes with another limitation: the spatial information provided by the visual depth cue of</offsets><italic><offsets xml_i="6871" xml_f="6882" txt_i="3429" txt_f="3440"> stereopsis</offsets></italic><offsets xml_i="6891" xml_f="7208" txt_i="3440" txt_f="3757"> is also lost. Stereopsis refers to the experience of spatial depth based on the brain's comparison of synchronous and overlapping visual information provided by an organism's paired eyes. Stereopsis is one of the most important depth cues for estimating distance and size for objects in close proximity to oneself ([</offsets><xref rid="B2" ref-type="bibr"><offsets xml_i="7239" xml_f="7241" txt_i="3757" txt_f="3759">10</offsets></xref><offsets xml_i="7248" xml_f="7261" txt_i="3759" txt_f="3772">]; see also [</offsets><xref rid="B8" ref-type="bibr"><offsets xml_i="7292" xml_f="7294" txt_i="3772" txt_f="3774">11</offsets></xref><offsets xml_i="7301" xml_f="7433" txt_i="3774" txt_f="3906">]). The loss of spatial information caused by digital mediation makes it harder to interpret spatially complex objects or contexts [</offsets><xref rid="B25" ref-type="bibr"><offsets xml_i="7465" xml_f="7467" txt_i="3906" txt_f="3908">12</offsets></xref><offsets xml_i="7474" xml_f="7528" txt_i="3908" txt_f="3962">], especially for people of low visuospatial ability [</offsets><xref rid="B14" ref-type="bibr"><offsets xml_i="7560" xml_f="7562" txt_i="3962" txt_f="3964">13</offsets></xref><offsets xml_i="7569" xml_f="7688" txt_i="3964" txt_f="4083">]. Human anatomy is spatially complex, and people of high visuospatial ability have an advantage in anatomy learning ([</offsets><xref rid="B10" ref-type="bibr"><offsets xml_i="7720" xml_f="7722" txt_i="4083" txt_f="4085">14</offsets></xref><offsets xml_i="7729" xml_f="7750" txt_i="4085" txt_f="4106">] provides a review).</offsets></p><p><offsets xml_i="7757" xml_f="7870" txt_i="4107" txt_f="4220">The visual depth cue of stereopsis however can be implemented in digital learning environments. Luursema et al. [</offsets><xref rid="B20" ref-type="bibr"><offsets xml_i="7902" xml_f="7903" txt_i="4220" txt_f="4221">7</offsets></xref><offsets xml_i="7910" xml_f="8478" txt_i="4221" txt_f="4789">] found that learning in a digital environment which implemented both stereopsis and direct manipulation allowed students of low visuospatial ability to perform on par with their high visuospatial counterparts in visual tests of anatomical knowledge. So far, this only has been shown in experiments with psychology students who studied anatomical material of relatively low complexity. To provide ecological validity to these earlier results, we aim to verify the conclusions of these studies working with medical students and more complex digital anatomical material.</offsets></p></sec><sec id="sec1.3"><title><offsets xml_i="8512" xml_f="8554" txt_i="4791" txt_f="4833">1.3. A New Wave of Virtual Reality Devices</offsets></title><p><offsets xml_i="8565" xml_f="8815" txt_i="4834" txt_f="5084">One way to implement stereopsis digitally is to use a virtual reality setup. A new wave of affordable virtual reality head-mounted devices has come to market at the time of writing, including the Oculus Rift, Google cardboard, and Samsung's Gear VR (</offsets><ext-link ext-link-type="uri" xlink:href="http://www.wired.co.uk/magazine/archive/2015/07/features/vr-total-immersion"><offsets xml_i="8934" xml_f="9009" txt_i="5084" txt_f="5159">http://www.wired.co.uk/magazine/archive/2015/07/features/vr-total-immersion</offsets></ext-link><offsets xml_i="9020" xml_f="9150" txt_i="5159" txt_f="5289">). With these devices, a long standing interest in the use of virtual reality for anatomy learning has gained new momentum (e.g., </offsets><ext-link ext-link-type="uri" xlink:href="http://anatomy.stanford.edu/research---development.html"><offsets xml_i="9249" xml_f="9304" txt_i="5289" txt_f="5344">http://anatomy.stanford.edu/research---development.html</offsets></ext-link><offsets xml_i="9315" xml_f="9516" txt_i="5344" txt_f="5545">). Although immersion in a virtual reality environment can lead to a heightened sense of presence, object manipulation in such an environment is not intuitive and may add to the users' cognitive load [</offsets><xref rid="B18" ref-type="bibr"><offsets xml_i="9548" xml_f="9550" txt_i="5545" txt_f="5547">15</offsets></xref><offsets xml_i="9557" xml_f="9867" txt_i="5547" txt_f="5857">]. This might especially be an issue for students of low visuospatial ability, who are more likely to depend on reasoning strategies for solving all task aspects, in contrast to people of high visuospatial ability, who can bring spatial visualization strategies to bear when solving visuospatial task aspects [</offsets><xref rid="B9" ref-type="bibr"><offsets xml_i="9898" xml_f="9900" txt_i="5857" txt_f="5859">16</offsets></xref><offsets xml_i="9907" xml_f="10111" txt_i="5859" txt_f="6063">]. We aim to (1) explore the contribution of stereopsis to digital anatomy learning and to (2) assess the usefulness of virtual reality for anatomical learning for users of differing visuospatial ability.</offsets></p></sec><sec id="sec1.4"><title><offsets xml_i="10145" xml_f="10164" txt_i="6065" txt_f="6084">1.4. The Experiment</offsets></title><p><offsets xml_i="10175" xml_f="10612" txt_i="6085" txt_f="6522">To this end, we created a virtual anatomy learning environment to study the anatomy of the neck. In a three-condition randomized experiment, participants were tested for anatomical spatial knowledge of the C1 and C2 cervical vertebrae after either studying this anatomy in a stereoptic condition, in a nonstereoptic condition, or exploring an unrelated virtual environment. A virtual reality head-mounted display, the Oculus Rift SDK 2 (</offsets><xref ref-type="fig" rid="fig1"><offsets xml_i="10644" xml_f="10652" txt_i="6522" txt_f="6530">Figure 1</offsets></xref><offsets xml_i="10659" xml_f="11099" txt_i="6530" txt_f="6970">), was used for all conditions of the study phase. All participants were tested for stereoptic vision and visuospatial ability. We hypothesized that both virtual anatomy groups would outperform the nonanatomical control condition. We also expected the participants in the stereoptic condition would outperform the participants of the nonstereoptic condition, mostly caused by increased learning in participants of low visuospatial ability [</offsets><xref rid="B20" ref-type="bibr"><offsets xml_i="11131" xml_f="11132" txt_i="6970" txt_f="6971">7</offsets></xref><offsets xml_i="11139" xml_f="11141" txt_i="6971" txt_f="6973">].</offsets></p><p><offsets xml_i="11148" xml_f="11725" txt_i="6974" txt_f="7551">To assess spatial anatomical knowledge, we created a test that asks the participant to localize a cross section of the studied anatomy on a frontal view of that same anatomy. The student can do this by clicking one of a number of horizontal lines drawn over this frontal view. This minimizes reliance on other aspects of anatomical knowledge (e.g., verbal, functional), while at the same time being relevant for learning to apply anatomical knowledge in a clinical setting, for example, to interpret cross-sectional material resulting from radiological or histological imaging.</offsets></p><p><offsets xml_i="11732" xml_f="12578" txt_i="7552" txt_f="8398">To clarify the outcomes of this experiment and to inform future research, we distributed a questionnaire about cognitive load and visuospatial versus analytical problem solving strategy among all participants after the experiment. We reasoned that the virtual reality environment of the study phase might have added extraneous cognitive load, caused by novelty and the not entirely intuitive interaction with this environment. We hypothesized less cognitive load would be experienced by participants of high visuospatial ability, because they were likely to combine visuospatial strategies with analytical strategies. In contrast participants of low visuospatial ability were thought to depend on analytical strategies alone, both for visuospatial and for nonvisuospatial task aspects, which would have led to a comparatively high cognitive load.</offsets></p></sec></sec><sec id="sec2"><title><offsets xml_i="12616" xml_f="12640" txt_i="8401" txt_f="8425">2. Materials and Methods</offsets></title><sec id="sec2.1"><title><offsets xml_i="12672" xml_f="12689" txt_i="8426" txt_f="8443">2.1. Participants</offsets></title><p><offsets xml_i="12700" xml_f="13608" txt_i="8444" txt_f="9352">Eighty first-year medical and biomedical students of the Radboud University Medical Center (Nijmegen, Netherlands) signed up for participation in this study and were randomly distributed over three conditions. Sixteen students did not follow up, and data for one participant who did not see stereoptically was excluded from analysis (see below). Thus, data for 63 participants were analyzed. The participants were distributed as follows over the three experimental conditions: 22 students (8 male) participated in the stereoptic condition, 23 (7 male) in the nonstereoptic condition, and 18 (10 male) in the control condition. Mean age was 19.3, ranging between 18 and 25. All participants voluntarily signed an informed consent document. Participants received a 15-Euro gift voucher for their participation. No formal ethics review was sought, as this is not required for this type of study under Dutch law.</offsets></p></sec><sec id="sec2.2"><title><offsets xml_i="13642" xml_f="13661" txt_i="9354" txt_f="9373">2.2. Study Protocol</offsets></title><p><offsets xml_i="13672" xml_f="13836" txt_i="9374" txt_f="9538">The following steps were part of each session. They are listed in the same order as offered during the experiment. Total time for each session was about 60 minutes:</offsets><italic><offsets xml_i="13844" xml_f="13852" txt_i="9538" txt_f="9546"> Welcome</offsets></italic><offsets xml_i="13861" xml_f="13978" txt_i="9546" txt_f="9663">, informed consent, description and explanation of the session, and opportunity for the participant to ask questions.</offsets></p><p><offsets xml_i="13985" xml_f="13995" txt_i="9664" txt_f="9674">During the</offsets><italic><offsets xml_i="14003" xml_f="14011" txt_i="9674" txt_f="9682"> pretest</offsets></italic><offsets xml_i="14020" xml_f="14166" txt_i="9682" txt_f="9828">, we provided the participants with four example questions so they could use the study phase to prepare for the similar questions of the posttest.</offsets></p><p><offsets xml_i="14173" xml_f="14176" txt_i="9829" txt_f="9832">The</offsets><italic><offsets xml_i="14184" xml_f="14196" txt_i="9832" txt_f="9844"> study phase</offsets></italic><offsets xml_i="14205" xml_f="14338" txt_i="9844" txt_f="9977"> differed for each experimental condition, except for duration, which was 150 seconds. This duration was based on previous research [</offsets><xref rid="B19" ref-type="bibr"><offsets xml_i="14370" xml_f="14372" txt_i="9977" txt_f="9979">17</offsets></xref><offsets xml_i="14379" xml_f="14785" txt_i="9979" txt_f="10385">] and a three-person pilot. We found that around the two-minute mark participants start losing focus, which led us to believe a study phase exceeding 150 seconds would not lead to more learning. Participants were alerted to the time left for exploring the study phase environment at the thirty-second, one-minute, and two-minute mark. Students were randomly distributed over the following three conditions.</offsets></p><p><offsets xml_i="14792" xml_f="14793" txt_i="10386" txt_f="10387">
</offsets><italic><offsets xml_i="14801" xml_f="14829" txt_i="10387" txt_f="10415">(1) The Stereoptic Condition</offsets></italic><offsets xml_i="14838" xml_f="14986" txt_i="10415" txt_f="10563">. Participants in this group studied a 3D reconstruction of anatomical objects of the deep neck wearing the Oculus Rift SDK 2 head-mounted display (</offsets><xref ref-type="fig" rid="fig2"><offsets xml_i="15018" xml_f="15026" txt_i="10563" txt_f="10571">Figure 2</offsets></xref><offsets xml_i="15033" xml_f="15432" txt_i="10571" txt_f="10970">). The anatomical objects were offered stereoptically in this condition, meaning that each eye was presented with a slightly different view of the virtual objects, to enable the brain to generate the experience of stereoptic visual depth. The virtual anatomy was positioned at a set distance from the participants' head, and they could rotate the objects using the arrow keys of a regular keyboard. </offsets></p><p><offsets xml_i="15439" xml_f="15440" txt_i="10971" txt_f="10972">
</offsets><italic><offsets xml_i="15448" xml_f="15479" txt_i="10972" txt_f="11003">(2) The Nonstereoptic Condition</offsets></italic><offsets xml_i="15488" xml_f="15782" txt_i="11003" txt_f="11297">. This condition was identical to the stereoptic condition, with the exception of the virtual objects being offered such that both eyes were presented with the exact same visual perspective. The visuospatial depth cue of stereopsis was thus not available to the participants in this condition. </offsets></p><p><offsets xml_i="15789" xml_f="15790" txt_i="11298" txt_f="11299">
</offsets><italic><offsets xml_i="15798" xml_f="15823" txt_i="11299" txt_f="11324">(3) The Control Condition</offsets></italic><offsets xml_i="15832" xml_f="15996" txt_i="11324" txt_f="11488">. Participants in this condition did wear the Oculus Rift headset for 150 seconds and only got to explore a virtual sea world instead of test-related human anatomy.</offsets></p><p><offsets xml_i="16003" xml_f="16283" txt_i="11489" txt_f="11769">The anatomy of the neck was selected for reasons of complexity and curriculum. The selected structures were spatially complex but with enough salient characteristics to allow participants to uniquely localize transverse cross sections along the transverse axis of a frontal view (</offsets><xref ref-type="fig" rid="fig3"><offsets xml_i="16315" xml_f="16323" txt_i="11769" txt_f="11777">Figure 3</offsets></xref><offsets xml_i="16330" xml_f="16553" txt_i="11777" txt_f="12000">). The experiment was scheduled to allow the participation of first-year medical students experienced in studying academic anatomy, but naive to this specific region (the study of which was lined up next in the curriculum).</offsets></p><p><offsets xml_i="16560" xml_f="16628" txt_i="12001" txt_f="12069">In order to minimize a potential speed-accuracy trade-off during the</offsets><italic><offsets xml_i="16636" xml_f="16645" txt_i="12069" txt_f="12078"> posttest</offsets></italic><offsets xml_i="16654" xml_f="17228" txt_i="12078" txt_f="12652">, students were instructed to prioritize working carefully, but also to work fast. In each of the 43 questions of the posttest, participants were presented on the left side of the screen with either a ventral view or a dorsal view of the anatomy just studied, overlaid with horizontal, clickable lines. In each consecutive question, to the right a different cross section of the studied anatomy was displayed. This could either be a picture of the actual cross section on which the 3D reconstruction they studied was based, or an abstracted version of such a cross section (</offsets><xref ref-type="fig" rid="fig3"><offsets xml_i="17260" xml_f="17268" txt_i="12652" txt_f="12660">Figure 3</offsets></xref><offsets xml_i="17275" xml_f="17780" txt_i="12660" txt_f="13165">). We used these two variants to implement different levels of complexity (the abstracted version was easier to interpret). The participants' task was to click the horizontal line over the left-hand picture that corresponded with the cross section shown to the right. If a participant indicated a line above or below the official correct answer as correct, this was accepted given the similarity between many consecutive cross sections. Number of correct answers and average response times were collected.</offsets></p><p><offsets xml_i="17787" xml_f="17788" txt_i="13166" txt_f="13167">
</offsets><italic><offsets xml_i="17796" xml_f="17821" txt_i="13167" txt_f="13192">Visuospatial Ability Test</offsets></italic><offsets xml_i="17830" xml_f="18119" txt_i="13192" txt_f="13481">. To assess participants' visuospatial ability, we used a test battery consisting of electronic versions of two standard tests in this area. Both tests assess the ability to mentally manipulate relatively complex visuospatial stimuli; they were Vandenberg and Kuse's Mental Rotation Test [</offsets><xref rid="B24" ref-type="bibr"><offsets xml_i="18151" xml_f="18153" txt_i="13481" txt_f="13483">18</offsets></xref><offsets xml_i="18160" xml_f="18162" txt_i="13483" txt_f="13485">, </offsets><xref rid="B29" ref-type="bibr"><offsets xml_i="18194" xml_f="18196" txt_i="13485" txt_f="13487">19</offsets></xref><offsets xml_i="18203" xml_f="18239" txt_i="13487" txt_f="13523">] and the Surface Development test [</offsets><xref rid="B6" ref-type="bibr"><offsets xml_i="18270" xml_f="18272" txt_i="13523" txt_f="13525">20</offsets></xref><offsets xml_i="18279" xml_f="18282" txt_i="13525" txt_f="13528">]. </offsets></p><p><offsets xml_i="18289" xml_f="18290" txt_i="13529" txt_f="13530">
</offsets><italic><offsets xml_i="18298" xml_f="18323" txt_i="13530" txt_f="13555">Anatomical Knowledge Test</offsets></italic><offsets xml_i="18332" xml_f="18714" txt_i="13555" txt_f="13937">. To control for preexisting individual differences in anatomical (nonspatial) knowledge of the area under study, a 10-item multiple choice questionnaire was filled out digitally by all participants. This questionnaire was based on a consensus discussion of two expert anatomists (authors JK and MV) who compared and edited question lists they had previously compiled independently.</offsets></p><p><offsets xml_i="18721" xml_f="18722" txt_i="13938" txt_f="13939">
</offsets><italic><offsets xml_i="18730" xml_f="18745" txt_i="13939" txt_f="13954">Stereopsis Test</offsets></italic><offsets xml_i="18754" xml_f="18868" txt_i="13954" txt_f="14068">. Some 5 to 10 percent of a population does not perceive stereoptic visual depth, often due to early astigmatism [</offsets><xref rid="B21" ref-type="bibr"><offsets xml_i="18900" xml_f="18902" txt_i="14068" txt_f="14070">21</offsets></xref><offsets xml_i="18909" xml_f="19098" txt_i="14070" txt_f="14259">]. Not being able to see stereoptically is an exclusion criterion for this study, so we tested all participants for stereoptic vision using the Random Dot 3 LEA SYMBOLS® Stereoacuity Test [</offsets><xref rid="B1" ref-type="bibr"><offsets xml_i="19129" xml_f="19131" txt_i="14259" txt_f="14261">22</offsets></xref><offsets xml_i="19138" xml_f="19255" txt_i="14261" txt_f="14378">], a standard test in this area. Performance data for one participant were excluded from analysis based on this test.</offsets></p><p><offsets xml_i="19262" xml_f="19263" txt_i="14379" txt_f="14380">A</offsets><italic><offsets xml_i="19271" xml_f="19298" txt_i="14380" txt_f="14407"> demographics questionnaire</offsets></italic><offsets xml_i="19307" xml_f="19379" txt_i="14407" txt_f="14479"> was filled out by all participants digitally at the end of the session.</offsets></p><p><offsets xml_i="19386" xml_f="19438" txt_i="14480" txt_f="14532">The link to a digital, 12-item 5-point Likert scale,</offsets><italic><offsets xml_i="19446" xml_f="19515" txt_i="14532" txt_f="14601"> post hoc mixed cognitive load/problem solving strategy questionnaire</offsets></italic><offsets xml_i="19524" xml_f="19741" txt_i="14601" txt_f="14818"> was emailed to the participants five months after the experiment. This was done to contextualize our research findings and inform future research designs. Four of the questions assessed cognitive load (adapted from [</offsets><xref rid="B16" ref-type="bibr"><offsets xml_i="19773" xml_f="19775" txt_i="14818" txt_f="14820">23</offsets></xref><offsets xml_i="19782" xml_f="19938" txt_i="14820" txt_f="14976">]), four assessed the use of analytic problem solving strategies, and four questions assessed the use of visuospatial problem solving strategies (Appendix).</offsets></p></sec><sec id="sec2.3"><title><offsets xml_i="19972" xml_f="19986" txt_i="14978" txt_f="14992">2.3. Materials</offsets></title><p><offsets xml_i="19997" xml_f="20343" txt_i="14993" txt_f="15339">The experiment was run from a desktop computer under Windows 7 professional, SP1. The computer ran on an Intel Xeon W3565 CPU @ 3.20 Ghz, with 6-gigabyte RAM. Video was provided by a Nvidia Quadro 5000 video card. The Oculus Rift SDK2 VR head-mounted display was used to explore the virtual anatomical learning environment during the study phase.</offsets></p><p><offsets xml_i="20350" xml_f="20464" txt_i="15340" txt_f="15454">The study phase was created using the Unity game engine v. 5.0, with additional Oculus Rift plugins (available at </offsets><ext-link ext-link-type="uri" xlink:href="https://developer.oculus.com/downloads/"><offsets xml_i="20547" xml_f="20586" txt_i="15454" txt_f="15493">https://developer.oculus.com/downloads/</offsets></ext-link><offsets xml_i="20597" xml_f="20790" txt_i="15493" txt_f="15686">). Pretest and posttest were made and run in Open Sesame v. 2.9, a package for creating psychological experiments, provided as open source to the community by the Vrije Universiteit Amsterdam [</offsets><xref rid="B22" ref-type="bibr"><offsets xml_i="20822" xml_f="20824" txt_i="15686" txt_f="15688">24</offsets></xref><offsets xml_i="20831" xml_f="21077" txt_i="15688" txt_f="15934">]. Analysis was done in the SPSS statistical package, v. 21. The anatomical knowledge questionnaire and cognitive load/problem solving strategy questionnaire were made in LimeSurvey v. 1.92, the demographics questionnaire in Microsoft Excel 2013.</offsets></p><p><offsets xml_i="21084" xml_f="21459" txt_i="15935" txt_f="16310">The virtual anatomical objects were based on a manual segmentation of histological coupes of the neck. These histological coupes also formed the basis for the slices used at the pretest and posttest stage. For the graphical aspects of this, the Gimp v. 2.8 and Inkscape v. 0.91 were used, and the 3D reconstructions were made with the Surfdriver package and Meshlab v. 1.3.4.</offsets></p><p><offsets xml_i="21466" xml_f="21570" txt_i="16311" txt_f="16415">The post hoc questionnaire was created in the LimeSurvey v. 1.92 package for creating web-based surveys.</offsets></p></sec><sec id="sec2.4"><title><offsets xml_i="21604" xml_f="21634" txt_i="16417" txt_f="16447">2.4. Data Preparation/Analysis</offsets></title><sec id="sec2.4.1"><title><offsets xml_i="21668" xml_f="21696" txt_i="16448" txt_f="16476">2.4.1. Experimental Analysis</offsets></title><p><offsets xml_i="21707" xml_f="22295" txt_i="16477" txt_f="17065">Performance at the anatomical knowledge test was at chance level. Of the ten multiple choice questions, four had four answering options, and six had three answering options. Overall, for participants this results in 30 percent (3 answers) correct as performance at chance level. Answers came in at a mean of 2.8 correct, with an SD of 1.3, a minimum of 0, and a maximum of 6 answers correct. Consequently, this variable was not further analyzed in relation to other variables. This however rules out preexisting anatomical knowledge as a source of variability on the experimental results.</offsets></p><p><offsets xml_i="22302" xml_f="22892" txt_i="17066" txt_f="17656">The scores of the two visuospatial ability subtests were normalized and mean values from both were calculated as a proxy for participants' visuospatial ability. For each participant, the number of correct posttest answers and the average posttest question reaction time were calculated. The participants performed above chance, and a KS1 test showed these variables to not deviate significantly from the normal distribution, allowing for analysis of variance testing. We also calculated correlations between correct answers and reaction times to assess a potential speed-accuracy trade-off.</offsets></p><p><offsets xml_i="22899" xml_f="23384" txt_i="17657" txt_f="18142">An ANOVA for visuospatial ability × experimental condition was performed to verify a similar distribution of visuospatial ability between experimental groups. To assess the effect of stereoptic vision on anatomical learning in a virtual reality environment, an ANCOVA for the impact of experimental condition on correct answers and reaction times was performed, with spatial ability as a covariable. Additional ANOVAs were done to compare the effects of the three conditions pair-wise.</offsets></p></sec><sec id="sec2.4.2"><title><offsets xml_i="23420" xml_f="23458" txt_i="18144" txt_f="18182">2.4.2. Post Hoc Questionnaire Analysis</offsets></title><p><offsets xml_i="23469" xml_f="23909" txt_i="18183" txt_f="18623">Responses from 51 of the 63 participants were collected. The responses for the four items of each of the three question categories were averaged, resulting in variables for cognitive load, visuospatial problem solving strategy, and analytic problem solving strategy. A strategy preference variable was created by calculating the difference values of the visuospatial problem solving strategy and analytic problem solving strategy responses.</offsets></p><p><offsets xml_i="23916" xml_f="24101" txt_i="18624" txt_f="18809">To contextualize our main results, correlations were calculated for cognitive load, strategy preference, and visuospatial ability × posttest correct answers and posttest reaction times.</offsets></p></sec></sec></sec><sec id="sec3"><title><offsets xml_i="24145" xml_f="24155" txt_i="18813" txt_f="18823">3. Results</offsets></title><sec id="sec3.1"><title><offsets xml_i="24187" xml_f="24202" txt_i="18824" txt_f="18839">3.1. Experiment</offsets></title><p><offsets xml_i="24213" xml_f="24308" txt_i="18840" txt_f="18935">Descriptive statistics of the main variables, split by experimental condition, are provided in </offsets><xref ref-type="table" rid="tab1"><offsets xml_i="24342" xml_f="24349" txt_i="18935" txt_f="18942">Table 1</offsets></xref><offsets xml_i="24356" xml_f="24451" txt_i="18942" txt_f="19037">. A moderate but significant correlation was found between correct answers and reaction times (</offsets><italic><offsets xml_i="24459" xml_f="24460" txt_i="19037" txt_f="19038">r</offsets></italic><offsets xml_i="24469" xml_f="24477" txt_i="19038" txt_f="19046"> = .36, </offsets><italic><offsets xml_i="24485" xml_f="24486" txt_i="19046" txt_f="19047">p</offsets></italic><offsets xml_i="24495" xml_f="24720" txt_i="19047" txt_f="19269"> &lt; .005), which means that participants who took more time during the posttest phase were more likely to provide correct answers. This speed-accuracy trade-off did not influence the experimental results as explained below.</offsets></p><p><offsets xml_i="24727" xml_f="24953" txt_i="19270" txt_f="19496">The ANOVA for visuospatial ability × experimental condition showed a similar distribution of visuospatial ability over the experimental conditions, indicating a random selection from a homogenous population for each condition.</offsets></p><p><offsets xml_i="24960" xml_f="25232" txt_i="19497" txt_f="19769">The ANCOVA to assess the influence of digitally implemented stereopsis on virtual learning showed no effect of experimental condition on either reaction times or correct answers. As a covariate, visuospatial ability did impact correct answers (but not reaction times), at </offsets><italic><offsets xml_i="25240" xml_f="25241" txt_i="19769" txt_f="19770">F</offsets></italic><offsets xml_i="25250" xml_f="25265" txt_i="19770" txt_f="19785">(1) = 5.63 and </offsets><italic><offsets xml_i="25273" xml_f="25274" txt_i="19785" txt_f="19786">p</offsets></italic><offsets xml_i="25283" xml_f="25291" txt_i="19786" txt_f="19794"> = .02 (</offsets><xref ref-type="table" rid="tab2"><offsets xml_i="25325" xml_f="25332" txt_i="19794" txt_f="19801">Table 2</offsets></xref><offsets xml_i="25339" xml_f="25846" txt_i="19801" txt_f="20308">). Further comparing the experimental conditions and the control condition in paired ANOVAs, we did not find a difference between the two experimental conditions, and we also did not find a difference between experimental condition and the control condition. All of this renders the potential effect of a speed-accuracy trade-off mentioned above a moot point. Given these results, no further interaction effects for visuospatial ability and condition were studied as they would be statistically meaningless.</offsets></p></sec><sec id="sec3.2"><title><offsets xml_i="25880" xml_f="25907" txt_i="20310" txt_f="20337">3.2. Post Hoc Questionnaire</offsets></title><p><offsets xml_i="25918" xml_f="26134" txt_i="20338" txt_f="20554">Neither cognitive load nor strategy preference correlated significantly with posttest correct answers or visuospatial ability. Strategy preference, but not cognitive load, did correlate with posttest reaction times, </offsets><italic><offsets xml_i="26142" xml_f="26143" txt_i="20554" txt_f="20555">r</offsets></italic><offsets xml_i="26152" xml_f="26164" txt_i="20555" txt_f="20567"> = −.40 and </offsets><italic><offsets xml_i="26172" xml_f="26173" txt_i="20567" txt_f="20568">p</offsets></italic><offsets xml_i="26182" xml_f="26352" txt_i="20568" txt_f="20735"> &lt; .01. This indicates that the people who reported both a high use of analytical problem solving strategies and a low use of visuospatial strategies performed faster.</offsets></p></sec></sec><sec id="sec4"><title><offsets xml_i="26390" xml_f="26403" txt_i="20738" txt_f="20751">4. Discussion</offsets></title><p><offsets xml_i="26414" xml_f="26518" txt_i="20752" txt_f="20856">Confirming earlier research in this area, visuospatial ability positively impacted anatomical learning [</offsets><xref rid="B20" ref-type="bibr"><offsets xml_i="26550" xml_f="26551" txt_i="20856" txt_f="20857">7</offsets></xref><offsets xml_i="26558" xml_f="26560" txt_i="20857" txt_f="20859">, </offsets><xref rid="B10" ref-type="bibr"><offsets xml_i="26592" xml_f="26594" txt_i="20859" txt_f="20861">14</offsets></xref><offsets xml_i="26601" xml_f="26755" txt_i="20861" txt_f="21015">]. In contrast, we were not able to confirm earlier research that suggests stereopsis in digital learning environments can positively influence learning [</offsets><xref rid="B19" ref-type="bibr"><offsets xml_i="26787" xml_f="26789" txt_i="21015" txt_f="21017">17</offsets></xref><offsets xml_i="26796" xml_f="26927" txt_i="21017" txt_f="21148">]. We did in fact not find an effect for either of our experimental conditions compared to each other, or to the control condition.</offsets></p><p><offsets xml_i="26934" xml_f="27618" txt_i="21149" txt_f="21833">Possibly, regardless of whether any learning took place during the study phase, participants were able to solve the questions of the posttest based solely on the visual information in these questions, and this might have been a more attractive strategy than retrieving the studied anatomy from memory. If this is in fact the reason for our (lack of) results, we may conclude that in both future studies and educational practice it will be extremely important to better control the amount of visuospatial information available in test questions, especially when we start developing tests that rely more heavily on spatial reasoning compared to traditional, mostly text oriented tests [</offsets><xref rid="B11" ref-type="bibr"><offsets xml_i="27650" xml_f="27652" txt_i="21833" txt_f="21835">25</offsets></xref><offsets xml_i="27659" xml_f="27661" txt_i="21835" txt_f="21837">, </offsets><xref rid="B30" ref-type="bibr"><offsets xml_i="27693" xml_f="27695" txt_i="21837" txt_f="21839">26</offsets></xref><offsets xml_i="27702" xml_f="27867" txt_i="21839" txt_f="22004">]. We had no reason to expect such an effect however, given the experience of one of the authors (JML) who in an earlier series of studies with a very similar design</offsets><italic><offsets xml_i="27875" xml_f="27879" txt_i="22004" txt_f="22008"> did</offsets></italic><offsets xml_i="27888" xml_f="27940" txt_i="22008" txt_f="22060"> find effects for different study phase conditions [</offsets><xref rid="B20" ref-type="bibr"><offsets xml_i="27972" xml_f="27973" txt_i="22060" txt_f="22061">7</offsets></xref><offsets xml_i="27980" xml_f="27982" txt_i="22061" txt_f="22063">, </offsets><xref rid="B19" ref-type="bibr"><offsets xml_i="28014" xml_f="28016" txt_i="22063" txt_f="22065">17</offsets></xref><offsets xml_i="28023" xml_f="28284" txt_i="22065" txt_f="22326">]. These contrasting results may have been caused by the earlier studies working with participants naive to the study of human anatomy, who might have been less able to use visuospatial information available in anatomical questions compared to medical students.</offsets></p><p><offsets xml_i="28291" xml_f="28684" txt_i="22327" txt_f="22720">An additional explanation for the lack of effect of our experimental treatment is that the pretest questions may have primed our participants to pay little attention to the study phase. Finding they would be able at posttest to answer the questions based on the information available in the questions themselves, they may have interpreted the study phase as an appropriate time for relaxation.</offsets></p><p><offsets xml_i="28691" xml_f="28836" txt_i="22721" txt_f="22866">In contrast to received wisdom that visuospatial problem solving strategies temporally outperform analytic strategies for visuospatial problems [</offsets><xref rid="B15" ref-type="bibr"><offsets xml_i="28868" xml_f="28870" txt_i="22866" txt_f="22868">27</offsets></xref><offsets xml_i="28877" xml_f="29576" txt_i="22868" txt_f="23567">], participants reporting a relative high use of analytical problem solving strategies performed faster. The high complexity of the virtual anatomy may have caused visuospatial problem solving strategies to actually slow participants down. If this is so, there might be an optimum level of visuospatial complexity where high visuospatial people outperform low visuospatial people, above which low visuospatial people, used to revert to analytical strategies sooner, will actually start outperforming their high visuospatial counterparts again. More studies are needed that explore the relation between visuospatial ability, reasoning ability, problem solving strategies, and visuospatial complexity.</offsets></p><sec id="sec4.1"><title><offsets xml_i="29604" xml_f="29620" txt_i="23568" txt_f="23584">4.1. Limitations</offsets></title><p><offsets xml_i="29631" xml_f="30552" txt_i="23585" txt_f="24506">The implementation of virtual reality was incomplete. For example, exploration during the study phase was somewhat cumbersome: the objects could only be rotated using the arrow keys of the keyboard. A motion controlled sensor could have made interaction more natural; however this technology was not available to us when we were building the experiment. Secondly, the virtual anatomical objects were not positioned in a specific location in virtual space; instead they were tethered to the Oculus Rift at a set distance and orientation. While this allowed us to abolish “distance to the objects” as a source of variability between participants, this also distorted the illusion of being in a stable environment containing virtual objects. In a follow-up study, we will explore gesture recognition technology (such as the Leap or the Kinekt) to implement interaction and provide the virtual objects with a stable location.</offsets></p><p><offsets xml_i="30559" xml_f="31291" txt_i="24507" txt_f="25239">Cognitive load was not found to influence any of our results; however high stimulus complexity and unfamiliarity with the virtual reality hardware might still have hindered learning during the study phase. Easier digital anatomical objects might have led to more learning during the study phase, which could have translated to participants using visual memory based strategies more often during posttest execution. More variety in terms of visuospatial complexity for the objects studied will allow us to test this hypothesis. In addition to this, a familiarization phase at the beginning of the experiment for the virtual reality setup should diminish extraneous cognitive load caused by unfamiliarity with this type of technology.</offsets></p></sec><sec id="sec4.2"><title><offsets xml_i="31325" xml_f="31341" txt_i="25241" txt_f="25257">4.2. Future Work</offsets></title><p><offsets xml_i="31352" xml_f="32124" txt_i="25258" txt_f="26030">In a follow-up study in preparation we want to explore the relation between visuospatial ability, spatial complexity, and problem solving strategy. We will prime our participants to the existence of both visuospatial and reasoning problem solving strategies to enable them to report their use of those throughout the experiment. We will also systematically vary the complexity of our stimulus materials. Thirdly, we will minimize the amount of visuospatial information present in posttest questions to stimulate recall of information learned during the study phase. Lastly, we will continue developing our virtual reality environment to maximize the feeling of being present by fully localizing the virtual objects and by implementing easier to use interaction technology.</offsets></p></sec></sec><sec id="sec5"><title><offsets xml_i="32162" xml_f="32176" txt_i="26033" txt_f="26047">5. Conclusions</offsets></title><p><offsets xml_i="32187" xml_f="32508" txt_i="26048" txt_f="26369">We do not know yet whether stereopsis in digital learning environments helps or hinders anatomy learning. It is important in both research and educational practice to control the amount of visual information provided by test questions. Given the speed of the development of virtual reality enabling technologies, research</offsets></p></sec></body><back><app-group><app><title>Appendix</title><sec id="secA"><title>A. Post Hoc Questionnaire “Cognitive Load and Problem Solving Strategies” (5-Point Likert Scale, Translated from Dutch)</title><sec id="secA.1"><title>A.1. Cognitive Load</title><p>
<list list-type="roman-lower"><list-item><p>The anatomical objects I had to study were very complex.</p></list-item><list-item><p>The virtual reality setup was very hard to use.</p></list-item><list-item><p>The explanation of the experiment was very unclear.</p></list-item><list-item><p>After the study phase my understanding of the anatomy of the topmost cervical vertebrae was much better.</p></list-item></list>
</p></sec><sec id="secA.2"><title>A.2. Problem Solving Strategy, Analytic</title><p>
<list list-type="roman-lower"><list-item><p>I compared specific elements of the test's cross sections with specific elements of the frontal or dorsal view.</p></list-item><list-item><p>I systematically discarded options until the correct answer remained.</p></list-item><list-item><p>I always combined the top and bottom elements of the cross sections.</p></list-item><list-item><p>I rarely recalled the studied model in order to solve the test questions.</p></list-item></list>
</p></sec><sec id="secA.3"><title>A.3. Problem Solving Strategy, Visuospatial</title><p>
<list list-type="roman-lower"><list-item><p>I mentally rotated the cross section to see where it would fit in the frontal or dorsal view.</p></list-item><list-item><p>It was easy for me to visually recall the studied anatomy to help me identify the cross sections' correct location.</p></list-item><list-item><p>It was easy for me to mentally rotate the studied anatomy help me identify the cross sections' correct location.</p></list-item><list-item><p>The negative spaces (“holes”) in the cross sections really helped me select the correct answer.</p></list-item></list>
</p></sec></sec></app></app-group><sec><title>Conflicts of Interest</title><p>The authors declare that there are no conflicts of interest regarding the publication of this paper.</p></sec><ref-list><ref id="B27"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steuer</surname><given-names>J.</given-names></name></person-group><article-title>Defining virtual reality: dimensions determining telepresence</article-title><source><italic>Journal of Communication</italic></source><year>1992</year><volume>42</volume><issue>4</issue><fpage>73</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1111/j.1460-2466.1992.tb00812.x</pub-id><pub-id pub-id-type="other">2-s2.0-84985084394</pub-id></element-citation></ref><ref id="B31"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yammine</surname><given-names>K.</given-names></name><name><surname>Violato</surname><given-names>C.</given-names></name></person-group><article-title>A meta-analysis of the educational effectiveness of three-dimensional visualization technologies in teaching anatomy</article-title><source><italic>Anatomical Sciences Education</italic></source><year>2015</year><volume>8</volume><issue>6</issue><fpage>525</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1002/ase.1510</pub-id><pub-id pub-id-type="other">2-s2.0-84945447481</pub-id><pub-id pub-id-type="pmid">25557582</pub-id></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenton</surname><given-names>H.</given-names></name><name><surname>Hernandez</surname><given-names>J.</given-names></name><name><surname>Bello</surname><given-names>F.</given-names></name><etal></etal></person-group><article-title>Using multimedia and Web3D to enhance anatomy teaching</article-title><source><italic>Computers &amp; Education</italic></source><year>2007</year><volume>49</volume><issue>1</issue><fpage>32</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.compedu.2005.06.005</pub-id><pub-id pub-id-type="other">2-s2.0-33846498496</pub-id></element-citation></ref><ref id="B28"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Trelease</surname><given-names>R. B.</given-names></name></person-group><article-title>Essential E-Learning and M-Learning Methods for Teaching Anatomy</article-title><source><italic>In Teaching Anatomy</italic></source><year>2015</year><publisher-name>Springer</publisher-name><fpage>247</fpage><lpage>258</lpage></element-citation></ref><ref id="B7"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garg</surname><given-names>A. X.</given-names></name><name><surname>Norman</surname><given-names>G. R.</given-names></name><name><surname>Eva</surname><given-names>K. W.</given-names></name><name><surname>Spero</surname><given-names>L.</given-names></name><name><surname>Sharan</surname><given-names>S.</given-names></name></person-group><article-title>Is there any real virtue of virtual reality?: The minor role of multiple orientations in learning anatomy from computers</article-title><source><italic>Academic Medicine</italic></source><year>2002</year><volume>77</volume><issue>10</issue><fpage>S97</fpage><lpage>S99</lpage><pub-id pub-id-type="doi">10.1097/00001888-200210001-00030</pub-id><pub-id pub-id-type="other">2-s2.0-0036794159</pub-id><pub-id pub-id-type="pmid">12377717</pub-id></element-citation></ref><ref id="B17"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levinson</surname><given-names>A. J.</given-names></name><name><surname>Weaver</surname><given-names>B.</given-names></name><name><surname>Garside</surname><given-names>S.</given-names></name><name><surname>McGinn</surname><given-names>H.</given-names></name><name><surname>Norman</surname><given-names>G. R.</given-names></name></person-group><article-title>Virtual reality and brain anatomy: A randomised trial of e-learning instructional designs</article-title><source><italic>Medical Education</italic></source><year>2007</year><volume>41</volume><issue>5</issue><fpage>495</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2929.2006.02694.x</pub-id><pub-id pub-id-type="other">2-s2.0-34247389749</pub-id><pub-id pub-id-type="pmid">17470079</pub-id></element-citation></ref><ref id="B20"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luursema</surname><given-names>J.-M.</given-names></name><name><surname>Verwey</surname><given-names>W. B.</given-names></name><name><surname>Kommers</surname><given-names>P. A. M.</given-names></name><name><surname>Geelkerken</surname><given-names>R. H.</given-names></name><name><surname>Vos</surname><given-names>H. J.</given-names></name></person-group><article-title>Optimizing conditions for computer-assisted anatomical learning</article-title><source><italic>Interacting with Computers</italic></source><year>2006</year><volume>18</volume><issue>5</issue><fpage>1123</fpage><lpage>1138</lpage><pub-id pub-id-type="doi">10.1016/j.intcom.2006.01.005</pub-id><pub-id pub-id-type="other">2-s2.0-33748909398</pub-id></element-citation></ref><ref id="B23"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>D. T.</given-names></name><name><surname>Chalk</surname><given-names>C.</given-names></name><name><surname>Funnell</surname><given-names>W. R. J.</given-names></name><name><surname>Daniel</surname><given-names>S. J.</given-names></name></person-group><article-title>Can virtual reality improve anatomy education? A randomised controlled study of a computer-generated three-dimensional anatomical ear model</article-title><source><italic>Medical Education</italic></source><year>2006</year><volume>40</volume><issue>11</issue><fpage>1081</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2929.2006.02611.x</pub-id><pub-id pub-id-type="other">2-s2.0-33750194429</pub-id><pub-id pub-id-type="pmid">17054617</pub-id></element-citation></ref><ref id="B13"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H. M.</given-names></name><name><surname>Liaw</surname><given-names>S. S.</given-names></name><name><surname>Lai</surname><given-names>C. M.</given-names></name></person-group><article-title>Exploring learner acceptance of the use of virtual reality in medical education: a case study of desktop and projection-based display systems</article-title><source><italic>Interactive Learning Environments</italic></source><year>2016</year><volume>24</volume><issue>1</issue><fpage>3</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1080/10494820.2013.817436</pub-id><pub-id pub-id-type="other">2-s2.0-84955175326</pub-id></element-citation></ref><ref id="B2"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradshaw</surname><given-names>M. F.</given-names></name><name><surname>Parton</surname><given-names>A. D.</given-names></name><name><surname>Eagle</surname><given-names>R. A.</given-names></name></person-group><article-title>The interaction of binocular disparity and motion parallax in determining perceived depth and perceived size</article-title><source><italic>Perception</italic></source><year>1998</year><volume>27</volume><issue>11</issue><fpage>1317</fpage><lpage>1331</lpage><pub-id pub-id-type="doi">10.1068/p271317</pub-id><pub-id pub-id-type="other">2-s2.0-0032249320</pub-id><pub-id pub-id-type="pmid">10505177</pub-id></element-citation></ref><ref id="B8"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>M. A.</given-names></name></person-group><article-title>Transforming vision into action</article-title><source><italic>Vision Research</italic></source><year>2011</year><volume>51</volume><issue>13</issue><fpage>1567</fpage><lpage>1587</lpage><pub-id pub-id-type="pmid">20691202</pub-id></element-citation></ref><ref id="B25"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preece</surname><given-names>D.</given-names></name><name><surname>Williams</surname><given-names>S. B.</given-names></name><name><surname>Lam</surname><given-names>R.</given-names></name><name><surname>Weller</surname><given-names>R.</given-names></name></person-group><article-title>"Let's Get Physical": advantages of a physical model over 3d computer models and textbooks in learning imaging anatomy</article-title><source><italic>Anatomical Sciences Education</italic></source><year>2013</year><volume>6</volume><issue>4</issue><fpage>216</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1002/ase.1345</pub-id><pub-id pub-id-type="other">2-s2.0-84880045776</pub-id><pub-id pub-id-type="pmid">23349117</pub-id></element-citation></ref><ref id="B14"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khot</surname><given-names>Z.</given-names></name><name><surname>Quinlan</surname><given-names>K.</given-names></name><name><surname>Norman</surname><given-names>G. R.</given-names></name><name><surname>Wainman</surname><given-names>B.</given-names></name></person-group><article-title>The relative effectiveness of computer‐based and traditional resources for education in anatomy</article-title><source><italic>Anatomical Sciences Education</italic></source><year>2013</year><volume>6</volume><issue>4</issue><fpage>211</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1002/ase.1355</pub-id><pub-id pub-id-type="other">2-s2.0-84880044007</pub-id><pub-id pub-id-type="pmid">23509000</pub-id></element-citation></ref><ref id="B10"><label>14</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hegarty</surname><given-names>M.</given-names></name><name><surname>Keehner</surname><given-names>M.</given-names></name><name><surname>Cohen</surname><given-names>C.</given-names></name><name><surname>Montello</surname><given-names>D. R.</given-names></name><name><surname>Lippa</surname><given-names>Y.</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Allen</surname><given-names>G.</given-names></name></person-group><article-title>The role of spatial cognition in medicine: Applications for selecting and training professionals</article-title><source><italic>In Applied spatial cognition: From Research to Cognitive Technology</italic></source><year>2007</year><publisher-loc>G</publisher-loc><publisher-name>Allen Lawrence Erlbaum Associates: Mahwah, NJ</publisher-name></element-citation></ref><ref id="B18"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>G.</given-names></name><name><surname>Shark</surname><given-names>L. K.</given-names></name><name><surname>Hall</surname><given-names>G.</given-names></name><name><surname>Zeshan</surname><given-names>U.</given-names></name></person-group><article-title>Immersive manipulation of virtual objects through glove-based hand gesture interaction</article-title><source><italic>Virtual Reality</italic></source><year>2012</year><volume>16</volume><issue>3</issue><fpage>243</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s10055-011-0195-9</pub-id><pub-id pub-id-type="other">2-s2.0-84865481263</pub-id></element-citation></ref><ref id="B9"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gur</surname><given-names>R. C.</given-names></name><name><surname>Alsop</surname><given-names>D.</given-names></name><name><surname>Glahn</surname><given-names>D.</given-names></name><etal></etal></person-group><article-title>An fMRI study of sex differences in regional activation to a verbal and a spatial task</article-title><source><italic>Brain and Language</italic></source><year>2000</year><volume>74</volume><issue>2</issue><fpage>157</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1006/brln.2000.2325</pub-id><pub-id pub-id-type="other">2-s2.0-0033842477</pub-id><pub-id pub-id-type="pmid">10950912</pub-id></element-citation></ref><ref id="B19"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luursema</surname><given-names>J. M.</given-names></name><name><surname>Verwey</surname><given-names>W. B.</given-names></name><name><surname>Kommers</surname><given-names>P. A. M.</given-names></name><name><surname>Annema</surname><given-names>J.-H.</given-names></name></person-group><article-title>The role of stereopsis in virtual anatomical learning</article-title><source><italic>Interacting with Computers</italic></source><year>2008</year><volume>20</volume><issue>4-5</issue><fpage>455</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.intcom.2008.04.003</pub-id><pub-id pub-id-type="other">2-s2.0-53149106619</pub-id></element-citation></ref><ref id="B24"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>M.</given-names></name><name><surname>Laeng</surname><given-names>B.</given-names></name><name><surname>Latham</surname><given-names>K.</given-names></name><name><surname>Jackson</surname><given-names>M.</given-names></name><name><surname>Zaiyouna</surname><given-names>R.</given-names></name><name><surname>Richardson</surname><given-names>C.</given-names></name></person-group><article-title>A redrawn vandenberg and kuse mental rotations test - different versions and factors that affect performance</article-title><source><italic>Brain and Cognition</italic></source><year>1995</year><volume>28</volume><issue>1</issue><fpage>39</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1006/brcg.1995.1032</pub-id><pub-id pub-id-type="other">2-s2.0-0029315746</pub-id><pub-id pub-id-type="pmid">7546667</pub-id></element-citation></ref><ref id="B29"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandenberg</surname><given-names>S. G.</given-names></name><name><surname>Kuse</surname><given-names>A. R.</given-names></name></person-group><article-title>Mental rotations, a group test of three-dimensional spatial visualization</article-title><source><italic>Perceptual and Motor Skills</italic></source><year>1978</year><volume>47</volume><issue>2</issue><fpage>599</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.2466/pms.1978.47.2.599</pub-id><pub-id pub-id-type="other">2-s2.0-0018180305</pub-id><pub-id pub-id-type="pmid">724398</pub-id></element-citation></ref><ref id="B6"><label>20</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekstrom</surname><given-names>R. B.</given-names></name><name><surname>French</surname><given-names>J. W.</given-names></name><name><surname>Harman</surname><given-names>H. H.</given-names></name><name><surname>Dermen</surname><given-names>D.</given-names></name></person-group><article-title>Manual for kit of factor referenced cognitive tests</article-title><source><italic>Educational Testing Service Princeton</italic></source><year>1976</year><publisher-loc>Princeton, NJ</publisher-loc><publisher-name>Educational Testing Service</publisher-name></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mather</surname><given-names>G.</given-names></name></person-group><source><italic>Foundations of Perception</italic></source><year>2006</year><publisher-name>Taylor &amp; Francis</publisher-name></element-citation></ref><ref id="B1"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birch</surname><given-names>E.</given-names></name><name><surname>Williams</surname><given-names>C.</given-names></name><name><surname>Drover</surname><given-names>J.</given-names></name><etal></etal></person-group><article-title>Randot Preschool Stereoacuity Test: Normative data and validity</article-title><source><italic>Journal of American Association for Pediatric Ophthalmology and Strabismus</italic></source><year>2008</year><volume>12</volume><issue>1</issue><fpage>23</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.jaapos.2007.06.003</pub-id><pub-id pub-id-type="other">2-s2.0-39749136198</pub-id><pub-id pub-id-type="pmid">17720573</pub-id></element-citation></ref><ref id="B16"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leppink</surname><given-names>J.</given-names></name><name><surname>Paas</surname><given-names>F.</given-names></name><name><surname>Van der Vleuten</surname><given-names>C. P. M.</given-names></name><name><surname>Van Gog</surname><given-names>T.</given-names></name><name><surname>Van Merriënboer</surname><given-names>J. J. G.</given-names></name></person-group><article-title>Development of an instrument for measuring different types of cognitive load</article-title><source><italic>Behavior Research Methods</italic></source><year>2013</year><volume>45</volume><issue>4</issue><fpage>1058</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.3758/s13428-013-0334-1</pub-id><pub-id pub-id-type="other">2-s2.0-84889241941</pub-id><pub-id pub-id-type="pmid">23572251</pub-id></element-citation></ref><ref id="B22"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname><given-names>S.</given-names></name><name><surname>Schreij</surname><given-names>D.</given-names></name><name><surname>Theeuwes</surname><given-names>J.</given-names></name></person-group><article-title>OpenSesame: an open-source, graphical experiment builder for the social sciences</article-title><source><italic>Behavior Research Methods</italic></source><year>2012</year><volume>44</volume><issue>2</issue><fpage>314</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.3758/s13428-011-0168-7</pub-id><pub-id pub-id-type="other">2-s2.0-84861408364</pub-id><pub-id pub-id-type="pmid">22083660</pub-id></element-citation></ref><ref id="B11"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holland</surname><given-names>J.</given-names></name><name><surname>O'Sullivan</surname><given-names>R.</given-names></name><name><surname>Arnett</surname><given-names>R.</given-names></name></person-group><article-title>Is a picture worth a thousand words: an analysis of the difficulty and discrimination parameters of illustrated vs. text-alone vignettes in histology multiple choice questions</article-title><source><italic>BMC Medical Education</italic></source><year>2015</year><volume>15</volume><issue>1, article 184</issue><pub-id pub-id-type="doi">10.1186/s12909-015-0452-9</pub-id><pub-id pub-id-type="other">2-s2.0-84945312219</pub-id></element-citation></ref><ref id="B30"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vorstenbosch</surname><given-names>M. A.</given-names></name><name><surname>Klaassen</surname><given-names>T. P.</given-names></name><name><surname>Kooloos</surname><given-names>J. G.</given-names></name><name><surname>Bolhuis</surname><given-names>S. M.</given-names></name><name><surname>Laan</surname><given-names>R. F.</given-names></name></person-group><article-title>Do images influence assessment in anatomy? Exploring the effect of images on item difficulty and item discrimination</article-title><source><italic>Anatomical Sciences Education</italic></source><year>2013</year><volume>6</volume><issue>1</issue><fpage>29</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1002/ase.1290</pub-id><pub-id pub-id-type="other">2-s2.0-84872166309</pub-id><pub-id pub-id-type="pmid">22674609</pub-id></element-citation></ref><ref id="B15"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinteberg</surname><given-names>B. A.</given-names></name><name><surname>Levander</surname><given-names>S. E.</given-names></name><name><surname>Schalling</surname><given-names>D.</given-names></name></person-group><article-title>Cognitive sex differences: speed and problem-solving strategies on computerized neuropsychological tasks</article-title><source><italic>Perceptual and Motor Skills</italic></source><year>1988</year><volume>65</volume><issue>3</issue><fpage>683</fpage><lpage>697</lpage><pub-id pub-id-type="other">2-s2.0-0023883089</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Oculus Rift SDK 2. The virtual learning environment used in this study was developed for this head-mounted display.</p></caption><graphic xlink:href="ARI2017-1493135.001"></graphic></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>View of the learning environment the participants interacted with during the study phase. In this screenshot the 3D anatomy looks a bit small. This is caused by the relatively large screen area reserved within the Oculus Rift for peripheral vision and does not correspond to the user experience.</p></caption><graphic xlink:href="ARI2017-1493135.002"></graphic></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Four example questions of the posttest. The participants were instructed to click the horizontal line over the left-hand picture corresponding to the cross section shown on the right.</p></caption><graphic xlink:href="ARI2017-1493135.003"></graphic></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Descriptive statistics of the main variables, split by experimental condition. Reported as mean (standard deviation).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Variables∖conditions</th><th align="center" rowspan="1" colspan="1">Stereoptic</th><th align="center" rowspan="1" colspan="1">Nonstereoptic</th><th align="center" rowspan="1" colspan="1">Control</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Posttest accuracy (# correct from 43 items)</td><td align="center" rowspan="1" colspan="1">18.86 (5.65)</td><td align="center" rowspan="1" colspan="1">18.78 (5.47)</td><td align="center" rowspan="1" colspan="1">18.61 (4.47)</td></tr><tr><td align="left" rowspan="1" colspan="1">Posttest speed (<italic>s</italic>)</td><td align="center" rowspan="1" colspan="1">22.39 (12.19)</td><td align="center" rowspan="1" colspan="1">22.43 (10.48)</td><td align="center" rowspan="1" colspan="1">23.15 (15.95)</td></tr><tr><td align="left" rowspan="1" colspan="1">Visuospatial ability (normalized)</td><td align="center" rowspan="1" colspan="1">−0.12 (1.18)</td><td align="center" rowspan="1" colspan="1">0.03 (0.99)</td><td align="center" rowspan="1" colspan="1">0.11 (0.78)</td></tr><tr><td align="left" rowspan="1" colspan="1">Strategy bias (difference score from 2 Likert 5 pt. scale variables)</td><td align="center" rowspan="1" colspan="1">0.15 (0.48)</td><td align="center" rowspan="1" colspan="1">0.15 (0.68)</td><td align="center" rowspan="1" colspan="1">−0.20 (1.15)</td></tr><tr><td align="left" rowspan="1" colspan="1">Cognitive load (5 pt. Likert)</td><td align="center" rowspan="1" colspan="1">3.01 (0.53)</td><td align="center" rowspan="1" colspan="1">2.93 (0.55)</td><td align="center" rowspan="1" colspan="1">2.77 (0.53)</td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Results of our ANCOVA, investigating the relation between posttest performance, experimental condition, and visuospatial ability.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Source</th><th align="center" rowspan="1" colspan="1">Dependent variable</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic> (df)</th><th align="center" rowspan="1" colspan="1">
<italic>p</italic>
</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Experimental condition</td><td align="center" rowspan="1" colspan="1">Correct answers</td><td align="center" rowspan="1" colspan="1">.07 (2)</td><td align="center" rowspan="1" colspan="1">.93</td></tr><tr><td align="left" rowspan="1" colspan="1">Experimental condition</td><td align="center" rowspan="1" colspan="1">Total duration</td><td align="center" rowspan="1" colspan="1">.01 (2)</td><td align="center" rowspan="1" colspan="1">.99</td></tr><tr><td align="left" rowspan="1" colspan="1">Visuospatial ability</td><td align="center" rowspan="1" colspan="1">Correct answers</td><td align="center" rowspan="1" colspan="1">5.63 (1)</td><td align="center" rowspan="1" colspan="1">.02</td></tr><tr><td align="left" rowspan="1" colspan="1">Visuospatial ability</td><td align="center" rowspan="1" colspan="1">Total duration</td><td align="center" rowspan="1" colspan="1">.80 (1)</td><td align="center" rowspan="1" colspan="1">.37</td></tr></tbody></table></table-wrap></floats-group></article>