<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23565168</article-id><article-id pub-id-type="pmc">3614914</article-id><article-id pub-id-type="publisher-id">PONE-D-12-29445</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0059795</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational Biology</subject><subj-group><subject>Microarrays</subject><subject>Systems Biology</subject></subj-group></subj-group><subj-group><subject>Molecular Cell Biology</subject><subj-group><subject>Gene Expression</subject></subj-group></subj-group><subj-group><subject>Systems Biology</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer Science</subject><subj-group><subject>Algorithms</subject></subj-group><subj-group><subject>Software Engineering</subject><subj-group><subject>Software Tools</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Mathematics</subject><subj-group><subject>Probability Theory</subject><subj-group><subject>Bayes Theorem</subject></subj-group></subj-group><subj-group><subject>Statistics</subject><subj-group><subject>Biostatistics</subject><subject>Statistical Methods</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Accelerating Bayesian Hierarchical Clustering of Time Series Data with a Randomised Algorithm</article-title><alt-title alt-title-type="running-head">Clustering Time Series with a Randomised Algorithm</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Darkins</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="author-notes" rid="fn1">
<sup>¤a</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Cooke</surname><given-names>Emma J.</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name><xref ref-type="aff" rid="aff3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Kirk</surname><given-names>Paul D. W.</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="author-notes" rid="fn2">
<sup>¤b</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Wild</surname><given-names>David L.</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Savage</surname><given-names>Richard S.</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<addr-line>Systems Biology Centre, University of Warwick, Coventry, United Kingdom</addr-line>
</aff><aff id="aff2">
<label>2</label>
<addr-line>Department of Chemistry, University of Warwick, Coventry, United Kingdom</addr-line>
</aff><aff id="aff3">
<label>3</label>
<addr-line>Department of Engineering, University of Cambridge, Cambridge, United Kingdom</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Rattray</surname><given-names>Magnus</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"></xref></contrib></contrib-group><aff id="edit1">
<addr-line>University of Manchester, United Kingdom</addr-line>
</aff><author-notes><corresp id="cor1">* E-mail: <email>r.s.savage@warwick.ac.uk</email></corresp><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p>Conceived and designed the experiments: RD RSS EJC ZG PDWK DLW. Performed the experiments: RD. Analyzed the data: RD RSS EJC PDWK DLW. Contributed reagents/materials/analysis tools: RD RSS. Wrote the paper: RD RSS EJC ZG PDWK DLW.</p></fn><fn id="fn1" fn-type="current-aff"><label>¤a</label><p>Current address: London Centre for Nanotechnology, University College London, London, United Kingdom</p></fn><fn id="fn2" fn-type="current-aff"><label>¤b</label><p>Current address: Centre for Bioinformatics, Imperial College London, London, United Kingdom</p></fn></author-notes><pub-date pub-type="collection"><year>2013</year></pub-date><pub-date pub-type="epub"><day>2</day><month>4</month><year>2013</year></pub-date><volume>8</volume><issue>4</issue><elocation-id>e59795</elocation-id><history><date date-type="received"><day>25</day><month>9</month><year>2012</year></date><date date-type="accepted"><day>19</day><month>2</month><year>2013</year></date></history><permissions><copyright-statement>© 2013 Darkins et al</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Darkins et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p><offsets xml_i="5985" xml_f="6942" txt_i="11" txt_f="968">We live in an era of abundant data. This has necessitated the development of new and innovative statistical algorithms to get the most from experimental data. For example, faster algorithms make practical the analysis of larger genomic data sets, allowing us to extend the utility of cutting-edge statistical methods. We present a randomised algorithm that accelerates the clustering of time series data using the Bayesian Hierarchical Clustering (BHC) statistical method. BHC is a general method for clustering any discretely sampled time series data. In this paper we focus on a particular application to microarray gene expression data. We define and analyse the randomised algorithm, before presenting results on both synthetic and real biological data sets. We show that the randomised algorithm leads to substantial gains in speed with minimal loss in clustering quality. The randomised time series BHC algorithm is available as part of the R package </offsets><italic><offsets xml_i="6950" xml_f="6953" txt_i="968" txt_f="971">BHC</offsets></italic><offsets xml_i="6962" xml_f="7043" txt_i="971" txt_f="1052">, which is available for download from Bioconductor (version 2.10 and above) via </offsets><ext-link ext-link-type="uri" xlink:href="http://bioconductor.org/packages/2.10/bioc/html/BHC.html"><offsets xml_i="7143" xml_f="7199" txt_i="1052" txt_f="1108">http://bioconductor.org/packages/2.10/bioc/html/BHC.html</offsets></ext-link><offsets xml_i="7210" xml_f="7374" txt_i="1108" txt_f="1272">. We have also made available a set of R scripts which can be used to reproduce the analyses carried out in this paper. These are available from the following URL. </offsets><ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/randomisedbhc/"><offsets xml_i="7462" xml_f="7506" txt_i="1272" txt_f="1316">https://sites.google.com/site/randomisedbhc/</offsets></ext-link><offsets xml_i="7517" xml_f="7518" txt_i="1316" txt_f="1317">.</offsets></p></abstract><funding-group><funding-statement>The authors acknowledge support from EPSRC grants EP/F027400/1 (DLW, PDWK, RD and ZG) and EP/I036575/1 (DLW, PDWK and ZG). RSS acknowledges support from an MRC Biostatistics Fellowship. EJC acknowledges support from the Warwick MOAC Doctoral Training Centre. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"></page-count></counts></article-meta></front><body><sec id="s1"><title><offsets xml_i="8088" xml_f="8100" txt_i="1325" txt_f="1337">Introduction</offsets></title><p><offsets xml_i="8111" xml_f="8433" txt_i="1338" txt_f="1660">Many scientific disciplines are becoming data intensive. These subjects require the development of new and innovative statistical algorithms to fully utilise these data. Time series clustering methods in particular have become popular in many disciplines such as clustering stocks with different price dynamics in finance </offsets><xref rid="pone.0059795-Bauwens1" ref-type="bibr"><offsets xml_i="8483" xml_f="8486" txt_i="1660" txt_f="1663">[1]</offsets></xref><offsets xml_i="8493" xml_f="8545" txt_i="1663" txt_f="1715">, clustering regions with different growth patterns </offsets><xref rid="pone.0059795-FrhwirthSchnatter1" ref-type="bibr"><offsets xml_i="8605" xml_f="8608" txt_i="1715" txt_f="1718">[2]</offsets></xref><offsets xml_i="8615" xml_f="8637" txt_i="1718" txt_f="1740"> or signal clustering </offsets><xref rid="pone.0059795-Jackson1" ref-type="bibr"><offsets xml_i="8687" xml_f="8690" txt_i="1740" txt_f="1743">[3]</offsets></xref><offsets xml_i="8697" xml_f="8698" txt_i="1743" txt_f="1744">.</offsets></p><p><offsets xml_i="8705" xml_f="9029" txt_i="1745" txt_f="2069">Molecular biology is one such subject. New and increasingly affordable measurement technologies such as microarrays have led to an explosion of high-quality data for transcriptomics, proteomics and metabolomics. These data are generally high-dimensional and are often time-courses rather than single time point measurements.</offsets></p><p><offsets xml_i="9036" xml_f="9217" txt_i="2070" txt_f="2251">It is well-established that clustering genes on the basis of expression time series profiles can identify genes that are likely to be co-regulated by the same transcription factors </offsets><xref rid="pone.0059795-Eisen1" ref-type="bibr"><offsets xml_i="9265" xml_f="9268" txt_i="2251" txt_f="2254">[4]</offsets></xref><offsets xml_i="9275" xml_f="9411" txt_i="2254" txt_f="2390">. There have been a number of approaches developed to clustering time series, for example using finite or infinite hidden Markov models </offsets><xref rid="pone.0059795-Schliep1" ref-type="bibr"><offsets xml_i="9461" xml_f="9464" txt_i="2390" txt_f="2393">[5]</offsets></xref><offsets xml_i="9471" xml_f="9473" txt_i="2393" txt_f="2395">, </offsets><xref rid="pone.0059795-Beal1" ref-type="bibr"><offsets xml_i="9520" xml_f="9523" txt_i="2395" txt_f="2398">[6]</offsets></xref><offsets xml_i="9530" xml_f="9598" txt_i="2398" txt_f="2466">. Another popular approach is the use of splines as basis functions </offsets><xref rid="pone.0059795-BarJoseph1" ref-type="bibr"><offsets xml_i="9650" xml_f="9653" txt_i="2466" txt_f="2469">[7]</offsets></xref><offsets xml_i="9660" xml_f="9661" txt_i="2469" txt_f="2470">–</offsets><xref rid="pone.0059795-Ma1" ref-type="bibr"><offsets xml_i="9706" xml_f="9710" txt_i="2470" txt_f="2474">[10]</offsets></xref><offsets xml_i="9717" xml_f="9719" txt_i="2474" txt_f="2476">. </offsets><xref rid="pone.0059795-Liverani1" ref-type="bibr"><offsets xml_i="9770" xml_f="9774" txt_i="2476" txt_f="2480">[11]</offsets></xref><offsets xml_i="9781" xml_f="9909" txt_i="2480" txt_f="2608"> also use Fourier series as basis functions. A number of additional methods for time series data analysis have been reviewed by </offsets><xref rid="pone.0059795-BarJoseph2" ref-type="bibr"><offsets xml_i="9961" xml_f="9965" txt_i="2608" txt_f="2612">[12]</offsets></xref><offsets xml_i="9972" xml_f="9973" txt_i="2612" txt_f="2613">.</offsets></p><p><offsets xml_i="9980" xml_f="10306" txt_i="2614" txt_f="2940">These statistical methods often provide superior results to standard clustering algorithms, at the cost of a much greater computational load. This limits the size of data set to which a given method can be applied in a given fixed time frame. Fast implementations of the best statistical methods are therefore highly valuable.</offsets></p><p><offsets xml_i="10313" xml_f="10440" txt_i="2941" txt_f="3068">The Bayesian Hierarchical Clustering (BHC) algorithm has proven a highly successful tool for the clustering of microarray data </offsets><xref rid="pone.0059795-Heller1" ref-type="bibr"><offsets xml_i="10489" xml_f="10493" txt_i="3068" txt_f="3072">[13]</offsets></xref><offsets xml_i="10500" xml_f="10501" txt_i="3072" txt_f="3073">–</offsets><xref rid="pone.0059795-Cooke1" ref-type="bibr"><offsets xml_i="10549" xml_f="10553" txt_i="3073" txt_f="3077">[15]</offsets></xref><offsets xml_i="10560" xml_f="10744" txt_i="3077" txt_f="3261">. The time series BHC method uses Gaussian processes to model time series in a flexible way, making the method highly adaptive and able to handle a wide range of structure in the data.</offsets></p><p><offsets xml_i="10751" xml_f="10900" txt_i="3262" txt_f="3411">The principal downside of the BHC algorithm is its run-time, in particular its scaling with the number of items clustered. This can be addressed via </offsets><italic><offsets xml_i="10908" xml_f="10929" txt_i="3411" txt_f="3432">randomised algorithms</offsets></italic><offsets xml_i="10938" xml_f="10939" txt_i="3432" txt_f="3433">
</offsets><xref rid="pone.0059795-Motwani1" ref-type="bibr"><offsets xml_i="10989" xml_f="10993" txt_i="3433" txt_f="3437">[16]</offsets></xref><offsets xml_i="11000" xml_f="11378" txt_i="3437" txt_f="3815">, a class of techniques that can be highly powerful in this regard. Randomised algorithms employ a degree of randomness as part of their logic, aiming to achieve good average case performance with high probability. Because the requirement for guaranteeing a certain (e.g. optimal) result is relaxed, it is often possible to obtain significantly improved performance as a result.</offsets></p><p><offsets xml_i="11385" xml_f="11425" txt_i="3816" txt_f="3856">In this paper, we apply the approach of </offsets><xref rid="pone.0059795-Heller2" ref-type="bibr"><offsets xml_i="11474" xml_f="11478" txt_i="3856" txt_f="3860">[17]</offsets></xref><offsets xml_i="11485" xml_f="11716" txt_i="3860" txt_f="4091"> to create a randomised BHC algorithm for clustering microarray time series. This allows much larger time series data sets to be analysed in a given amount of time, substantially extending the utility of the time series BHC method.</offsets></p></sec><sec id="s2"><title><offsets xml_i="11746" xml_f="11753" txt_i="4093" txt_f="4100">Results</offsets></title><sec id="s2a"><title><offsets xml_i="11782" xml_f="11804" txt_i="4101" txt_f="4123">Synthetic Data Results</offsets></title><p><offsets xml_i="11815" xml_f="12007" txt_i="4124" txt_f="4316">To demonstrate the effectiveness of the randomised BHC algorithm, we test its performance on a realistic synthetic data set. We use synthetic data constructed from several realisations of the </offsets><italic><offsets xml_i="12015" xml_f="12028" txt_i="4316" txt_f="4329">S. cerevisiae</offsets></italic><offsets xml_i="12037" xml_f="12066" txt_i="4329" txt_f="4358"> synthetic data generated in </offsets><xref rid="pone.0059795-Cooke1" ref-type="bibr"><offsets xml_i="12114" xml_f="12118" txt_i="4358" txt_f="4362">[15]</offsets></xref><offsets xml_i="12125" xml_f="12308" txt_i="4362" txt_f="4545">. Using the fact that Gaussian processes are generative models, we draw random realisations from the BHC model obtained on a 169-gene subset of the cell cycle gene expression data of </offsets><xref rid="pone.0059795-Cho1" ref-type="bibr"><offsets xml_i="12354" xml_f="12358" txt_i="4545" txt_f="4549">[18]</offsets></xref><offsets xml_i="12365" xml_f="12433" txt_i="4549" txt_f="4617">, to give a total of 1000 genes, spread across 13 distinct clusters.</offsets></p><p><offsets xml_i="12440" xml_f="12580" txt_i="4618" txt_f="4758">Given that for these synthetic data we know the ground truth clustering partition, we use the adjusted Rand index as our performance metric </offsets><xref rid="pone.0059795-Hubert1" ref-type="bibr"><offsets xml_i="12629" xml_f="12633" txt_i="4758" txt_f="4762">[19]</offsets></xref><offsets xml_i="12640" xml_f="12641" txt_i="4762" txt_f="4763">.</offsets></p><p><offsets xml_i="12648" xml_f="12649" txt_i="4764" txt_f="4765">
</offsets><xref ref-type="fig" rid="pone-0059795-g001"><offsets xml_i="12694" xml_f="12702" txt_i="4765" txt_f="4773">Figure 1</offsets></xref><offsets xml_i="12709" xml_f="12813" txt_i="4773" txt_f="4877"> shows how the adjusted Rand index (averaged over runs) varies with the randomised algorithm parameter, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e001.jpg"></inline-graphic></inline-formula><offsets xml_i="12914" xml_f="12920" txt_i="4877" txt_f="4883">. For </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e002.jpg"></inline-graphic></inline-formula><offsets xml_i="13021" xml_f="13079" txt_i="4883" txt_f="4941">, there is some loss in accuracy performance; however for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e003.jpg"></inline-graphic></inline-formula><offsets xml_i="13180" xml_f="13252" txt_i="4941" txt_f="5013">, the adjusted Rand index is approximately that of the greedy algorithm.</offsets></p><fig id="pone-0059795-g001" orientation="portrait" position="float"><object-id pub-id-type="doi"><offsets xml_i="13353" xml_f="13386" txt_i="5014" txt_f="5047">10.1371/journal.pone.0059795.g001</offsets></object-id><label><offsets xml_i="13405" xml_f="13413" txt_i="5047" txt_f="5055">Figure 1</offsets></label><caption><title><offsets xml_i="13437" xml_f="13488" txt_i="5055" txt_f="5106">Adjusted Rand index scores for different values of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e004.jpg"></inline-graphic></inline-formula><offsets xml_i="13589" xml_f="13624" txt_i="5106" txt_f="5141">, analysing the synthetic data set.</offsets></title><p><offsets xml_i="13635" xml_f="13802" txt_i="5142" txt_f="5309">Each point is the average of 10 runs, with the error bars denoting the standard error on the mean. The horizontal dashed line shows the result for the full BHC method.</offsets></p></caption><graphic xlink:href="pone.0059795.g001"></graphic></fig><p><offsets xml_i="13875" xml_f="13876" txt_i="5310" txt_f="5311">
</offsets><xref ref-type="fig" rid="pone-0059795-g002"><offsets xml_i="13921" xml_f="13929" txt_i="5311" txt_f="5319">Figure 2</offsets></xref><offsets xml_i="13936" xml_f="14037" txt_i="5319" txt_f="5420"> shows the corresponding run-time performance. As expected, the algorithm is approximately linear in </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e005.jpg"></inline-graphic></inline-formula><offsets xml_i="14138" xml_f="14260" txt_i="5420" txt_f="5542"> and a significant speed-up can be obtained over the greedy algorithm. For these synthetic data, one could therefore pick </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e006.jpg"></inline-graphic></inline-formula><offsets xml_i="14361" xml_f="14455" txt_i="5542" txt_f="5636"> and get approximately the same performance as for the greedy algorithm, but with more than a </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e007.jpg"></inline-graphic></inline-formula><offsets xml_i="14556" xml_f="14784" txt_i="5636" txt_f="5864"> speed-up. And if some performance drop-off was acceptable, as much as an order of magnitude improvement is possible. We note that such a run takes only approximately 5 hours to complete on a single node 2.40 GHz Intel Xeon CPU.</offsets></p><fig id="pone-0059795-g002" orientation="portrait" position="float"><object-id pub-id-type="doi"><offsets xml_i="14885" xml_f="14918" txt_i="5865" txt_f="5898">10.1371/journal.pone.0059795.g002</offsets></object-id><label><offsets xml_i="14937" xml_f="14945" txt_i="5898" txt_f="5906">Figure 2</offsets></label><caption><title><offsets xml_i="14969" xml_f="15003" txt_i="5906" txt_f="5940">Run-times for different values of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e008.jpg"></inline-graphic></inline-formula><offsets xml_i="15104" xml_f="15139" txt_i="5940" txt_f="5975">, analysing the synthetic data set.</offsets></title><p><offsets xml_i="15150" xml_f="15317" txt_i="5976" txt_f="6143">Each point is the average of 10 runs, with the error bars denoting the standard error on the mean. The horizontal dashed line shows the result for the full BHC method.</offsets></p></caption><graphic xlink:href="pone.0059795.g002"></graphic></fig><p><offsets xml_i="15390" xml_f="15484" txt_i="6144" txt_f="6238">We also consider how the run-time varies as a function of the total number of genes analysed, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e009.jpg"></inline-graphic></inline-formula><offsets xml_i="15585" xml_f="15587" txt_i="6238" txt_f="6240">. </offsets><xref ref-type="fig" rid="pone-0059795-g003"><offsets xml_i="15632" xml_f="15640" txt_i="6240" txt_f="6248">Figure 3</offsets></xref><offsets xml_i="15647" xml_f="15691" txt_i="6248" txt_f="6292"> shows this variation for several different </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e010.jpg"></inline-graphic></inline-formula><offsets xml_i="15792" xml_f="15801" txt_i="6292" txt_f="6301"> values. </offsets><xref ref-type="fig" rid="pone-0059795-g004"><offsets xml_i="15846" xml_f="15854" txt_i="6301" txt_f="6309">Figure 4</offsets></xref><offsets xml_i="15861" xml_f="15939" txt_i="6309" txt_f="6387"> shows the same information, expressed a a speed-up over the greedy algorithm.</offsets></p><fig id="pone-0059795-g003" orientation="portrait" position="float"><object-id pub-id-type="doi"><offsets xml_i="16040" xml_f="16073" txt_i="6388" txt_f="6421">10.1371/journal.pone.0059795.g003</offsets></object-id><label><offsets xml_i="16092" xml_f="16100" txt_i="6421" txt_f="6429">Figure 3</offsets></label><caption><title><offsets xml_i="16124" xml_f="16171" txt_i="6429" txt_f="6476">Run-time as a function of the number of genes, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e011.jpg"></inline-graphic></inline-formula><offsets xml_i="16272" xml_f="16312" txt_i="6476" txt_f="6516">, using (subsets of) the synthetic data.</offsets></title><p><offsets xml_i="16323" xml_f="16349" txt_i="6517" txt_f="6543">Shown are the results for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e012.jpg"></inline-graphic></inline-formula><offsets xml_i="16450" xml_f="16458" txt_i="6543" txt_f="6551"> (red), </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e013.jpg"></inline-graphic></inline-formula><offsets xml_i="16559" xml_f="16572" txt_i="6551" txt_f="6564"> (green) and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e014.jpg"></inline-graphic></inline-formula><offsets xml_i="16673" xml_f="16725" txt_i="6564" txt_f="6616"> (blue), as well as for the full BHC method (black).</offsets></p></caption><graphic xlink:href="pone.0059795.g003"></graphic></fig><fig id="pone-0059795-g004" orientation="portrait" position="float"><object-id pub-id-type="doi"><offsets xml_i="16892" xml_f="16925" txt_i="6617" txt_f="6650">10.1371/journal.pone.0059795.g004</offsets></object-id><label><offsets xml_i="16944" xml_f="16952" txt_i="6650" txt_f="6658">Figure 4</offsets></label><caption><title><offsets xml_i="16976" xml_f="17030" txt_i="6658" txt_f="6712">Speed up factor as a function of the number of genes, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e015.jpg"></inline-graphic></inline-formula><offsets xml_i="17131" xml_f="17204" txt_i="6712" txt_f="6785">, relative to the full BHC method, using (subsets of) the synthetic data.</offsets></title><p><offsets xml_i="17215" xml_f="17241" txt_i="6786" txt_f="6812">Shown are the results for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e016.jpg"></inline-graphic></inline-formula><offsets xml_i="17342" xml_f="17350" txt_i="6812" txt_f="6820"> (red), </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e017.jpg"></inline-graphic></inline-formula><offsets xml_i="17451" xml_f="17464" txt_i="6820" txt_f="6833"> (green) and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e018.jpg"></inline-graphic></inline-formula><offsets xml_i="17565" xml_f="17627" txt_i="6833" txt_f="6895"> (blue). The horizontal dashed line shows the full BHC result.</offsets></p></caption><graphic xlink:href="pone.0059795.g004"></graphic></fig><p><offsets xml_i="17700" xml_f="17754" txt_i="6896" txt_f="6950">We note an interesting effect for the lowest value of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e019.jpg"></inline-graphic></inline-formula><offsets xml_i="17855" xml_f="17857" txt_i="6950" txt_f="6952"> (</offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e020.jpg"></inline-graphic></inline-formula><offsets xml_i="17958" xml_f="17963" txt_i="6952" txt_f="6957">) in </offsets><xref ref-type="fig" rid="pone-0059795-g001"><offsets xml_i="18008" xml_f="18016" txt_i="6957" txt_f="6965">Figure 1</offsets></xref><offsets xml_i="18023" xml_f="18085" txt_i="6965" txt_f="7027">. A significant part of the performance degradation for lower </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e021.jpg"></inline-graphic></inline-formula><offsets xml_i="18186" xml_f="18197" txt_i="7027" txt_f="7038"> values in </offsets><xref ref-type="fig" rid="pone-0059795-g001"><offsets xml_i="18242" xml_f="18250" txt_i="7038" txt_f="7046">Figure 1</offsets></xref><offsets xml_i="18257" xml_f="18429" txt_i="7046" txt_f="7218"> comes from the randomised algorithm over-estimating the number of clusters (these being synthetic data, we know the ground truth number of clusters). Investigation of the </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e022.jpg"></inline-graphic></inline-formula><offsets xml_i="18530" xml_f="18605" txt_i="7218" txt_f="7293"> point shows that this effect is lessened for the synthetic data for small </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e023.jpg"></inline-graphic></inline-formula><offsets xml_i="18706" xml_f="18969" txt_i="7293" txt_f="7556">. We believe that this is because for small numbers of data items, the inferred noise level is more weakly constrained. This in turn allows for clusters with higher noise levels, meaning the algorithm can explain the data using a smaller number of noisy clusters.</offsets></p></sec><sec id="s2b"><title><offsets xml_i="19000" xml_f="19018" txt_i="7558" txt_f="7576">Microarray Results</offsets></title><p><offsets xml_i="19029" xml_f="19155" txt_i="7577" txt_f="7703">It is also important to validate the randomised algorithm on real microarray data. To do this, we use a subset of the data of </offsets><xref rid="pone.0059795-Cho1" ref-type="bibr"><offsets xml_i="19201" xml_f="19205" txt_i="7703" txt_f="7707">[18]</offsets></xref><offsets xml_i="19212" xml_f="19328" txt_i="7707" txt_f="7823">, selecting genes that have a KEGG pathway annotation, using the version of the KEGG database to match that used in </offsets><xref rid="pone.0059795-Savage2" ref-type="bibr"><offsets xml_i="19377" xml_f="19381" txt_i="7823" txt_f="7827">[20]</offsets></xref><offsets xml_i="19388" xml_f="19490" txt_i="7827" txt_f="7929">. This consists of yeast cell cycle microarray time series for 1165 genes, measured at 17 time points.</offsets></p><p><offsets xml_i="19497" xml_f="19571" txt_i="7930" txt_f="8004">As a performance metric, we choose the Biological Homogeneity Index (BHI) </offsets><xref rid="pone.0059795-Datta1" ref-type="bibr"><offsets xml_i="19619" xml_f="19623" txt_i="8004" txt_f="8008">[21]</offsets></xref><offsets xml_i="19630" xml_f="19672" txt_i="8008" txt_f="8050">, as implemented in the R package clValid </offsets><xref rid="pone.0059795-Brock1" ref-type="bibr"><offsets xml_i="19720" xml_f="19724" txt_i="8050" txt_f="8054">[22]</offsets></xref><offsets xml_i="19731" xml_f="20019" txt_i="8054" txt_f="8342">. The BHI metric scores a clustering partition between 0 and 1, with higher scores assigned to more biologically homogeneous partitions with respect to a reference annotation set. This has proven to be an effective metric for measuring the performance of microarray-based gene clustering </offsets><xref rid="pone.0059795-Savage1" ref-type="bibr"><offsets xml_i="20068" xml_f="20072" txt_i="8342" txt_f="8346">[14]</offsets></xref><offsets xml_i="20079" xml_f="20081" txt_i="8346" txt_f="8348">, </offsets><xref rid="pone.0059795-Cooke1" ref-type="bibr"><offsets xml_i="20129" xml_f="20133" txt_i="8348" txt_f="8352">[15]</offsets></xref><offsets xml_i="20140" xml_f="20141" txt_i="8352" txt_f="8353">.</offsets></p><p><offsets xml_i="20148" xml_f="20149" txt_i="8354" txt_f="8355">
</offsets><xref ref-type="fig" rid="pone-0059795-g005"><offsets xml_i="20194" xml_f="20202" txt_i="8355" txt_f="8363">Figure 5</offsets></xref><offsets xml_i="20209" xml_f="20308" txt_i="8363" txt_f="8462"> shows the BHI scores (averaged over 10 runs) as a function of the randomised-algorithm parameter, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e024.jpg"></inline-graphic></inline-formula><offsets xml_i="20409" xml_f="20457" txt_i="8462" txt_f="8510">. The BHI scores show very little variation for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e025.jpg"></inline-graphic></inline-formula><offsets xml_i="20558" xml_f="20643" txt_i="8510" txt_f="8595">, showing that the randomised algorithm is highly robust, in this case, to choice of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e026.jpg"></inline-graphic></inline-formula><offsets xml_i="20744" xml_f="20826" txt_i="8595" txt_f="8677">. There is typically a small drop in performance relative to the greedy algorithm.</offsets></p><fig id="pone-0059795-g005" orientation="portrait" position="float"><object-id pub-id-type="doi"><offsets xml_i="20927" xml_f="20960" txt_i="8678" txt_f="8711">10.1371/journal.pone.0059795.g005</offsets></object-id><label><offsets xml_i="20979" xml_f="20987" txt_i="8711" txt_f="8719">Figure 5</offsets></label><caption><title><offsets xml_i="21011" xml_f="21047" txt_i="8719" txt_f="8755">BHI scores for difference values of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e027.jpg"></inline-graphic></inline-formula><offsets xml_i="21148" xml_f="21190" txt_i="8755" txt_f="8797">, analysing the yeast microarray data set.</offsets></title><p><offsets xml_i="21201" xml_f="21627" txt_i="8798" txt_f="9224">Each point is the average of 10 runs, with the error bars denoting the standard error on the mean. The horizontal dashed line shows the results for the full BHC method. Shown are the results for the different gene ontologies, Biological Process (red), Molecular Function (green), Cellular Component (blue) and the logical-OR of all three (black). The BHI scores were all generated using the org.Sc.sgd.db annotation R package.</offsets></p></caption><graphic xlink:href="pone.0059795.g005"></graphic></fig><p><offsets xml_i="21700" xml_f="21701" txt_i="9225" txt_f="9226">
</offsets><xref ref-type="fig" rid="pone-0059795-g006"><offsets xml_i="21746" xml_f="21754" txt_i="9226" txt_f="9234">Figure 6</offsets></xref><offsets xml_i="21761" xml_f="21845" txt_i="9234" txt_f="9318"> shows the corresponding run-times. As with the synthetic data, we see the expected </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e028.jpg"></inline-graphic></inline-formula><offsets xml_i="21946" xml_f="22030" txt_i="9318" txt_f="9402"> scaling. We note that here the overhead of the randomised algorithm means that for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e029.jpg"></inline-graphic></inline-formula><offsets xml_i="22131" xml_f="22201" txt_i="9402" txt_f="9472"> the greedy algorithm is actually faster. However, the BHI results in </offsets><xref ref-type="fig" rid="pone-0059795-g005"><offsets xml_i="22246" xml_f="22254" txt_i="9472" txt_f="9480">Figure 5</offsets></xref><offsets xml_i="22261" xml_f="22285" txt_i="9480" txt_f="9504"> show that we could set </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e030.jpg"></inline-graphic></inline-formula><offsets xml_i="22386" xml_f="22588" txt_i="9504" txt_f="9706"> and gain almost a factor of 3 in speed while incurring only a minimal loss of performance. We note that such a run takes only approximately 2 hours to complete on a single node 2.40 GHz Intel Xeon CPU.</offsets></p><fig id="pone-0059795-g006" orientation="portrait" position="float"><object-id pub-id-type="doi"><offsets xml_i="22689" xml_f="22722" txt_i="9707" txt_f="9740">10.1371/journal.pone.0059795.g006</offsets></object-id><label><offsets xml_i="22741" xml_f="22749" txt_i="9740" txt_f="9748">Figure 6</offsets></label><caption><title><offsets xml_i="22773" xml_f="22807" txt_i="9748" txt_f="9782">Run-times for different values of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e031.jpg"></inline-graphic></inline-formula><offsets xml_i="22908" xml_f="22950" txt_i="9782" txt_f="9824">, analysing the yeast microarray data set.</offsets></title><p><offsets xml_i="22961" xml_f="23129" txt_i="9825" txt_f="9993">Each point is the average of 10 runs, with the error bars denoting the standard error on the mean. The horizontal dashed line shows the results for the full BHC method.</offsets></p></caption><graphic xlink:href="pone.0059795.g006"></graphic></fig><p><offsets xml_i="23202" xml_f="23244" txt_i="9994" txt_f="10036">We note an interesting difference between </offsets><xref ref-type="fig" rid="pone-0059795-g002"><offsets xml_i="23289" xml_f="23298" txt_i="10036" txt_f="10045">Figures 2</offsets></xref><offsets xml_i="23305" xml_f="23310" txt_i="10045" txt_f="10050"> and </offsets><xref ref-type="fig" rid="pone-0059795-g006"><offsets xml_i="23355" xml_f="23356" txt_i="10050" txt_f="10051">6</offsets></xref><offsets xml_i="23363" xml_f="24138" txt_i="10051" txt_f="10826"> in run time, relative to the greedy BHC algorithm. Because the number of genes is similar in both cases, one might expect the performance relative to the greedy algorithm to be similar. However (as is shown in these figures) the efficiency of the randomised BHC algorithm depends on how balanced (or otherwise) the dendrogram is. For example, if many levels of the dendrogram split into subsets of very different sizes (one big, one small), the randomised algorithm may have to go through many iterations in order to define the entire dendrogram. The run time is therefore dependent not only on the number of genes and time points, but also on the underlying clustering structure in the data. Essentially, unbalanced dendrograms make the randomised algorithm less efficient.</offsets></p><p><offsets xml_i="24145" xml_f="24167" txt_i="10827" txt_f="10849">We also note that for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e032.jpg"></inline-graphic></inline-formula><offsets xml_i="24268" xml_f="24691" txt_i="10849" txt_f="11272"> values close to the actual number of genes, it is a general feature that randomised BHC will tend to be slower than the greedy algorithm. This is because in this case, the randomised algorithm has to perform a greedy run with almost the entire set of genes to define the top branching of the dendrogram, and then assign all the genes to one of the two branches and run the greedy algorithm again for each of these subsets.</offsets></p><p><offsets xml_i="24698" xml_f="24699" txt_i="11273" txt_f="11274">
</offsets><xref ref-type="fig" rid="pone-0059795-g002"><offsets xml_i="24744" xml_f="24753" txt_i="11274" txt_f="11283">Figures 2</offsets></xref><offsets xml_i="24760" xml_f="24765" txt_i="11283" txt_f="11288"> and </offsets><xref ref-type="fig" rid="pone-0059795-g006"><offsets xml_i="24810" xml_f="24811" txt_i="11288" txt_f="11289">6</offsets></xref><offsets xml_i="24818" xml_f="24866" txt_i="11289" txt_f="11337"> show an increased variance in the run time for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e033.jpg"></inline-graphic></inline-formula><offsets xml_i="24967" xml_f="25131" txt_i="11337" txt_f="11501">. We believe this effect is due to the fact that for higher â€∼mâ€™ values, the run time is more likely to be dominated by a single run of the greedy algorithm for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e034.jpg"></inline-graphic></inline-formula><offsets xml_i="25232" xml_f="25286" txt_i="11501" txt_f="11555"> items. This will make the run time very sensitive to </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e035.jpg"></inline-graphic></inline-formula><offsets xml_i="25387" xml_f="25458" txt_i="11555" txt_f="11626">, which will be affected by the randomisation of the overall algorithm.</offsets></p></sec></sec><sec id="s3"><title><offsets xml_i="25494" xml_f="25504" txt_i="11629" txt_f="11639">Discussion</offsets></title><p><offsets xml_i="25515" xml_f="25690" txt_i="11640" txt_f="11815">We have presented a randomised algorithm for the BHC clustering method. The randomised algorithm is statistically well-motivated and leads to a number of concrete conclusions.</offsets></p><list list-type="bullet"><list-item><p><offsets xml_i="25733" xml_f="25837" txt_i="11816" txt_f="11920">The randomised BHC algorithm can be used to obtain a substantial speed-up over the greedy BHC algorithm.</offsets></p></list-item><list-item><p><offsets xml_i="25867" xml_f="25968" txt_i="11921" txt_f="12022">Substantial speed-up can be obtained at only small cost to the statistical performance of the method.</offsets></p></list-item><list-item><p><offsets xml_i="25998" xml_f="26070" txt_i="12023" txt_f="12095">The overall computational complexity of the randomised BHC algorithm is </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e036.jpg"></inline-graphic></inline-formula><offsets xml_i="26171" xml_f="26172" txt_i="12095" txt_f="12096">.</offsets></p></list-item></list><p><offsets xml_i="26198" xml_f="26298" txt_i="12097" txt_f="12197">The randomised BHC time series algorithm can therefore be used on data sets of well over 1000 genes.</offsets></p><p><offsets xml_i="26305" xml_f="26377" txt_i="12198" txt_f="12270">Use of the randomised BHC algorithm requires the user to set a value of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e037.jpg"></inline-graphic></inline-formula><offsets xml_i="26478" xml_f="26563" txt_i="12270" txt_f="12355">. On the basis of the analyses presented in this paper, we recommend that a value of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e038.jpg"></inline-graphic></inline-formula><offsets xml_i="26664" xml_f="26678" txt_i="12355" txt_f="12369"> in the range </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e039.jpg"></inline-graphic></inline-formula><offsets xml_i="26779" xml_f="26877" txt_i="12369" txt_f="12467"> is reasonable, giving significant speed-up with minimal cost in terms of statistical performance.</offsets></p><p><offsets xml_i="26884" xml_f="26963" txt_i="12468" txt_f="12547">The randomised time series BHC algorithm is available as part of the R package </offsets><italic><offsets xml_i="26971" xml_f="26974" txt_i="12547" txt_f="12550">BHC</offsets></italic><offsets xml_i="26983" xml_f="27064" txt_i="12550" txt_f="12631">, which is available for download from Bioconductor (version 2.10 and above) via </offsets><ext-link ext-link-type="uri" xlink:href="http://bioconductor.org/packages/2.10/bioc/html/BHC.html"><offsets xml_i="27164" xml_f="27220" txt_i="12631" txt_f="12687">http://bioconductor.org/packages/2.10/bioc/html/BHC.html</offsets></ext-link><offsets xml_i="27231" xml_f="27232" txt_i="12687" txt_f="12688">.</offsets></p><p><offsets xml_i="27239" xml_f="27401" txt_i="12689" txt_f="12851">We have also made available a set of R scripts which can be used to reproduce the analyses carried out in this paper. These are available from the following URL. </offsets><ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/randomisedbhc/"><offsets xml_i="27489" xml_f="27533" txt_i="12851" txt_f="12895">https://sites.google.com/site/randomisedbhc/</offsets></ext-link><offsets xml_i="27544" xml_f="27545" txt_i="12895" txt_f="12896">.</offsets></p></sec><sec sec-type="methods" id="s4"><title><offsets xml_i="27594" xml_f="27601" txt_i="12898" txt_f="12905">Methods</offsets></title><p><offsets xml_i="27612" xml_f="27729" txt_i="12906" txt_f="13023">In this section, we provide a mathematical overview of the time series BHC algorithm. Greater detail can be found in </offsets><xref rid="pone.0059795-Cooke1" ref-type="bibr"><offsets xml_i="27777" xml_f="27781" txt_i="13023" txt_f="13027">[15]</offsets></xref><offsets xml_i="27788" xml_f="28164" txt_i="13027" txt_f="13403">. time series BHC combines the BHC clustering algorithm, coupled with a Gaussian process data model to provide a flexible, generative representation of microarray time series. Here we replace the standard (greedy) BHC algorithm with a randomised algorithm, improving the computational complexity of the method and hence its run time for scientifically-useful numbers of genes.</offsets></p><sec id="s4a"><title><offsets xml_i="28189" xml_f="28202" txt_i="13404" txt_f="13417">BHC Algorithm</offsets></title><p><offsets xml_i="28213" xml_f="28231" txt_i="13418" txt_f="13436">The BHC algorithm </offsets><xref rid="pone.0059795-Heller1" ref-type="bibr"><offsets xml_i="28280" xml_f="28284" txt_i="13436" txt_f="13440">[13]</offsets></xref><offsets xml_i="28291" xml_f="28292" txt_i="13440" txt_f="13441">–</offsets><xref rid="pone.0059795-Cooke1" ref-type="bibr"><offsets xml_i="28340" xml_f="28344" txt_i="13441" txt_f="13445">[15]</offsets></xref><offsets xml_i="28351" xml_f="28353" txt_i="13445" txt_f="13447">, </offsets><xref rid="pone.0059795-Xu1" ref-type="bibr"><offsets xml_i="28398" xml_f="28402" txt_i="13447" txt_f="13451">[23]</offsets></xref><offsets xml_i="28409" xml_f="28930" txt_i="13451" txt_f="13972"> performs agglomerative hierarchical clustering in a Bayesian setting. In agglomerative clustering algorithms, each gene begins in its own cluster and at each stage the two most similar clusters are merged. BHC uses a model-based criterion to do this, also learning the most likely number of clusters given the data (something which many clustering methods are unable to do in a principled way). We note that the BHC algorithm can be interpreted as a fast approximate inference method for a Dirichlet Process Model (DPM) </offsets><xref rid="pone.0059795-Heller1" ref-type="bibr"><offsets xml_i="28979" xml_f="28983" txt_i="13972" txt_f="13976">[13]</offsets></xref><offsets xml_i="28990" xml_f="28991" txt_i="13976" txt_f="13977">.</offsets></p><p><offsets xml_i="28998" xml_f="29021" txt_i="13978" txt_f="14001">The prior probability, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e040.jpg"></inline-graphic></inline-formula><offsets xml_i="29122" xml_f="29155" txt_i="14001" txt_f="14034">, that a given pair of clusters, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e041.jpg"></inline-graphic></inline-formula><offsets xml_i="29256" xml_f="29261" txt_i="14034" txt_f="14039"> and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e042.jpg"></inline-graphic></inline-formula><offsets xml_i="29362" xml_f="29607" txt_i="14039" txt_f="14284">, should be merged is defined by the DPM and is determined solely by the concentration hyperparameter for the DPM and the number of genes currently in each partition of the clustering. Bayes' rule is then used to find the posterior probability, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e043.jpg"></inline-graphic></inline-formula><offsets xml_i="29708" xml_f="29753" txt_i="14284" txt_f="14329">, that the pair of clusters should be merged,</offsets><disp-formula id="pone.0059795.e044"><graphic xlink:href="pone.0059795.e044.jpg" position="anchor" orientation="portrait"></graphic><label><offsets xml_i="29892" xml_f="29895" txt_i="14329" txt_f="14332">(1)</offsets></label></disp-formula><offsets xml_i="29918" xml_f="29924" txt_i="14332" txt_f="14338">where </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e045.jpg"></inline-graphic></inline-formula><offsets xml_i="30025" xml_f="30040" txt_i="14338" txt_f="14353"> is the set of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e046.jpg"></inline-graphic></inline-formula><offsets xml_i="30141" xml_f="30176" txt_i="14353" txt_f="14388"> data points contained in clusters </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e047.jpg"></inline-graphic></inline-formula><offsets xml_i="30277" xml_f="30282" txt_i="14388" txt_f="14393"> and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e048.jpg"></inline-graphic></inline-formula><offsets xml_i="30383" xml_f="30385" txt_i="14393" txt_f="14395">. </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e049.jpg"></inline-graphic></inline-formula><offsets xml_i="30486" xml_f="30548" txt_i="14395" txt_f="14457"> is the marginal likelihood of the data given the hypothesis, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e050.jpg"></inline-graphic></inline-formula><offsets xml_i="30649" xml_f="30665" txt_i="14457" txt_f="14473">, that the data </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e051.jpg"></inline-graphic></inline-formula><offsets xml_i="30766" xml_f="30851" txt_i="14473" txt_f="14558"> belong to a single cluster and requires the specification of a likelihood function, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e052.jpg"></inline-graphic></inline-formula><offsets xml_i="30952" xml_f="31011" txt_i="14558" txt_f="14617">, as the probabilistic model generating the observed data, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e053.jpg"></inline-graphic></inline-formula><offsets xml_i="31112" xml_f="31114" txt_i="14617" txt_f="14619">. </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e054.jpg"></inline-graphic></inline-formula><offsets xml_i="31215" xml_f="31380" txt_i="14619" txt_f="14784"> is the probability that the data could be partitioned in any way which is consistent with the order of assembly of the current partition and is defined recursively,</offsets><disp-formula id="pone.0059795.e055"><graphic xlink:href="pone.0059795.e055.jpg" position="anchor" orientation="portrait"></graphic><label><offsets xml_i="31519" xml_f="31522" txt_i="14784" txt_f="14787">(2)</offsets></label></disp-formula><offsets xml_i="31545" xml_f="31551" txt_i="14787" txt_f="14793">where </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e056.jpg"></inline-graphic></inline-formula><offsets xml_i="31652" xml_f="31657" txt_i="14793" txt_f="14798"> and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e057.jpg"></inline-graphic></inline-formula><offsets xml_i="31758" xml_f="31824" txt_i="14798" txt_f="14864"> are previously merged clusters containing subsets of the data in </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e058.jpg"></inline-graphic></inline-formula><offsets xml_i="31925" xml_f="31926" txt_i="14864" txt_f="14865">.</offsets></p><p><offsets xml_i="31933" xml_f="31938" txt_i="14866" txt_f="14871">When </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e059.jpg"></inline-graphic></inline-formula><offsets xml_i="32039" xml_f="32126" txt_i="14871" txt_f="14958"> is greater than 0.5, it is more likely that the data points contained in the clusters </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e060.jpg"></inline-graphic></inline-formula><offsets xml_i="32227" xml_f="32232" txt_i="14958" txt_f="14963"> and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e061.jpg"></inline-graphic></inline-formula><offsets xml_i="32333" xml_f="32384" txt_i="14963" txt_f="15014"> were generated from the same underlying function, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e062.jpg"></inline-graphic></inline-formula><offsets xml_i="32485" xml_f="32557" txt_i="15014" txt_f="15086">, than that the data points should belong to two or more clusters. When </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e063.jpg"></inline-graphic></inline-formula><offsets xml_i="32658" xml_f="32793" txt_i="15086" txt_f="15221"> is less than 0.5 for all remaining pairs of clusters, the number of clusters and partitions best described by the data has been found.</offsets></p><p><offsets xml_i="32800" xml_f="33215" txt_i="15222" txt_f="15637">For the purposes of the BHC algorithm, a complete dendrogram is constructed, with at each step the most likely merger being made. This allows us to see the log-probability of mergers in the whole dendrogram, even when this value is very small. To determine the likely number of clusters, given the data, we then cut the dendrogram wherever the probability of merger falls below 0.5 (i.e. non-merger is more likely).</offsets></p><p><offsets xml_i="33222" xml_f="33238" txt_i="15638" txt_f="15654">As described in </offsets><xref rid="pone.0059795-Heller1" ref-type="bibr"><offsets xml_i="33287" xml_f="33291" txt_i="15654" txt_f="15658">[13]</offsets></xref><offsets xml_i="33298" xml_f="33304" txt_i="15658" txt_f="15664">, the </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e064.jpg"></inline-graphic></inline-formula><offsets xml_i="33405" xml_f="33463" txt_i="15664" txt_f="15722"> are dependent on a hyperparameter for the mixture model, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e065.jpg"></inline-graphic></inline-formula><inline-formula><inline-graphic xlink:href="pone.0059795.e066.jpg"></inline-graphic></inline-formula><offsets xml_i="33665" xml_f="33909" txt_i="15722" txt_f="15966"> as a fixed value. This has the effect of setting a prior assumption of only weak clustering. One could learn this parameter as part of the BHC algorithm; we choose to not do this as it will substantially increase the run time of the algorithm.</offsets></p><p><offsets xml_i="33916" xml_f="34000" txt_i="15967" txt_f="16051">The BHC algorithm provides a lower bound of the DP marginal likelihood, as shown in </offsets><xref rid="pone.0059795-Heller1" ref-type="bibr"><offsets xml_i="34049" xml_f="34053" txt_i="16051" txt_f="16055">[13]</offsets></xref><offsets xml_i="34060" xml_f="34200" txt_i="16055" txt_f="16195">. For the randomised algorithm, we note that the lower bound on the DP marginal likelihood is effectively determined using a subset of only </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e067.jpg"></inline-graphic></inline-formula><offsets xml_i="34301" xml_f="34482" txt_i="16195" txt_f="16376"> data items. These lower bounds are used in the usual way to optimise hyperparameters for each potential merger. One could attempt in principle to compute the lower bound using all </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e068.jpg"></inline-graphic></inline-formula><offsets xml_i="34583" xml_f="34687" txt_i="16376" txt_f="16480"> data items. However, this will be computationally intensive and so we do not consider it in this paper.</offsets></p></sec><sec id="s4b"><title><offsets xml_i="34718" xml_f="34745" txt_i="16482" txt_f="16509">Gaussian Process Regression</offsets></title><p><offsets xml_i="34756" xml_f="34936" txt_i="16510" txt_f="16690">Gaussian processes define priors over the space of functions, making them highly suited for use as non-linear regression models. This is highly valuable for microarray time series </offsets><xref rid="pone.0059795-Chu1" ref-type="bibr"><offsets xml_i="34982" xml_f="34986" txt_i="16690" txt_f="16694">[24]</offsets></xref><offsets xml_i="34993" xml_f="34994" txt_i="16694" txt_f="16695">–</offsets><xref rid="pone.0059795-Stegle1" ref-type="bibr"><offsets xml_i="35043" xml_f="35047" txt_i="16695" txt_f="16699">[27]</offsets></xref><offsets xml_i="35054" xml_f="35264" txt_i="16699" txt_f="16909">, where a wide range of functional forms can be expected. In essence, Gaussian Process Regression (GPR) allows us to minimise the assumptions we must make as to the underlying structure in our time series data.</offsets></p><p><offsets xml_i="35271" xml_f="35334" txt_i="16910" txt_f="16973">For the time series BHC model, we model an observation at time </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e069.jpg"></inline-graphic></inline-formula><offsets xml_i="35435" xml_f="35439" txt_i="16973" txt_f="16977"> as </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e070.jpg"></inline-graphic></inline-formula><offsets xml_i="35540" xml_f="35590" txt_i="16977" txt_f="17027">. For each cluster, we assume the latent function </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e071.jpg"></inline-graphic></inline-formula><offsets xml_i="35691" xml_f="35750" txt_i="17027" txt_f="17086"> is drawn from a Gaussian process with covariance function </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e072.jpg"></inline-graphic></inline-formula><offsets xml_i="35851" xml_f="35881" txt_i="17086" txt_f="17116">, defined by hyperparameters, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e073.jpg"></inline-graphic></inline-formula><offsets xml_i="35982" xml_f="35999" txt_i="17116" txt_f="17133">. We also assume </offsets><italic><offsets xml_i="36007" xml_f="36010" txt_i="17133" txt_f="17136">iid</offsets></italic><offsets xml_i="36019" xml_f="36036" txt_i="17136" txt_f="17153"> Gaussian noise, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e074.jpg"></inline-graphic></inline-formula><offsets xml_i="36137" xml_f="36138" txt_i="17153" txt_f="17154">.</offsets></p><p><offsets xml_i="36145" xml_f="36149" txt_i="17155" txt_f="17159">Let </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e075.jpg"></inline-graphic></inline-formula><offsets xml_i="36250" xml_f="36258" txt_i="17159" txt_f="17167"> be the </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e076.jpg"></inline-graphic></inline-formula><offsets xml_i="36359" xml_f="36389" txt_i="17167" txt_f="17197"> observations in a cluster of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e077.jpg"></inline-graphic></inline-formula><offsets xml_i="36490" xml_f="36508" txt_i="17197" txt_f="17215"> genes, where the </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e078.jpg"></inline-graphic></inline-formula><offsets xml_i="36609" xml_f="36629" txt_i="17215" txt_f="17235"> are time series of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e079.jpg"></inline-graphic></inline-formula><offsets xml_i="36730" xml_f="36841" txt_i="17235" txt_f="17346"> time points. Each gene is normalised to have mean 0 and standard deviation 1 across time points. The prior of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e080.jpg"></inline-graphic></inline-formula><offsets xml_i="36942" xml_f="36972" txt_i="17346" txt_f="17376"> is given for fixed values of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e081.jpg"></inline-graphic></inline-formula><offsets xml_i="37073" xml_f="37085" txt_i="17376" txt_f="17388">, such that </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e082.jpg"></inline-graphic></inline-formula><offsets xml_i="37186" xml_f="37232" txt_i="17388" txt_f="17434">. It follows that the likelihood function for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e083.jpg"></inline-graphic></inline-formula><offsets xml_i="37333" xml_f="37337" txt_i="17434" txt_f="17438"> is </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e084.jpg"></inline-graphic></inline-formula><offsets xml_i="37438" xml_f="37446" txt_i="17438" txt_f="17446">, where </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e085.jpg"></inline-graphic></inline-formula><offsets xml_i="37547" xml_f="37555" txt_i="17446" txt_f="17454"> is the </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e086.jpg"></inline-graphic></inline-formula><offsets xml_i="37656" xml_f="37711" txt_i="17454" txt_f="17509"> identity matrix. The marginal likelihood of the data, </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e087.jpg"></inline-graphic></inline-formula><offsets xml_i="37812" xml_f="37822" txt_i="17509" txt_f="17519">, is then:</offsets><disp-formula id="pone.0059795.e088"><graphic xlink:href="pone.0059795.e088.jpg" position="anchor" orientation="portrait"></graphic><label><offsets xml_i="37961" xml_f="37964" txt_i="17519" txt_f="17522">(3)</offsets></label></disp-formula><offsets xml_i="37987" xml_f="37988" txt_i="17522" txt_f="17523">
</offsets><disp-formula id="pone.0059795.e089"><graphic xlink:href="pone.0059795.e089.jpg" position="anchor" orientation="portrait"></graphic><label><offsets xml_i="38127" xml_f="38130" txt_i="17523" txt_f="17526">(4)</offsets></label></disp-formula><offsets xml_i="38153" xml_f="38159" txt_i="17526" txt_f="17532">where </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e090.jpg"></inline-graphic></inline-formula><offsets xml_i="38260" xml_f="38292" txt_i="17532" txt_f="17564"> is the covariance function for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e091.jpg"></inline-graphic></inline-formula><offsets xml_i="38393" xml_f="38394" txt_i="17564" txt_f="17565">.</offsets></p><p><offsets xml_i="38401" xml_f="38594" txt_i="17566" txt_f="17759">Time series BHC implements either the squared exponential or cubic spline covariance functions. In this paper, we restrict our attention to the default choice of squared exponential covariance:</offsets><disp-formula id="pone.0059795.e092"><graphic xlink:href="pone.0059795.e092.jpg" position="anchor" orientation="portrait"></graphic><label><offsets xml_i="38733" xml_f="38736" txt_i="17759" txt_f="17762">(5)</offsets></label></disp-formula><offsets xml_i="38759" xml_f="38765" txt_i="17762" txt_f="17768">where </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e093.jpg"></inline-graphic></inline-formula><offsets xml_i="38866" xml_f="38903" txt_i="17768" txt_f="17805"> is the Kronecker delta function and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e094.jpg"></inline-graphic></inline-formula><offsets xml_i="39004" xml_f="39009" txt_i="17805" txt_f="17810"> and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e095.jpg"></inline-graphic></inline-formula><offsets xml_i="39110" xml_f="39135" txt_i="17810" txt_f="17835"> are two time points for </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e096.jpg"></inline-graphic></inline-formula><offsets xml_i="39236" xml_f="39238" txt_i="17835" txt_f="17837">. </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e097.jpg"></inline-graphic></inline-formula><offsets xml_i="39339" xml_f="39405" txt_i="17837" txt_f="17903"> is the signal variance parameter for the covariance function and </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e098.jpg"></inline-graphic></inline-formula><offsets xml_i="39506" xml_f="39537" txt_i="17903" txt_f="17934"> is the length-scale parameter.</offsets></p></sec><sec id="s4c"><title><offsets xml_i="39568" xml_f="39592" txt_i="17936" txt_f="17960">Randomised BHC Algorithm</offsets></title><p><offsets xml_i="39603" xml_f="39681" txt_i="17961" txt_f="18039">To speed up the time series BHC, we implement the randomised BHC algorithm of </offsets><xref rid="pone.0059795-Heller2" ref-type="bibr"><offsets xml_i="39730" xml_f="39734" txt_i="18039" txt_f="18043">[17]</offsets></xref><offsets xml_i="39741" xml_f="40055" txt_i="18043" txt_f="18357"> (specifically, algorithm 1). The key insight from which we hope to benefit is that the standard, greedy BHC algorithm is dominated by the computation of merges at the lowest level of the tree. Therefore, if we can reduce this load in a sensible way, it may be possible to produce a substantially faster algorithm.</offsets></p><p><offsets xml_i="40062" xml_f="40105" txt_i="18358" txt_f="18401">Throughout this paper we will refer to the </offsets><italic><offsets xml_i="40113" xml_f="40116" txt_i="18401" txt_f="18404">top</offsets></italic><offsets xml_i="40125" xml_f="40245" txt_i="18404" txt_f="18524"> of the dendrogram. This is the highest level of the dendrogram, where the whole set of genes is split into two subsets.</offsets></p><p><offsets xml_i="40252" xml_f="40432" txt_i="18525" txt_f="18705">For reasonably balanced trees, the top levels should be well-defined even using only a random subset of the genes. From this idea, we can define the following randomised algorithm.</offsets></p><list list-type="bullet"><list-item><p><offsets xml_i="40475" xml_f="40494" txt_i="18706" txt_f="18725">Select a subset of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e099.jpg"></inline-graphic></inline-formula><offsets xml_i="40595" xml_f="40602" txt_i="18725" txt_f="18732"> genes.</offsets></p></list-item><list-item><p><offsets xml_i="40632" xml_f="40657" txt_i="18733" txt_f="18758">Run BHC on the subset of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e100.jpg"></inline-graphic></inline-formula><offsets xml_i="40758" xml_f="40765" txt_i="18758" txt_f="18765"> genes.</offsets></p></list-item><list-item><p><offsets xml_i="40795" xml_f="40816" txt_i="18766" txt_f="18787">Filter the remaining </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e101.jpg"></inline-graphic></inline-formula><offsets xml_i="40917" xml_f="41041" txt_i="18787" txt_f="18911"> genes through the top level of the tree, computing merge probabilities between each individual gene and the two subsets of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e102.jpg"></inline-graphic></inline-formula><offsets xml_i="41142" xml_f="41186" txt_i="18911" txt_f="18955"> to decide to which branch the gene belongs.</offsets></p></list-item><list-item><p><offsets xml_i="41216" xml_f="41239" txt_i="18956" txt_f="18979">Including the original </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e103.jpg"></inline-graphic></inline-formula><offsets xml_i="41340" xml_f="41430" txt_i="18979" txt_f="19069"> genes, we have now subdivided all genes on the basis of the top level branch of the tree.</offsets></p></list-item><list-item><p><offsets xml_i="41460" xml_f="41535" txt_i="19070" txt_f="19145">Now recurse for the gene subsets in each branch, until each subset size is </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e104.jpg"></inline-graphic></inline-formula><offsets xml_i="41636" xml_f="41725" txt_i="19145" txt_f="19234">, at which point use the standard BHC algorithm to complete the lower levels of the tree.</offsets></p></list-item></list><p><offsets xml_i="41751" xml_f="41933" txt_i="19235" txt_f="19417">In effect, we are using estimates of the higher levels of the tree to subdivide the genes so that it is not necessary to compute many of the potential low-level merge probabilities. </offsets><xref ref-type="fig" rid="pone-0059795-g007"><offsets xml_i="41978" xml_f="41986" txt_i="19417" txt_f="19425">Figure 7</offsets></xref><offsets xml_i="41993" xml_f="42038" txt_i="19425" txt_f="19470"> shows a flow chart describing the algorithm.</offsets></p><fig id="pone-0059795-g007" orientation="portrait" position="float"><object-id pub-id-type="doi"><offsets xml_i="42139" xml_f="42172" txt_i="19471" txt_f="19504">10.1371/journal.pone.0059795.g007</offsets></object-id><label><offsets xml_i="42191" xml_f="42199" txt_i="19504" txt_f="19512">Figure 7</offsets></label><caption><title><offsets xml_i="42223" xml_f="42271" txt_i="19512" txt_f="19560">Flow chart showing the randomised BHC algorithm.</offsets></title><p><offsets xml_i="42282" xml_f="42493" txt_i="19561" txt_f="19772">The main loop is the randomised part of the algorithm, which is used recursively until the remaining gene subsets are small enough that it uses the greedy version of BHC to complete the tree and then terminates.</offsets></p></caption><graphic xlink:href="pone.0059795.g007"></graphic></fig></sec><sec id="s4d"><title><offsets xml_i="42590" xml_f="42617" txt_i="19774" txt_f="19801">Setting the Hyperparameters</offsets></title><p><offsets xml_i="42628" xml_f="42848" txt_i="19802" txt_f="20022">The covariance function of the Gaussian processes used in this paper are characterised by a small number of hyperparameters. These hyperparameters are learned for each potential merger using the BFGS quasi-Newton method </offsets><xref rid="pone.0059795-Flannery1" ref-type="bibr"><offsets xml_i="42899" xml_f="42903" txt_i="20022" txt_f="20026">[28]</offsets></xref><offsets xml_i="42910" xml_f="42911" txt_i="20026" txt_f="20027">.</offsets></p><p><offsets xml_i="42918" xml_f="43144" txt_i="20028" txt_f="20254">This merge-by-merge optimisation allows each cluster to have different hyperparameter values, allowing for example for clusters with different intrinsic noise levels and time series with different characteristic length scales.</offsets></p></sec><sec id="s4e"><title><offsets xml_i="43175" xml_f="43222" txt_i="20256" txt_f="20303">Utilising the Covariance Matrix Block Structure</offsets></title><p><offsets xml_i="43233" xml_f="43484" txt_i="20304" txt_f="20555">We assume in this paper that each time series is sampled at the same set of time points. This leads to a block structure in the covariance matrix, which can be utilised to greatly accelerate the computation of the Gaussian process marginal likelihood.</offsets></p><p><offsets xml_i="43491" xml_f="43614" txt_i="20556" txt_f="20679">The computational complexity of BHC is dominated by inversion of the covariance matrix. Considering the case of a group of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e105.jpg"></inline-graphic></inline-formula><offsets xml_i="43715" xml_f="43748" txt_i="20679" txt_f="20712"> genes, each sampled at the same </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e106.jpg"></inline-graphic></inline-formula><offsets xml_i="43849" xml_f="43931" txt_i="20712" txt_f="20794"> time points, the naive approach to matrix inversion would require us to invert a </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e107.jpg"></inline-graphic></inline-formula><offsets xml_i="44032" xml_f="44053" txt_i="20794" txt_f="20815"> matrix, which is an </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e108.jpg"></inline-graphic></inline-formula><offsets xml_i="44154" xml_f="44318" txt_i="20815" txt_f="20979"> operation. However, we can instead use block matrix pseudoinversion, which recursively reduces the block size to one, at which point the remaining inversion is an </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e109.jpg"></inline-graphic></inline-formula><offsets xml_i="44419" xml_f="44430" txt_i="20979" txt_f="20990"> operation.</offsets></p><p><offsets xml_i="44437" xml_f="44703" txt_i="20991" txt_f="21257">We also note that this is equivalent to a Bayesian analysis using a standard multivariate Gaussian. Indeed, considering the task in this way may be a simpler way of doing so and is certainly a useful way of gaining additional insights into the workings of the model.</offsets></p></sec><sec id="s4f"><title><offsets xml_i="44734" xml_f="44758" txt_i="21259" txt_f="21283">Computational Complexity</offsets></title><p><offsets xml_i="44769" xml_f="44833" txt_i="21284" txt_f="21348">When proposed merges have constant cost (the case considered by </offsets><xref rid="pone.0059795-Heller2" ref-type="bibr"><offsets xml_i="44882" xml_f="44886" txt_i="21348" txt_f="21352">[17]</offsets></xref><offsets xml_i="44893" xml_f="44934" txt_i="21352" txt_f="21393">), the standard greedy BHC algorithm has </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e110.jpg"></inline-graphic></inline-formula><offsets xml_i="45035" xml_f="45061" txt_i="21393" txt_f="21419"> computational complexity.</offsets></p><p><offsets xml_i="45068" xml_f="45187" txt_i="21420" txt_f="21539">For the time series BHC algorithm however, the merges do not have have constant cost. For a given node, we are merging </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e111.jpg"></inline-graphic></inline-formula><offsets xml_i="45288" xml_f="45322" txt_i="21539" txt_f="21573"> gene time series, each of length </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e112.jpg"></inline-graphic></inline-formula><offsets xml_i="45423" xml_f="45457" txt_i="21573" txt_f="21607">. We therefore have to consider a </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e113.jpg"></inline-graphic></inline-formula><offsets xml_i="45558" xml_f="45612" txt_i="21607" txt_f="21661"> covariance matrix, which we must invert. As noted in </offsets><xref rid="pone.0059795-Cooke1" ref-type="bibr"><offsets xml_i="45660" xml_f="45664" txt_i="21661" txt_f="21665">[15]</offsets></xref><offsets xml_i="45671" xml_f="45726" txt_i="21665" txt_f="21720">, this matrix is actually a block matrix consisting of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e114.jpg"></inline-graphic></inline-formula><offsets xml_i="45827" xml_f="45868" txt_i="21720" txt_f="21761"> blocks, which means we can invert it in </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e115.jpg"></inline-graphic></inline-formula><offsets xml_i="45969" xml_f="45981" txt_i="21761" txt_f="21773"> operations.</offsets></p><p><offsets xml_i="45988" xml_f="45996" txt_i="21774" txt_f="21782">Because </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e116.jpg"></inline-graphic></inline-formula><offsets xml_i="46097" xml_f="46118" txt_i="21782" txt_f="21803"> will be as large as </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e117.jpg"></inline-graphic></inline-formula><offsets xml_i="46219" xml_f="46361" txt_i="21803" txt_f="21945"> for the merges closer to the root node of the tree, this gives the greedy time series BHC algorithm a worst-case computational complexity of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e118.jpg"></inline-graphic></inline-formula><offsets xml_i="46462" xml_f="46463" txt_i="21945" txt_f="21946">.</offsets></p><p><offsets xml_i="46470" xml_f="46532" txt_i="21947" txt_f="22009">The randomised algorithm for case of constant cost merges has </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e119.jpg"></inline-graphic></inline-formula><offsets xml_i="46633" xml_f="46645" txt_i="22009" txt_f="22021"> complexity </offsets><xref rid="pone.0059795-Heller2" ref-type="bibr"><offsets xml_i="46694" xml_f="46698" txt_i="22021" txt_f="22025">[17]</offsets></xref><offsets xml_i="46705" xml_f="46835" txt_i="22025" txt_f="22155">). Heller and Ghahramani show that, for reasonably balanced trees, the complexity is dominated by the filtering step. Each of the </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e120.jpg"></inline-graphic></inline-formula><offsets xml_i="46936" xml_f="46956" txt_i="22155" txt_f="22175"> filtering steps is </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e121.jpg"></inline-graphic></inline-formula><offsets xml_i="47057" xml_f="47084" txt_i="22175" txt_f="22202">, resulting in the overall </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e122.jpg"></inline-graphic></inline-formula><offsets xml_i="47185" xml_f="47255" txt_i="22202" txt_f="22272"> complexity. For the time series BHC algorithm, the filtering step is </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e123.jpg"></inline-graphic></inline-formula><offsets xml_i="47356" xml_f="47464" txt_i="22272" txt_f="22380">, because of the additional cost of merging time series clusters. As in the original analysis there will be </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e124.jpg"></inline-graphic></inline-formula><offsets xml_i="47565" xml_f="47675" txt_i="22380" txt_f="22490"> filtering steps, giving an overall computational complexity for the randomised version of time series BHC of </offsets><inline-formula><inline-graphic xlink:href="pone.0059795.e125.jpg"></inline-graphic></inline-formula><offsets xml_i="47776" xml_f="47777" txt_i="22490" txt_f="22491">.</offsets></p></sec></sec></body><back><ack><p>We thank Jim Griffin for useful discussions.</p></ack><ref-list><title>References</title><ref id="pone.0059795-Bauwens1"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Bauwens</surname><given-names>L</given-names></name>, <name><surname>Rombouts</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Bayesian clustering of many garch models</article-title>. <source>Econometric Reviews</source>
<volume>26</volume>: <fpage>365</fpage>–<lpage>386</lpage>.</mixed-citation></ref><ref id="pone.0059795-FrhwirthSchnatter1"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Frühwirth-Schnatter</surname><given-names>S</given-names></name>, <name><surname>Kaufmann</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Model-based clustering of multiple time series</article-title>. <source>Journal of Business and Economic Statistics</source>
<volume>26</volume>: <fpage>78</fpage>–<lpage>89</lpage>.</mixed-citation></ref><ref id="pone.0059795-Jackson1"><label>3</label><mixed-citation publication-type="book">Jackson E, Davy M, Doucet A, Fitzgerald W (2007) Bayesian unsupervised signal classification by Dirichlet process mixtures of Gaussian processes. In: Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, volume 3, pp. III–1077.</mixed-citation></ref><ref id="pone.0059795-Eisen1"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Eisen</surname><given-names>M</given-names></name>, <name><surname>Spellman</surname><given-names>P</given-names></name>, <name><surname>Brown</surname><given-names>P</given-names></name>, <name><surname>Botstein</surname><given-names>D</given-names></name> (<year>1998</year>) <article-title>Cluster Analysis and Display of Genome-wide Expression</article-title>. <source>Proceedings of the National Academy of Sciences</source>
<volume>95</volume>: <fpage>14863</fpage>–<lpage>14868</lpage>.</mixed-citation></ref><ref id="pone.0059795-Schliep1"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Schliep</surname><given-names>A</given-names></name>, <name><surname>Costa</surname><given-names>IG</given-names></name>, <name><surname>Steinhoff</surname><given-names>C</given-names></name>, <name><surname>Schonhuth</surname><given-names>A</given-names></name> (<year>2005</year>) <article-title>Analyzing gene expression time-courses</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source>
<volume>2</volume>: <fpage>179</fpage>–<lpage>193</lpage>.<pub-id pub-id-type="pmid">17044182</pub-id></mixed-citation></ref><ref id="pone.0059795-Beal1"><label>6</label><mixed-citation publication-type="book">Beal M, Krishnamurthy P (2006) Gene Expression Time Course Clustering with Countably Infinite Hidden Markov Models. In: Proceedings of the Proceedings of the Twenty-Second Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-06). Arlington, Virginia: AUAI Press, 23–30.</mixed-citation></ref><ref id="pone.0059795-BarJoseph1"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Bar-Joseph</surname><given-names>Z</given-names></name>, <name><surname>Gerber</surname><given-names>G</given-names></name>, <name><surname>Gifford</surname><given-names>D</given-names></name>, <name><surname>Jaakkola</surname><given-names>T</given-names></name>, <name><surname>Simon</surname><given-names>I</given-names></name> (<year>2003</year>) <article-title>Continuous representations of time-series gene expression data</article-title>. <source>Journal of Computational Biology</source>
<volume>10</volume>: <fpage>341</fpage>–<lpage>356</lpage>.<pub-id pub-id-type="pmid">12935332</pub-id></mixed-citation></ref><ref id="pone.0059795-Heard1"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Heard</surname><given-names>NA</given-names></name>, <name><surname>Holmes</surname><given-names>CC</given-names></name>, <name><surname>Stephens</surname><given-names>DA</given-names></name>, <name><surname>Hand</surname><given-names>DJ</given-names></name>, <name><surname>Dimopoulos</surname><given-names>G</given-names></name> (<year>2005</year>) <article-title>Bayesian coclustering of Anopheles gene expression time series: Study of immune defense response to multiple experimental challenges</article-title>. <source>Proceedings of the National Academy of Sciences</source>
<volume>102</volume>: <fpage>16939</fpage>–<lpage>16944</lpage>.</mixed-citation></ref><ref id="pone.0059795-Heard2"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Heard</surname><given-names>NA</given-names></name>, <name><surname>Holmes</surname><given-names>CC</given-names></name>, <name><surname>Stephens</surname><given-names>DA</given-names></name> (<year>2006</year>) <article-title>A Quantitative Study of Gene Regulation Involved in the Immune Response of Anopheline Mosquitoes: An Application of Bayesian Hierarchical Clustering of Curves</article-title>. <source>Journal of the American Statistical Association</source>
<volume>101</volume>: <fpage>18</fpage>.</mixed-citation></ref><ref id="pone.0059795-Ma1"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Ma</surname><given-names>P</given-names></name>, <name><surname>Castillo-Davis</surname><given-names>CI</given-names></name>, <name><surname>Zhong</surname><given-names>W</given-names></name>, <name><surname>Liu</surname><given-names>JS</given-names></name> (<year>2006</year>) <article-title>A data-driven clustering method for time course gene expression data</article-title>. <source>Nucleic Acids Research</source>
<volume>34</volume>: <fpage>1261</fpage>–<lpage>1269</lpage>.<pub-id pub-id-type="pmid">16510852</pub-id></mixed-citation></ref><ref id="pone.0059795-Liverani1"><label>11</label><mixed-citation publication-type="book">Liverani S, Cussens J, Smith JQ (2010) Searching a Multivariate Partition Space Using MAXSAT. In: Masulli F, Peterson L, Tagliaferri R, editors, Computational Intelligence Methods for Bioinformatics and Biostatistics, 6th International Meeting, CIBB 2009 Genova, Italy, Springer, Heidelberg, volume 6160 of Lecture Notes in Computer Science. 240–253.</mixed-citation></ref><ref id="pone.0059795-BarJoseph2"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Bar-Joseph</surname><given-names>Z</given-names></name> (<year>2004</year>) <article-title>Analyzing time series gene expression data</article-title>. <source>Bioinformatics</source>
<volume>20</volume>: <fpage>2493</fpage>.<pub-id pub-id-type="pmid">15130923</pub-id></mixed-citation></ref><ref id="pone.0059795-Heller1"><label>13</label><mixed-citation publication-type="book">Heller KA, Ghahramani Z (2005) Bayesian Hierarchical Clustering. In: Twenty-second International Conference on Machine Learning (ICML-2005).</mixed-citation></ref><ref id="pone.0059795-Savage1"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Savage</surname><given-names>RS</given-names></name>, <name><surname>Heller</surname><given-names>K</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Ghahramani</surname><given-names>Z</given-names></name>, <name><surname>Truman</surname><given-names>WM</given-names></name>, <etal>et al</etal> (<year>2009</year>) <article-title>R/BHC: Fast Bayesian Hierarchical Clustering for Microarray Data</article-title>. <source>BMC Bioinformatics</source>
<volume>10</volume>: <fpage>242</fpage>.<pub-id pub-id-type="pmid">19660130</pub-id></mixed-citation></ref><ref id="pone.0059795-Cooke1"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Cooke</surname><given-names>E</given-names></name>, <name><surname>Savage</surname><given-names>R</given-names></name>, <name><surname>Kirk</surname><given-names>P</given-names></name>, <name><surname>Darkins</surname><given-names>R</given-names></name>, <name><surname>Wild</surname><given-names>D</given-names></name> (<year>2011</year>) <article-title>Bayesian hierarchical clustering for microarray time series data with replicates and outlier measurements</article-title>. <source>BMC Bioinformatics</source>
<volume>12</volume>: <fpage>399</fpage>.<pub-id pub-id-type="pmid">21995452</pub-id></mixed-citation></ref><ref id="pone.0059795-Motwani1"><label>16</label><mixed-citation publication-type="book">Motwani R, Raghavan P (1995) Randomised Algorithms. Cambridge University Press.</mixed-citation></ref><ref id="pone.0059795-Heller2"><label>17</label><mixed-citation publication-type="journal">
<name><surname>Heller</surname><given-names>K</given-names></name>, <name><surname>Ghahramani</surname><given-names>Z</given-names></name> (<year>2005</year>) <article-title>Randomized algorithms for fast bayesian hierarchical clustering</article-title>. <source>PASCAL Workshop on Statistics and Optimization of Clustering</source>
<volume>25</volume>: <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation></ref><ref id="pone.0059795-Cho1"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Cho</surname><given-names>R</given-names></name>, <name><surname>Campbell</surname><given-names>M</given-names></name>, <name><surname>Steinmetz</surname><given-names>EWL</given-names></name>, <name><surname>Conway</surname><given-names>A</given-names></name>, <name><surname>Wodicka</surname><given-names>L</given-names></name>, <etal>et al</etal> (<year>1998</year>) <article-title>A Genome-Wide Transcriptional Analysis of the Mitotic Cell Cycle</article-title>. <source>Molecular Cell</source>
<volume>2</volume>: <fpage>65</fpage>–<lpage>73</lpage>.<pub-id pub-id-type="pmid">9702192</pub-id></mixed-citation></ref><ref id="pone.0059795-Hubert1"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Hubert</surname><given-names>L</given-names></name>, <name><surname>Arabie</surname><given-names>P</given-names></name> (<year>1985</year>) <article-title>Comparing partitions</article-title>. <source>Journal of the Classification</source>
<volume>2</volume>: <fpage>193</fpage>–<lpage>218</lpage>.</mixed-citation></ref><ref id="pone.0059795-Savage2"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Savage</surname><given-names>RS</given-names></name>, <name><surname>Ghahramani</surname><given-names>Z</given-names></name>, <name><surname>Griffin</surname><given-names>JE</given-names></name>, <name><surname>de la Cruz</surname><given-names>BJ</given-names></name>, <name><surname>Wild</surname><given-names>DL</given-names></name> (<year>2010</year>) <article-title>Discovering Transcriptional Modules by Bayesian Data Integration</article-title>. <source>Bioinformatics</source>
<volume>26</volume>: <fpage>i158</fpage>–<lpage>i167</lpage>.<pub-id pub-id-type="pmid">20529901</pub-id></mixed-citation></ref><ref id="pone.0059795-Datta1"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Datta</surname><given-names>S</given-names></name>, <name><surname>Datta</surname><given-names>S</given-names></name> (<year>2006</year>) <article-title>Methods for evaluating clustering algorithms for gene expression data using a reference set of functional classes</article-title>. <source>BMC Bioinformatics</source>
<volume>7</volume>: <fpage>397</fpage>.<pub-id pub-id-type="pmid">16945146</pub-id></mixed-citation></ref><ref id="pone.0059795-Brock1"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Brock</surname><given-names>G</given-names></name>, <name><surname>Pihur</surname><given-names>V</given-names></name>, <name><surname>Datta</surname><given-names>S</given-names></name>, <name><surname>Datta</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>clValid: An R package for cluster validation</article-title>. <source>Journal of Statistical Software</source>
<volume>25</volume>: <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation></ref><ref id="pone.0059795-Xu1"><label>23</label><mixed-citation publication-type="book">Xu Y, Heller K, Ghahramani Z (2009) Tree-based inference for Dirichlet process mixtures. AISTATS 2009 conference.</mixed-citation></ref><ref id="pone.0059795-Chu1"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Chu</surname><given-names>W</given-names></name>, <name><surname>Ghahramani</surname><given-names>Z</given-names></name>, <name><surname>Falciani</surname><given-names>F</given-names></name>, <name><surname>Wild</surname><given-names>DL</given-names></name> (<year>2005</year>) <article-title>Biomarker discovery in microarray gene expression data with Gaussian processes</article-title>. <source>Bioinformatics</source>
<volume>21</volume>: <fpage>3383</fpage>–<lpage>3393</lpage>.</mixed-citation></ref><ref id="pone.0059795-Kirk1"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Kirk</surname><given-names>PDW</given-names></name>, <name><surname>Stumpf</surname><given-names>MPH</given-names></name> (<year>2009</year>) <article-title>Gaussian process regression bootstrapping: exploring the e_ect of uncertainty in time course data</article-title>. <source>Bioinformatics</source>
<volume>25</volume>: <fpage>1300</fpage>–<lpage>1306</lpage>.<pub-id pub-id-type="pmid">19289448</pub-id></mixed-citation></ref><ref id="pone.0059795-Liu1"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Liu</surname><given-names>Q</given-names></name>, <name><surname>Lin</surname><given-names>K</given-names></name>, <name><surname>Anderson</surname><given-names>B</given-names></name>, <name><surname>Smyth</surname><given-names>P</given-names></name>, <name><surname>Ihler</surname><given-names>A</given-names></name> (<year>2010</year>) <article-title>Estimating replicate time shifts using Gaussian process regression</article-title>. <source>Bioinformatics</source>
<volume>26</volume>: <fpage>770</fpage>–<lpage>776</lpage>.<pub-id pub-id-type="pmid">20147305</pub-id></mixed-citation></ref><ref id="pone.0059795-Stegle1"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Stegle</surname><given-names>O</given-names></name>, <name><surname>Denby</surname><given-names>KJ</given-names></name>, <name><surname>Cooke</surname><given-names>EJ</given-names></name>, <name><surname>Wild</surname><given-names>DL</given-names></name>, <name><surname>Ghahramani</surname><given-names>Z</given-names></name>, <etal>et al</etal> (<year>2010</year>) <article-title>A Robust Bayesian Two-Sample Test for Detecting Intervals of Differential Gene Expression in Microarray Time Series</article-title>. <source>Journal of Computational Biology</source>
<volume>17</volume>: <fpage>355</fpage>–<lpage>367</lpage>.<pub-id pub-id-type="pmid">20377450</pub-id></mixed-citation></ref><ref id="pone.0059795-Flannery1"><label>28</label><mixed-citation publication-type="book">Flannery B, Press W, Teukolsky S, Vetterling W (1992) Numerical recipes in c. Press Syndicate of the University of Cambridge, New York.</mixed-citation></ref></ref-list></back></article>