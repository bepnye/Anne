<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1662-4548</issn><issn pub-type="epub">1662-453X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26869874</article-id><article-id pub-id-type="pmc">4735350</article-id><article-id pub-id-type="doi">10.3389/fnins.2016.00017</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Methods</subject></subj-group></subj-group></article-categories><title-group><article-title>Memory Efficient PCA Methods for Large Group ICA</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rachakonda</surname><given-names>Srinivas</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/4523/overview"></uri></contrib><contrib contrib-type="author"><name><surname>Silva</surname><given-names>Rogers F.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/17874/overview"></uri></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Jingyu</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/17893/overview"></uri></contrib><contrib contrib-type="author"><name><surname>Calhoun</surname><given-names>Vince D.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/884/overview"></uri></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>The Mind Research Network and Lovelace Biomedical and Environmental Research Institute</institution><country>Albuquerque, NM, USA</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Electrical and Computer Engineering, The University of New Mexico</institution><country>Albuquerque, NM, USA</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Computer Science, The University of New Mexico</institution><country>Albuquerque, NM, USA</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Pedro Antonio Valdes-Sosa, Centro de Neurociencias de Cuba, Cuba</p></fn><fn fn-type="edited-by"><p>Reviewed by: Anand Joshi, University of Southern California, USA; Eugene Duff, University of Oxford, UK</p></fn><corresp id="fn001">*Correspondence: Srinivas Rachakonda <email xlink:type="simple">srachakonda@mrn.org</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Brain Imaging Methods, a section of the journal Frontiers in Neuroscience</p></fn></author-notes><pub-date pub-type="epub"><day>02</day><month>2</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>10</volume><elocation-id>17</elocation-id><history><date date-type="received"><day>08</day><month>10</month><year>2015</year></date><date date-type="accepted"><day>12</day><month>1</month><year>2016</year></date></history><permissions><copyright-statement>Copyright © 2016 Rachakonda, Silva, Liu and Calhoun.</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Rachakonda, Silva, Liu and Calhoun</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p><offsets xml_i="4628" xml_f="7015" txt_i="11" txt_f="2398">Principal component analysis (PCA) is widely used for data reduction in group independent component analysis (ICA) of fMRI data. Commonly, group-level PCA of temporally concatenated datasets is computed prior to ICA of the group principal components. This work focuses on reducing very high dimensional temporally concatenated datasets into its group PCA space. Existing randomized PCA methods can determine the PCA subspace with minimal memory requirements and, thus, are ideal for solving large PCA problems. Since the number of dataloads is not typically optimized, we extend one of these methods to compute PCA of very large datasets with a minimal number of dataloads. This method is coined multi power iteration (MPOWIT). The key idea behind MPOWIT is to estimate a subspace larger than the desired one, while checking for convergence of only the smaller subset of interest. The number of iterations is reduced considerably (as well as the number of dataloads), accelerating convergence without loss of accuracy. More importantly, in the proposed implementation of MPOWIT, the memory required for successful recovery of the group principal components becomes independent of the number of subjects analyzed. Highly efficient subsampled eigenvalue decomposition techniques are also introduced, furnishing excellent PCA subspace approximations that can be used for intelligent initialization of randomized methods such as MPOWIT. Together, these developments enable efficient estimation of accurate principal components, as we illustrate by solving a 1600-subject group-level PCA of fMRI with standard acquisition parameters, on a regular desktop computer with only 4 GB RAM, in just a few hours. MPOWIT is also highly scalable and could realistically solve group-level PCA of fMRI on thousands of subjects, or more, using standard hardware, limited only by time, not memory. Also, the MPOWIT algorithm is highly parallelizable, which would enable fast, distributed implementations ideal for big data analysis. Implications to other methods such as expectation maximization PCA (EM PCA) are also presented. Based on our results, general recommendations for efficient application of PCA methods are given according to problem size and available computational resources. MPOWIT and all other methods discussed here are implemented and readily available in the open source GIFT software.</offsets></p></abstract><kwd-group><kwd>group ICA</kwd><kwd>big data</kwd><kwd>PCA</kwd><kwd>subspace iteration</kwd><kwd>EVD</kwd><kwd>SVD</kwd><kwd>memory</kwd><kwd>power iteration</kwd></kwd-group><counts><fig-count count="8"></fig-count><table-count count="0"></table-count><equation-count count="37"></equation-count><ref-count count="35"></ref-count><page-count count="15"></page-count><word-count count="11048"></word-count></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title><offsets xml_i="7512" xml_f="7524" txt_i="2406" txt_f="2418">Introduction</offsets></title><p><offsets xml_i="7535" xml_f="7688" txt_i="2419" txt_f="2572">Principal component analysis (PCA) is used as both a data reduction and de-noising method in group independent component analysis (ICA) (Calhoun et al., </offsets><xref rid="B8" ref-type="bibr"><offsets xml_i="7719" xml_f="7723" txt_i="2572" txt_f="2576">2001</offsets></xref><offsets xml_i="7730" xml_f="7752" txt_i="2576" txt_f="2598">; Beckmann and Smith, </offsets><xref rid="B2" ref-type="bibr"><offsets xml_i="7783" xml_f="7787" txt_i="2598" txt_f="2602">2004</offsets></xref><offsets xml_i="7794" xml_f="7815" txt_i="2602" txt_f="2623">; Calhoun and Adali, </offsets><xref rid="B7" ref-type="bibr"><offsets xml_i="7846" xml_f="7850" txt_i="2623" txt_f="2627">2012</offsets></xref><offsets xml_i="7857" xml_f="7970" txt_i="2627" txt_f="2740">). PCA is typically carried out by computing the eigenvalue decomposition (EVD) of the sample covariance matrix (</offsets><italic><offsets xml_i="7978" xml_f="7979" txt_i="2740" txt_f="2741">C</offsets></italic><offsets xml_i="7988" xml_f="8109" txt_i="2741" txt_f="2862">) or by using singular value decomposition (SVD) directly on the data. For large datasets, both EVD (plus computation of </offsets><italic><offsets xml_i="8117" xml_f="8118" txt_i="2862" txt_f="2863">C</offsets></italic><offsets xml_i="8127" xml_f="8542" txt_i="2863" txt_f="3278">) and SVD become computationally intensive in both memory and speed. For instance, group ICA is commonly used for functional magnetic resonance imaging (fMRI) studies. A typical fMRI study may collect image volumes from a single subject for about 20 min using TR = 1000 ms and 3 × 3 × 3 mm voxel resolution, resulting in approximately 53 × 63 × 46 × 1200 data points. To compute group PCA using the standard EVD of </offsets><italic><offsets xml_i="8550" xml_f="8551" txt_i="3278" txt_f="3279">C</offsets></italic><offsets xml_i="8560" xml_f="8827" txt_i="3279" txt_f="3546"> approach on 100 subjects stacked in the temporal dimension (and only the nearly 70000 in-brain voxels) requires approximately 100 GB RAM and more than 16 h on a Linux server. Using the SVD approach would incur similar memory requirements as EVD (plus computation of </offsets><italic><offsets xml_i="8835" xml_f="8836" txt_i="3546" txt_f="3547">C</offsets></italic><offsets xml_i="8845" xml_f="9038" txt_i="3547" txt_f="3740">). In either case, the computational requirements can quickly become prohibitive, especially with the constant advance of imaging techniques [such as multi-band EPI sequences (Feinberg et al., </offsets><xref rid="B13" ref-type="bibr"><offsets xml_i="9070" xml_f="9074" txt_i="3740" txt_f="3744">2010</offsets></xref><offsets xml_i="9081" xml_f="9107" txt_i="3744" txt_f="3770">; Feinberg and Setsompop, </offsets><xref rid="B12" ref-type="bibr"><offsets xml_i="9139" xml_f="9143" txt_i="3770" txt_f="3774">2013</offsets></xref><offsets xml_i="9150" xml_f="9390" txt_i="3774" txt_f="4014">)] and a tendency to share data within the imaging community. This means very large size imaging data will become even more common for fMRI studies, encouraging the development of novel computational methods to face the upcoming challenges.</offsets></p><p><offsets xml_i="9397" xml_f="9798" txt_i="4015" txt_f="4416">There are several methods to estimate dominant PCA components with minimal memory requirements, like sequential SVD, cascade recursive least squares (CRLS) PCA, and randomized PCA approaches, to name a few. Sequential or “online” SVD is usually applied in a streaming memory setting where the data streams over time and only a single pass over the datasets is possible. There exist algorithms (Brand, </offsets><xref rid="B6" ref-type="bibr"><offsets xml_i="9829" xml_f="9833" txt_i="4416" txt_f="4420">2003</offsets></xref><offsets xml_i="9840" xml_f="9846" txt_i="4420" txt_f="4426">; Li, </offsets><xref rid="B23" ref-type="bibr"><offsets xml_i="9878" xml_f="9882" txt_i="4426" txt_f="4430">2004</offsets></xref><offsets xml_i="9889" xml_f="9897" txt_i="4430" txt_f="4438">; Funk, </offsets><xref rid="B16" ref-type="bibr"><offsets xml_i="9929" xml_f="9933" txt_i="4438" txt_f="4442">2006</offsets></xref><offsets xml_i="9940" xml_f="10123" txt_i="4442" txt_f="4625">) which provide incremental SVD update and downdate capacity. However, principal components obtained with sequential SVD approaches are typically not as accurate as those from EVD of </offsets><italic><offsets xml_i="10131" xml_f="10132" txt_i="4625" txt_f="4626">C</offsets></italic><offsets xml_i="10141" xml_f="10277" txt_i="4626" txt_f="4762"> and, therefore, sequential SVD approaches are considered not suitable for data reduction in group ICA analyses. CRLS PCA (Wang et al., </offsets><xref rid="B35" ref-type="bibr"><offsets xml_i="10309" xml_f="10313" txt_i="4762" txt_f="4766">2006</offsets></xref><offsets xml_i="10320" xml_f="10657" txt_i="4766" txt_f="5103">) uses a subspace deflation technique to extract dominant components of interest with limited training. The number of training epochs required is dependent on the data and, therefore, the CRLS PCA algorithm has slower performance in very large datasets and when higher model order (i.e., high number of components) needs to be estimated.</offsets></p><p><offsets xml_i="10664" xml_f="11322" txt_i="5104" txt_f="5762">Randomized PCA methods are a class of algorithms that iteratively estimate the principal components from the data and are particularly useful when only a few components need to be estimated from very large datasets. They provide a much more efficient solution than the EVD approach, which always estimates the complete set of eigenvectors, many of which are eventually discarded for data reduction and de-noising purposes. Clearly, iterative approaches can make a much more intelligent use of the available computational resources. Some popular and upcoming randomized PCA approaches are: implicitly restarted Arnoldi iteration (IRAM; (Lehoucq and Sorensen, </offsets><xref rid="B22" ref-type="bibr"><offsets xml_i="11354" xml_f="11358" txt_i="5762" txt_f="5766">1996</offsets></xref><offsets xml_i="11365" xml_f="11399" txt_i="5766" txt_f="5800">)), power iteration (Recktenwald, </offsets><xref rid="B28" ref-type="bibr"><offsets xml_i="11431" xml_f="11435" txt_i="5800" txt_f="5804">2000</offsets></xref><offsets xml_i="11442" xml_f="11478" txt_i="5804" txt_f="5840">), subspace iteration (Rutishauser, </offsets><xref rid="B30" ref-type="bibr"><offsets xml_i="11510" xml_f="11514" txt_i="5840" txt_f="5844">1970</offsets></xref><offsets xml_i="11521" xml_f="11570" txt_i="5844" txt_f="5893">) expectation maximization PCA (EM PCA) (Roweis, </offsets><xref rid="B29" ref-type="bibr"><offsets xml_i="11602" xml_f="11606" txt_i="5893" txt_f="5897">1997</offsets></xref><offsets xml_i="11613" xml_f="11647" txt_i="5897" txt_f="5931">), and “Large PCA” (Halko et al., </offsets><xref rid="B17" ref-type="bibr"><offsets xml_i="11679" xml_f="11684" txt_i="5931" txt_f="5936">2011a</offsets></xref><offsets xml_i="11691" xml_f="11741" txt_i="5936" txt_f="5986">). IRAM as implemented in ARPACK (Lehoucq et al., </offsets><xref rid="B21" ref-type="bibr"><offsets xml_i="11773" xml_f="11777" txt_i="5986" txt_f="5990">1998</offsets></xref><offsets xml_i="11784" xml_f="12746" txt_i="5990" txt_f="6952">) requires that the sample covariance matrix be computed from the data and, thus, has higher computational demands on memory. Power iteration determines PCA components in a so-called “deflationary” mode (i.e., one at a time) and has very poor convergence properties when more than one component needs to be extracted from the data. Also, the error accumulates in subsequent estimations. Subspace iteration is a symmetric version of the power iteration method which extracts multiple components simultaneously from the data using explicit orthogonalization of the subspace in each iteration. EM PCA uses expectation and maximization steps to estimate multiple components simultaneously from the data. Both EM PCA and subspace iteration methods converge faster when only a few components are estimated from very large datasets and have slower convergence properties when a higher number of components needs to be estimated. More recently, Large PCA (Halko et al., </offsets><xref rid="B17" ref-type="bibr"><offsets xml_i="12778" xml_f="12783" txt_i="6952" txt_f="6957">2011a</offsets></xref><offsets xml_i="12790" xml_f="12959" txt_i="6957" txt_f="7126">) was proposed to evaluate the principal components from very large datasets. Large PCA is a randomized version of the block Lanczos method (Kuczynski and Wozniakowski, </offsets><xref rid="B20" ref-type="bibr"><offsets xml_i="12991" xml_f="12995" txt_i="7126" txt_f="7130">1992</offsets></xref><offsets xml_i="13002" xml_f="13142" txt_i="7130" txt_f="7270">) and is highly dependent on appropriate block size determination (typically large) in order to give accurate results with default settings.</offsets></p><p><offsets xml_i="13149" xml_f="14003" txt_i="7271" txt_f="8125">In this paper, we show how to overcome the problem of slow convergence in subspace iteration when a high number of components is estimated by introducing a new approach, named multi power iteration (MPOWIT). Our approach takes into account the number of dataloads, which has often been overlooked in the development of randomized PCA methods. We also show that both subspace iteration and EM PCA methods converge to the same subspace in each iteration. Thus, the acceleration scheme we propose in MPOWIT can also be applied to EM PCA. In addition, we compare the performance of MPOWIT with existing PCA methods like EVD and Large PCA using real fMRI data from 1600 subjects with standard acquisition parameters. Moreover, acknowledging the recent popularization and promising developments in the area of multi-band EPI sequences (Feinberg and Setsompop, </offsets><xref rid="B12" ref-type="bibr"><offsets xml_i="14035" xml_f="14039" txt_i="8125" txt_f="8129">2013</offsets></xref><offsets xml_i="14046" xml_f="14814" txt_i="8129" txt_f="8897">), we provide performance assessments of the PCA methods discussed here in the case of hypothetical 5000-subject fMRI studies using TR = 200 ms, 2 × 2 × 2 mm voxel resolution, and 30 min-long sessions. Based on our current estimates, group-level PCA using our new randomized PCA approach and retaining 200 principal components in the subject-level PCA could be performed on such data in 40 h (nearly 36 h just loading the subject-level PCA results) using a Windows desktop with 4 GB RAM. Alternatively, the same analysis could be performed in just 2 h using a Linux server with 512 GB RAM (assuming the subject-level PCA results are kept in memory after their estimation). In either case, this is without the additional benefits of GPU acceleration or parallelization.</offsets></p><p><offsets xml_i="14821" xml_f="15426" txt_i="8898" txt_f="9503">We provide descriptions of EVD, Subsampled PCA, Large PCA, MPOWIT and EM PCA in the Materials and Methods Section. The same section also includes a description of the datasets and experiments conducted for each PCA method. Experiments are performed on the real fMRI data. In the Results Section, we present our experimental results and compare the performance of MPOWIT with existing PCA methods. Finally, we discuss these results and draw conclusions based on the analyses we performed. Additional details are provided in the appendices, including a proof that EM PCA is equivalent to subspace iteration.</offsets></p></sec><sec sec-type="materials and methods" id="s2"><title><offsets xml_i="15489" xml_f="15510" txt_i="9505" txt_f="9526">Materials and methods</offsets></title><sec><title><offsets xml_i="15530" xml_f="15539" txt_i="9527" txt_f="9536">Group ICA</offsets></title><p><offsets xml_i="15550" xml_f="15652" txt_i="9537" txt_f="9639">In this paper, we are interested in group ICA of fMRI data as originally described in Calhoun et al. (</offsets><xref rid="B8" ref-type="bibr"><offsets xml_i="15683" xml_f="15687" txt_i="9639" txt_f="9643">2001</offsets></xref><offsets xml_i="15694" xml_f="15749" txt_i="9643" txt_f="9698">) and further expanded and reviewed in Erhardt et al. (</offsets><xref rid="B11" ref-type="bibr"><offsets xml_i="15781" xml_f="15785" txt_i="9698" txt_f="9702">2011</offsets></xref><offsets xml_i="15792" xml_f="15817" txt_i="9702" txt_f="9727">) and Calhoun and Adali (</offsets><xref rid="B7" ref-type="bibr"><offsets xml_i="15848" xml_f="15852" txt_i="9727" txt_f="9731">2012</offsets></xref><offsets xml_i="15859" xml_f="15881" txt_i="9731" txt_f="9753">). In this technique, </offsets><italic><offsets xml_i="15889" xml_f="15890" txt_i="9753" txt_f="9754">Z</offsets></italic><sub><italic><offsets xml_i="15912" xml_f="15913" txt_i="9754" txt_f="9755">i</offsets></italic></sub><offsets xml_i="15928" xml_f="15958" txt_i="9755" txt_f="9785"> are the fMRI data of subject </offsets><italic><offsets xml_i="15966" xml_f="15967" txt_i="9785" txt_f="9786">i</offsets></italic><offsets xml_i="15976" xml_f="15992" txt_i="9786" txt_f="9802"> with dimension </offsets><italic><offsets xml_i="16000" xml_f="16001" txt_i="9802" txt_f="9803">v</offsets></italic><offsets xml_i="16010" xml_f="16013" txt_i="9803" txt_f="9806"> × </offsets><italic><offsets xml_i="16021" xml_f="16022" txt_i="9806" txt_f="9807">t</offsets></italic><offsets xml_i="16031" xml_f="16039" txt_i="9807" txt_f="9815">, where </offsets><italic><offsets xml_i="16047" xml_f="16048" txt_i="9815" txt_f="9816">v</offsets></italic><offsets xml_i="16057" xml_f="16062" txt_i="9816" txt_f="9821"> and </offsets><italic><offsets xml_i="16070" xml_f="16071" txt_i="9821" txt_f="9822">t</offsets></italic><offsets xml_i="16080" xml_f="16137" txt_i="9822" txt_f="9879"> are the number of voxels and time points, respectively. </offsets><italic><offsets xml_i="16145" xml_f="16146" txt_i="9879" txt_f="9880">Z</offsets></italic><sub><italic><offsets xml_i="16168" xml_f="16169" txt_i="9880" txt_f="9881">i</offsets></italic></sub><offsets xml_i="16184" xml_f="16314" txt_i="9881" txt_f="10011"> is mean-centered on zero at each time point. Each subject's data is reduced along the time dimension using PCA to retain the top </offsets><italic><offsets xml_i="16322" xml_f="16323" txt_i="10011" txt_f="10012">p</offsets></italic><offsets xml_i="16332" xml_f="16368" txt_i="10012" txt_f="10048"> components, which are then whitened</offsets><xref ref-type="fn" rid="fn0001"><sup><offsets xml_i="16406" xml_f="16407" txt_i="10048" txt_f="10049">1</offsets></sup></xref><offsets xml_i="16420" xml_f="16437" txt_i="10049" txt_f="10066">. Following, all </offsets><italic><offsets xml_i="16445" xml_f="16446" txt_i="10066" txt_f="10067">M</offsets></italic><offsets xml_i="16455" xml_f="16521" txt_i="10067" txt_f="10133"> subjects are stacked along the (reduced) temporal dimension. Let </offsets><italic><offsets xml_i="16529" xml_f="16530" txt_i="10133" txt_f="10134">Y</offsets></italic><offsets xml_i="16539" xml_f="16543" txt_i="10134" txt_f="10138"> = [</offsets><italic><offsets xml_i="16551" xml_f="16552" txt_i="10138" txt_f="10139">Y</offsets></italic><sub><offsets xml_i="16566" xml_f="16567" txt_i="10139" txt_f="10140">1</offsets></sub><offsets xml_i="16573" xml_f="16575" txt_i="10140" txt_f="10142">, </offsets><italic><offsets xml_i="16583" xml_f="16584" txt_i="10142" txt_f="10143">Y</offsets></italic><sub><offsets xml_i="16598" xml_f="16599" txt_i="10143" txt_f="10144">2</offsets></sub><offsets xml_i="16605" xml_f="16610" txt_i="10144" txt_f="10149">, …, </offsets><italic><offsets xml_i="16618" xml_f="16619" txt_i="10149" txt_f="10150">Y</offsets></italic><sub><italic><offsets xml_i="16641" xml_f="16642" txt_i="10150" txt_f="10151">M</offsets></italic></sub><offsets xml_i="16657" xml_f="16701" txt_i="10151" txt_f="10195">] be the temporally concatenated data where </offsets><italic><offsets xml_i="16709" xml_f="16710" txt_i="10195" txt_f="10196">Y</offsets></italic><sub><italic><offsets xml_i="16732" xml_f="16733" txt_i="10196" txt_f="10197">i</offsets></italic></sub><offsets xml_i="16748" xml_f="16766" txt_i="10197" txt_f="10215"> is the zero-mean </offsets><italic><offsets xml_i="16774" xml_f="16775" txt_i="10215" txt_f="10216">v</offsets></italic><offsets xml_i="16784" xml_f="16787" txt_i="10216" txt_f="10219"> × </offsets><italic><offsets xml_i="16795" xml_f="16796" txt_i="10219" txt_f="10220">p</offsets></italic><offsets xml_i="16805" xml_f="16834" txt_i="10220" txt_f="10249"> PCA-reduced data of subject </offsets><italic><offsets xml_i="16842" xml_f="16843" txt_i="10249" txt_f="10250">i</offsets></italic><offsets xml_i="16852" xml_f="16946" txt_i="10250" txt_f="10344">. Group-level PCA is then performed on the temporally reduced concatenated data, resulting in </offsets><italic><offsets xml_i="16954" xml_f="16955" txt_i="10344" txt_f="10345">k</offsets></italic><offsets xml_i="16964" xml_f="17021" txt_i="10345" txt_f="10402"> group principal components in the group-level PCA space </offsets><italic><offsets xml_i="17029" xml_f="17030" txt_i="10402" txt_f="10403">X</offsets></italic><offsets xml_i="17039" xml_f="17048" txt_i="10403" txt_f="10412">. Figure </offsets><xref ref-type="fig" rid="F1"><offsets xml_i="17078" xml_f="17079" txt_i="10412" txt_f="10413">1</offsets></xref><offsets xml_i="17086" xml_f="17133" txt_i="10413" txt_f="10460"> shows a graphical representation of group PCA.</offsets></p><fig id="F1" position="float"><label><offsets xml_i="17174" xml_f="17182" txt_i="10461" txt_f="10469">Figure 1</offsets></label><caption><p><bold><offsets xml_i="17208" xml_f="17227" txt_i="10469" txt_f="10488">Group PCA framework</offsets></bold><offsets xml_i="17234" xml_f="17883" txt_i="10488" txt_f="11137">. Graphical representation of the two-step PCA approach to group-level PCA we consider in this work. The first step performs subject-level PCA on each subject, followed by (optional) whitening. The second step performs group-level PCA on the stacked (concatenated) reduced-data from all subjects, and is the focus of our attention. Particularly, if time and memory resources were unlimited, standard PCA routines for subject-level PCA (such as EVD) would suffice for group-level PCA on the stacked data. For that reason, we consider that to be the “ideal” scenario and strive to replicate its results in the more realistic case of limited resources.</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0001"></graphic></fig><p><offsets xml_i="17959" xml_f="18046" txt_i="11138" txt_f="11225">Group ICA is regularly being used to analyze large numbers of subjects (Biswal et al., </offsets><xref rid="B4" ref-type="bibr"><offsets xml_i="18077" xml_f="18081" txt_i="11225" txt_f="11229">2010</offsets></xref><offsets xml_i="18088" xml_f="18104" txt_i="11229" txt_f="11245">; Allen et al., </offsets><xref rid="B1" ref-type="bibr"><offsets xml_i="18135" xml_f="18139" txt_i="11245" txt_f="11249">2011</offsets></xref><offsets xml_i="18146" xml_f="18196" txt_i="11249" txt_f="11299">), and multi-band EPI sequences (Feinberg et al., </offsets><xref rid="B13" ref-type="bibr"><offsets xml_i="18228" xml_f="18232" txt_i="11299" txt_f="11303">2010</offsets></xref><offsets xml_i="18239" xml_f="18259" txt_i="11303" txt_f="11323">; Setsompop et al., </offsets><xref rid="B32" ref-type="bibr"><offsets xml_i="18291" xml_f="18295" txt_i="11323" txt_f="11327">2012</offsets></xref><offsets xml_i="18302" xml_f="18540" txt_i="11327" txt_f="11565">) put even more memory demands on group PCA if the number of components retained in the first PCA step is increased significantly. We previously implemented some memory efficient ways to solve the PCA problem on large datasets in the GIFT</offsets><xref ref-type="fn" rid="fn0002"><sup><offsets xml_i="18578" xml_f="18579" txt_i="11565" txt_f="11566">2</offsets></sup></xref><offsets xml_i="18592" xml_f="18935" txt_i="11566" txt_f="11909"> toolbox using EVD, SVD and EM PCA. Options are provided in the GIFT toolbox to select the appropriate PCA method based on the problem size and computer RAM requirements. In this paper, we present ways to further accelerate the group-level PCA step using new algorithms, and discuss the scalability of PCA algorithms based on the problem size.</offsets></p></sec><sec><title><offsets xml_i="18957" xml_f="18972" txt_i="11911" txt_f="11926">Group-level PCA</offsets></title><p><offsets xml_i="18983" xml_f="19104" txt_i="11927" txt_f="12048">Group ICA of temporally concatenated fMRI can be used to identify either spatial independent components (Calhoun et al., </offsets><xref rid="B8" ref-type="bibr"><offsets xml_i="19135" xml_f="19139" txt_i="12048" txt_f="12052">2001</offsets></xref><offsets xml_i="19146" xml_f="19162" txt_i="12052" txt_f="12068">; Allen et al., </offsets><xref rid="B1" ref-type="bibr"><offsets xml_i="19193" xml_f="19197" txt_i="12068" txt_f="12072">2011</offsets></xref><offsets xml_i="19204" xml_f="19256" txt_i="12072" txt_f="12124">) or temporal independent components (Smith et al., </offsets><xref rid="B34" ref-type="bibr"><offsets xml_i="19288" xml_f="19292" txt_i="12124" txt_f="12128">2012</offsets></xref><offsets xml_i="19299" xml_f="19445" txt_i="12128" txt_f="12274">). In spatial group ICA, subject-level PCA is typically carried out to reduce the time dimension prior to group-level PCA, as described in Figure </offsets><xref ref-type="fig" rid="F1"><offsets xml_i="19475" xml_f="19476" txt_i="12274" txt_f="12275">1</offsets></xref><offsets xml_i="19483" xml_f="19602" txt_i="12275" txt_f="12394">. However, in temporal group ICA, subject-level PCA to reduce the spatial dimension is neither required nor recommended</offsets><xref ref-type="fn" rid="fn0003"><sup><offsets xml_i="19640" xml_f="19641" txt_i="12394" txt_f="12395">3</offsets></sup></xref><offsets xml_i="19654" xml_f="19728" txt_i="12395" txt_f="12469">; instead, group-level PCA is carried out directly on the input fMRI data </offsets><italic><offsets xml_i="19736" xml_f="19737" txt_i="12469" txt_f="12470">Z</offsets></italic><sub><italic><offsets xml_i="19759" xml_f="19760" txt_i="12470" txt_f="12471">i</offsets></italic></sub><offsets xml_i="19775" xml_f="19873" txt_i="12471" txt_f="12569">. In the following, we present algorithms for PCA estimation assuming the case depicted in Figure </offsets><xref ref-type="fig" rid="F1"><offsets xml_i="19903" xml_f="19904" txt_i="12569" txt_f="12570">1</offsets></xref><offsets xml_i="19911" xml_f="19996" txt_i="12570" txt_f="12655"> and deferring any comments about temporal group-level PCA to the Discussion Section.</offsets></p><p><offsets xml_i="20003" xml_f="21235" txt_i="12656" txt_f="13888">In the following, we present a selection of approaches for group PCA, starting with the traditional EVD method, which we consider the standard for accuracy in later comparisons. Then, based on considerations made on the EVD method and properties of the fMRI data, two approaches are proposed for efficient approximate estimation of the group PCA solution, namely subsampled voxel PCA (SVP) and subsampled time PCA (STP). Both approaches are useful for efficient initialization and accelerated convergence of the highly accurate randomized methods presented later. Following, Large PCA, a recent block Lanczos method with high accuracy and potential for application in large group PCA, is introduced for the purpose of comparison. Power methods, including the introduction of our novel MPOWIT technique, are discussed next. The connections and implications of MPOWIT on the popular expectation maximization PCA (EM PCA) approach are presented lastly. At every stage, we strive to present every algorithmic improvement and theoretical development in the context of fMRI data under various conditions. Nevertheless, all considerations should be straightforwardly extensible to other modalities and datatypes. Throughout the following, </offsets><italic><offsets xml_i="21243" xml_f="21244" txt_i="13888" txt_f="13889">F</offsets></italic><offsets xml_i="21253" xml_f="21336" txt_i="13889" txt_f="13972"> is the eigenvector matrix required for GICA1 back-reconstruction (Erhardt et al., </offsets><xref rid="B11" ref-type="bibr"><offsets xml_i="21368" xml_f="21372" txt_i="13972" txt_f="13976">2011</offsets></xref><offsets xml_i="21379" xml_f="21406" txt_i="13976" txt_f="14003">). Finally, we assume that </offsets><italic><offsets xml_i="21414" xml_f="21415" txt_i="14003" txt_f="14004">k</offsets></italic><offsets xml_i="21424" xml_f="21435" txt_i="14004" txt_f="14009"> &lt; &lt; </offsets><italic><offsets xml_i="21443" xml_f="21445" txt_i="14009" txt_f="14011">Mp</offsets></italic><offsets xml_i="21454" xml_f="21459" txt_i="14011" txt_f="14016"> and </offsets><italic><offsets xml_i="21467" xml_f="21468" txt_i="14016" txt_f="14017">k</offsets></italic><offsets xml_i="21477" xml_f="21488" txt_i="14017" txt_f="14022"> &lt; &lt; </offsets><italic><offsets xml_i="21496" xml_f="21497" txt_i="14022" txt_f="14023">v</offsets></italic><offsets xml_i="21506" xml_f="21586" txt_i="14023" txt_f="14103"> in all time complexity assessments presented hereafter, unless otherwise noted.</offsets></p><sec><title><offsets xml_i="21602" xml_f="21632" txt_i="14104" txt_f="14134">Eigenvalue decomposition (EVD)</offsets></title><p><offsets xml_i="21643" xml_f="21693" txt_i="14135" txt_f="14185">Using the EVD approach, the group-level PCA space </offsets><italic><offsets xml_i="21701" xml_f="21702" txt_i="14185" txt_f="14186">X</offsets></italic><offsets xml_i="21711" xml_f="21788" txt_i="14186" txt_f="14263"> can be determined from the temporally stacked (concatenated) zero-mean data </offsets><italic><offsets xml_i="21796" xml_f="21797" txt_i="14263" txt_f="14264">Y</offsets></italic><offsets xml_i="21806" xml_f="21818" txt_i="14264" txt_f="14276"> as follows:</offsets></p><list list-type="alpha-lower"><list-item><p><offsets xml_i="21866" xml_f="21903" txt_i="14277" txt_f="14314">Compute the sample covariance matrix </offsets><italic><offsets xml_i="21911" xml_f="21912" txt_i="14314" txt_f="14315">C</offsets></italic><offsets xml_i="21921" xml_f="21974" txt_i="14315" txt_f="14368"> in the smallest dimension of the data (Wang et al., </offsets><xref rid="B35" ref-type="bibr"><offsets xml_i="22006" xml_f="22010" txt_i="14368" txt_f="14372">2006</offsets></xref><offsets xml_i="22017" xml_f="22023" txt_i="14372" txt_f="14378">). If </offsets><italic><offsets xml_i="22031" xml_f="22033" txt_i="14378" txt_f="14380">Mp</offsets></italic><offsets xml_i="22042" xml_f="22048" txt_i="14380" txt_f="14383"> &lt; </offsets><italic><offsets xml_i="22056" xml_f="22057" txt_i="14383" txt_f="14384">v</offsets></italic><offsets xml_i="22066" xml_f="22072" txt_i="14384" txt_f="14390">, the </offsets><italic><offsets xml_i="22080" xml_f="22082" txt_i="14390" txt_f="14392">Mp</offsets></italic><offsets xml_i="22091" xml_f="22094" txt_i="14392" txt_f="14395"> × </offsets><italic><offsets xml_i="22102" xml_f="22104" txt_i="14395" txt_f="14397">Mp</offsets></italic><offsets xml_i="22113" xml_f="22136" txt_i="14397" txt_f="14420"> covariance matrix is:
</offsets><disp-formula id="E1"><label><offsets xml_i="22165" xml_f="22168" txt_i="14420" txt_f="14423">(1)</offsets></label><mml:math id="M1"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="22251" xml_f="22252" txt_i="14423" txt_f="14424">C</offsets></mml:mi><mml:mo><offsets xml_i="22269" xml_f="22270" txt_i="14424" txt_f="14425">=</offsets></mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi><offsets xml_i="22328" xml_f="22329" txt_i="14425" txt_f="14426">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="22367" xml_f="22368" txt_i="14426" txt_f="14427">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mi><offsets xml_i="22407" xml_f="22408" txt_i="14427" txt_f="14428">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="22446" xml_f="22447" txt_i="14428" txt_f="14429">v</offsets></mml:mi><mml:mo><offsets xml_i="22464" xml_f="22465" txt_i="14429" txt_f="14430">-</offsets></mml:mo><mml:mn><offsets xml_i="22482" xml_f="22483" txt_i="14430" txt_f="14431">1</offsets></mml:mn></mml:mrow></mml:mfrac><mml:mo><offsets xml_i="22523" xml_f="22524" txt_i="14431" txt_f="14432">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item><list-item><p><offsets xml_i="22622" xml_f="22671" txt_i="14433" txt_f="14482">EVD factorizes the (symmetric) covariance matrix </offsets><italic><offsets xml_i="22679" xml_f="22680" txt_i="14482" txt_f="14483">C</offsets></italic><offsets xml_i="22689" xml_f="22708" txt_i="14483" txt_f="14502"> into eigenvectors </offsets><italic><offsets xml_i="22716" xml_f="22717" txt_i="14502" txt_f="14503">F</offsets></italic><offsets xml_i="22726" xml_f="22746" txt_i="14503" txt_f="14523"> and eigenvalues Λ:
</offsets><disp-formula id="E2"><label><offsets xml_i="22775" xml_f="22778" txt_i="14523" txt_f="14526">(2)</offsets></label><mml:math id="M2"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="22861" xml_f="22862" txt_i="14526" txt_f="14527">C</offsets></mml:mi><mml:mo><offsets xml_i="22879" xml_f="22880" txt_i="14527" txt_f="14528">=</offsets></mml:mo><mml:mi><offsets xml_i="22897" xml_f="22898" txt_i="14528" txt_f="14529">F</offsets></mml:mi><mml:mi><offsets xml_i="22915" xml_f="22916" txt_i="14529" txt_f="14530">Λ</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="22953" xml_f="22954" txt_i="14530" txt_f="14531">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="22992" xml_f="22993" txt_i="14531" txt_f="14532">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="23032" xml_f="23033" txt_i="14532" txt_f="14533">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item><list-item><p><offsets xml_i="23131" xml_f="23135" txt_i="14534" txt_f="14538">The </offsets><italic><offsets xml_i="23143" xml_f="23144" txt_i="14538" txt_f="14539">v</offsets></italic><offsets xml_i="23153" xml_f="23156" txt_i="14539" txt_f="14542"> × </offsets><italic><offsets xml_i="23164" xml_f="23165" txt_i="14542" txt_f="14543">k</offsets></italic><offsets xml_i="23174" xml_f="23197" txt_i="14543" txt_f="14566"> group-level PCA space </offsets><italic><offsets xml_i="23205" xml_f="23206" txt_i="14566" txt_f="14567">X</offsets></italic><offsets xml_i="23215" xml_f="23250" txt_i="14567" txt_f="14602"> is obtained by projecting the top </offsets><italic><offsets xml_i="23258" xml_f="23259" txt_i="14602" txt_f="14603">k</offsets></italic><offsets xml_i="23268" xml_f="23295" txt_i="14603" txt_f="14630"> eigenvectors (columns) of </offsets><italic><offsets xml_i="23303" xml_f="23304" txt_i="14630" txt_f="14631">F</offsets></italic><offsets xml_i="23313" xml_f="23370" txt_i="14631" txt_f="14688"> with largest eigenvalues onto the data, as shown below:
</offsets><disp-formula id="E3"><label><offsets xml_i="23399" xml_f="23402" txt_i="14688" txt_f="14691">(3)</offsets></label><mml:math id="M3"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="23505" xml_f="23506" txt_i="14691" txt_f="14692">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="23544" xml_f="23545" txt_i="14692" txt_f="14693">v</offsets></mml:mi><mml:mo><offsets xml_i="23562" xml_f="23563" txt_i="14693" txt_f="14694">×</offsets></mml:mo><mml:mi><offsets xml_i="23580" xml_f="23581" txt_i="14694" txt_f="14695">k</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="23620" xml_f="23621" txt_i="14695" txt_f="14696">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="23658" xml_f="23659" txt_i="14696" txt_f="14697">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="23697" xml_f="23698" txt_i="14697" txt_f="14698">v</offsets></mml:mi><mml:mo><offsets xml_i="23715" xml_f="23716" txt_i="14698" txt_f="14699">×</offsets></mml:mo><mml:mi><offsets xml_i="23733" xml_f="23734" txt_i="14699" txt_f="14700">M</offsets></mml:mi><mml:mi><offsets xml_i="23751" xml_f="23752" txt_i="14700" txt_f="14701">p</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="23811" xml_f="23812" txt_i="14701" txt_f="14702">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="23850" xml_f="23851" txt_i="14702" txt_f="14703">M</offsets></mml:mi><mml:mi><offsets xml_i="23868" xml_f="23869" txt_i="14703" txt_f="14704">p</offsets></mml:mi><mml:mo><offsets xml_i="23886" xml_f="23887" txt_i="14704" txt_f="14705">×</offsets></mml:mo><mml:mi><offsets xml_i="23904" xml_f="23905" txt_i="14705" txt_f="14706">k</offsets></mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="23967" xml_f="23968" txt_i="14706" txt_f="14707">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="24006" xml_f="24007" txt_i="14707" txt_f="14708">k</offsets></mml:mi><mml:mo><offsets xml_i="24024" xml_f="24025" txt_i="14708" txt_f="14709">×</offsets></mml:mo><mml:mi><offsets xml_i="24042" xml_f="24043" txt_i="14709" txt_f="14710">k</offsets></mml:mi></mml:mrow><mml:mrow><mml:mo><offsets xml_i="24081" xml_f="24082" txt_i="14710" txt_f="14711">-</offsets></mml:mo><mml:mn><offsets xml_i="24099" xml_f="24100" txt_i="14711" txt_f="14712">1</offsets></mml:mn><mml:mo><offsets xml_i="24117" xml_f="24118" txt_i="14712" txt_f="14713">∕</offsets></mml:mo><mml:mn><offsets xml_i="24135" xml_f="24136" txt_i="14713" txt_f="14714">2</offsets></mml:mn></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="24178" xml_f="24179" txt_i="14714" txt_f="14715">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item></list><p><offsets xml_i="24273" xml_f="24321" txt_i="14716" txt_f="14764">From Equation (1) and the description in Figure </offsets><xref ref-type="fig" rid="F1"><offsets xml_i="24351" xml_f="24352" txt_i="14764" txt_f="14765">1</offsets></xref><offsets xml_i="24359" xml_f="24374" txt_i="14765" txt_f="14780">, we note that </offsets><italic><offsets xml_i="24382" xml_f="24383" txt_i="14780" txt_f="14781">C</offsets></italic><offsets xml_i="24392" xml_f="24472" txt_i="14781" txt_f="14861"> has structure and could be visualized as a cross-covariance matrix between the </offsets><italic><offsets xml_i="24480" xml_f="24481" txt_i="14861" txt_f="14862">M</offsets></italic><offsets xml_i="24490" xml_f="24526" txt_i="14862" txt_f="14898"> subject-level PCA components, with </offsets><inline-formula><mml:math id="M4"><mml:msub><mml:mrow><mml:mi><offsets xml_i="24588" xml_f="24589" txt_i="14898" txt_f="14899">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="24627" xml_f="24628" txt_i="14899" txt_f="14900">i</offsets></mml:mi><mml:mi><offsets xml_i="24645" xml_f="24646" txt_i="14900" txt_f="14901">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="24685" xml_f="24686" txt_i="14901" txt_f="14902">=</offsets></mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="24747" xml_f="24748" txt_i="14902" txt_f="14903">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="24786" xml_f="24787" txt_i="14903" txt_f="14904">i</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="24825" xml_f="24826" txt_i="14904" txt_f="14905">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="24888" xml_f="24889" txt_i="14905" txt_f="14906">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="24927" xml_f="24928" txt_i="14906" txt_f="14907">j</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="24988" xml_f="24989" txt_i="14907" txt_f="14908">v</offsets></mml:mi><mml:mo><offsets xml_i="25006" xml_f="25007" txt_i="14908" txt_f="14909">-</offsets></mml:mo><mml:mn><offsets xml_i="25024" xml_f="25025" txt_i="14909" txt_f="14910">1</offsets></mml:mn></mml:mrow></mml:mfrac><mml:mo><offsets xml_i="25065" xml_f="25066" txt_i="14910" txt_f="14911">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="25103" xml_f="25104" txt_i="14911" txt_f="14912">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="25142" xml_f="25143" txt_i="14912" txt_f="14913">j</offsets></mml:mi><mml:mi><offsets xml_i="25160" xml_f="25161" txt_i="14913" txt_f="14914">i</offsets></mml:mi></mml:mrow></mml:msub></mml:math></inline-formula><offsets xml_i="25220" xml_f="25237" txt_i="14914" txt_f="14931"> as shown below:
</offsets><disp-formula id="E4"><label><offsets xml_i="25266" xml_f="25269" txt_i="14931" txt_f="14934">(4)</offsets></label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="25352" xml_f="25353" txt_i="14934" txt_f="14935">C</offsets></mml:mi><mml:mo><offsets xml_i="25370" xml_f="25371" txt_i="14935" txt_f="14936">=</offsets></mml:mo><mml:mrow><mml:mo><offsets xml_i="25398" xml_f="25399" txt_i="14936" txt_f="14937">[</offsets></mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none none none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="25583" xml_f="25584" txt_i="14937" txt_f="14938">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="25622" xml_f="25624" txt_i="14938" txt_f="14940">11</offsets></mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="25702" xml_f="25703" txt_i="14940" txt_f="14941">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="25741" xml_f="25743" txt_i="14941" txt_f="14943">12</offsets></mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo><offsets xml_i="25801" xml_f="25802" txt_i="14943" txt_f="14944">…</offsets></mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="25858" xml_f="25859" txt_i="14944" txt_f="14945">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="25897" xml_f="25898" txt_i="14945" txt_f="14946">1</offsets></mml:mn><mml:mi><offsets xml_i="25915" xml_f="25916" txt_i="14946" txt_f="14947">M</offsets></mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="26013" xml_f="26014" txt_i="14947" txt_f="14948">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="26052" xml_f="26054" txt_i="14948" txt_f="14950">21</offsets></mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="26132" xml_f="26133" txt_i="14950" txt_f="14951">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="26171" xml_f="26173" txt_i="14951" txt_f="14953">22</offsets></mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo><offsets xml_i="26231" xml_f="26232" txt_i="14953" txt_f="14954">…</offsets></mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="26288" xml_f="26289" txt_i="14954" txt_f="14955">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="26327" xml_f="26328" txt_i="14955" txt_f="14956">2</offsets></mml:mn><mml:mi><offsets xml_i="26345" xml_f="26346" txt_i="14956" txt_f="14957">M</offsets></mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo><offsets xml_i="26423" xml_f="26424" txt_i="14957" txt_f="14958">⋮</offsets></mml:mo></mml:mtd><mml:mtd><mml:mo><offsets xml_i="26460" xml_f="26461" txt_i="14958" txt_f="14959">⋮</offsets></mml:mo></mml:mtd><mml:mtd><mml:mo><offsets xml_i="26497" xml_f="26498" txt_i="14959" txt_f="14960">⋮</offsets></mml:mo></mml:mtd><mml:mtd><mml:mo><offsets xml_i="26534" xml_f="26535" txt_i="14960" txt_f="14961">⋮</offsets></mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="26610" xml_f="26611" txt_i="14961" txt_f="14962">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="26649" xml_f="26650" txt_i="14962" txt_f="14963">M</offsets></mml:mi><mml:mn><offsets xml_i="26667" xml_f="26668" txt_i="14963" txt_f="14964">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="26746" xml_f="26747" txt_i="14964" txt_f="14965">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="26785" xml_f="26786" txt_i="14965" txt_f="14966">M</offsets></mml:mi><mml:mn><offsets xml_i="26803" xml_f="26804" txt_i="14966" txt_f="14967">2</offsets></mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo><offsets xml_i="26862" xml_f="26863" txt_i="14967" txt_f="14968">…</offsets></mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="26919" xml_f="26920" txt_i="14968" txt_f="14969">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="26958" xml_f="26959" txt_i="14969" txt_f="14970">M</offsets></mml:mi><mml:mi><offsets xml_i="26976" xml_f="26977" txt_i="14970" txt_f="14971">M</offsets></mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo><offsets xml_i="27060" xml_f="27061" txt_i="14971" txt_f="14972">]</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="27089" xml_f="27090" txt_i="14972" txt_f="14973">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><offsets xml_i="27165" xml_f="27192" txt_i="14974" txt_f="15001">Exploiting this structure, </offsets><italic><offsets xml_i="27200" xml_f="27201" txt_i="15001" txt_f="15002">C</offsets></italic><offsets xml_i="27210" xml_f="27315" txt_i="15002" txt_f="15107"> can be computed with only two datasets in memory at a time, instead of stacking the entire data to form </offsets><italic><offsets xml_i="27323" xml_f="27324" txt_i="15107" txt_f="15108">Y</offsets></italic><offsets xml_i="27333" xml_f="27395" txt_i="15108" txt_f="15170">, leading to fewer computations and memory usage. However, as </offsets><italic><offsets xml_i="27403" xml_f="27404" txt_i="15170" txt_f="15171">M</offsets></italic><offsets xml_i="27413" xml_f="27504" txt_i="15171" txt_f="15262"> increases, computation of the cross-covariance matrix becomes very slow since it requires </offsets><inline-formula><mml:math id="M6"><mml:mfrac><mml:mrow><mml:mi><offsets xml_i="27567" xml_f="27568" txt_i="15262" txt_f="15263">M</offsets></mml:mi><mml:mrow><mml:mo><offsets xml_i="27595" xml_f="27596" txt_i="15263" txt_f="15264">(</offsets></mml:mo><mml:mrow><mml:mi><offsets xml_i="27623" xml_f="27624" txt_i="15264" txt_f="15265">M</offsets></mml:mi><mml:mo><offsets xml_i="27641" xml_f="27642" txt_i="15265" txt_f="15266">-</offsets></mml:mo><mml:mn><offsets xml_i="27659" xml_f="27660" txt_i="15266" txt_f="15267">1</offsets></mml:mn></mml:mrow><mml:mo><offsets xml_i="27688" xml_f="27689" txt_i="15267" txt_f="15268">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn><offsets xml_i="27738" xml_f="27739" txt_i="15268" txt_f="15269">2</offsets></mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula><offsets xml_i="27799" xml_f="27895" txt_i="15269" txt_f="15365"> steps and an equal number of dataloads (hard disk swaps and data transfers). Therefore, EVD of </offsets><italic><offsets xml_i="27903" xml_f="27904" txt_i="15365" txt_f="15366">C</offsets></italic><offsets xml_i="27913" xml_f="28041" txt_i="15366" txt_f="15494"> is a fast solution for small datasets but can quickly become inefficient on very large datasets. Moreover, Equation (3) incurs </offsets><italic><offsets xml_i="28049" xml_f="28050" txt_i="15494" txt_f="15495">M</offsets></italic><offsets xml_i="28059" xml_f="28251" txt_i="15495" txt_f="15687"> additional dataloads, though it allows a memory efficient implementation with only one dataset in memory at a time. Also, note that computing the covariance matrix itself has time complexity </offsets><italic><offsets xml_i="28259" xml_f="28260" txt_i="15687" txt_f="15688">O</offsets></italic><offsets xml_i="28269" xml_f="28270" txt_i="15688" txt_f="15689">(</offsets><italic><offsets xml_i="28278" xml_f="28279" txt_i="15689" txt_f="15690">v</offsets></italic><offsets xml_i="28288" xml_f="28289" txt_i="15690" txt_f="15691">(</offsets><italic><offsets xml_i="28297" xml_f="28299" txt_i="15691" txt_f="15693">Mp</offsets></italic><offsets xml_i="28308" xml_f="28309" txt_i="15693" txt_f="15694">)</offsets><sup><offsets xml_i="28314" xml_f="28315" txt_i="15694" txt_f="15695">2</offsets></sup><offsets xml_i="28321" xml_f="28328" txt_i="15695" txt_f="15702">) when </offsets><italic><offsets xml_i="28336" xml_f="28338" txt_i="15702" txt_f="15704">Mp</offsets></italic><offsets xml_i="28347" xml_f="28353" txt_i="15704" txt_f="15707"> &lt; </offsets><italic><offsets xml_i="28361" xml_f="28362" txt_i="15707" txt_f="15708">v</offsets></italic><offsets xml_i="28371" xml_f="28376" txt_i="15708" txt_f="15713"> and </offsets><italic><offsets xml_i="28384" xml_f="28385" txt_i="15713" txt_f="15714">O</offsets></italic><offsets xml_i="28394" xml_f="28395" txt_i="15714" txt_f="15715">(</offsets><italic><offsets xml_i="28403" xml_f="28406" txt_i="15715" txt_f="15718">Mpv</offsets></italic><sup><offsets xml_i="28420" xml_f="28421" txt_i="15718" txt_f="15719">2</offsets></sup><offsets xml_i="28427" xml_f="28434" txt_i="15719" txt_f="15726">) when </offsets><italic><offsets xml_i="28442" xml_f="28443" txt_i="15726" txt_f="15727">v</offsets></italic><offsets xml_i="28452" xml_f="28458" txt_i="15727" txt_f="15730"> &lt; </offsets><italic><offsets xml_i="28466" xml_f="28468" txt_i="15730" txt_f="15732">Mp</offsets></italic><offsets xml_i="28477" xml_f="28647" txt_i="15732" txt_f="15902">. The latter, however, has the convenience of requiring only one dataload per subject. Even better, in that case the covariance matrix in the voxel dimension, defined as </offsets><inline-formula><mml:math id="M7"><mml:msup><mml:mrow><mml:mi><offsets xml_i="28709" xml_f="28710" txt_i="15902" txt_f="15903">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="28748" xml_f="28749" txt_i="15903" txt_f="15904">v</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="28788" xml_f="28789" txt_i="15904" txt_f="15905">=</offsets></mml:mo><mml:mfrac><mml:mrow><mml:mn><offsets xml_i="28827" xml_f="28828" txt_i="15905" txt_f="15906">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="28866" xml_f="28867" txt_i="15906" txt_f="15907">v</offsets></mml:mi><mml:mo><offsets xml_i="28884" xml_f="28885" txt_i="15907" txt_f="15908">-</offsets></mml:mo><mml:mn><offsets xml_i="28902" xml_f="28903" txt_i="15908" txt_f="15909">1</offsets></mml:mn></mml:mrow></mml:mfrac><mml:mi><offsets xml_i="28943" xml_f="28944" txt_i="15909" txt_f="15910">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="28981" xml_f="28982" txt_i="15910" txt_f="15911">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="29020" xml_f="29021" txt_i="15911" txt_f="15912">T</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="29080" xml_f="29124" txt_i="15912" txt_f="15956"> in order to retain the same eigenvalues as </offsets><italic><offsets xml_i="29132" xml_f="29133" txt_i="15956" txt_f="15957">C</offsets></italic><offsets xml_i="29142" xml_f="29200" txt_i="15957" txt_f="16015">, can be written as a sum of subject-specific covariances </offsets><inline-formula><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="29265" xml_f="29266" txt_i="16015" txt_f="16016">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="29304" xml_f="29305" txt_i="16016" txt_f="16017">i</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="29343" xml_f="29344" txt_i="16017" txt_f="16018">v</offsets></mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula><offsets xml_i="29406" xml_f="29456" txt_i="16018" txt_f="16068">, and an efficient approach to compute the EVD of </offsets><italic><offsets xml_i="29464" xml_f="29465" txt_i="16068" txt_f="16069">C</offsets></italic><sup><italic><offsets xml_i="29487" xml_f="29488" txt_i="16069" txt_f="16070">v</offsets></italic></sup><offsets xml_i="29503" xml_f="29508" txt_i="16070" txt_f="16075"> is:
</offsets><disp-formula id="E5"><label><offsets xml_i="29537" xml_f="29540" txt_i="16075" txt_f="16078">(5)</offsets></label><mml:math id="M9"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi><offsets xml_i="29643" xml_f="29644" txt_i="16078" txt_f="16079">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="29682" xml_f="29683" txt_i="16079" txt_f="16080">v</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="29722" xml_f="29723" txt_i="16080" txt_f="16081">=</offsets></mml:mo><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo><offsets xml_i="29833" xml_f="29834" txt_i="16081" txt_f="16082">∑</offsets></mml:mo></mml:mrow><mml:mrow><mml:mi><offsets xml_i="29872" xml_f="29873" txt_i="16082" txt_f="16083">i</offsets></mml:mi><mml:mo><offsets xml_i="29890" xml_f="29891" txt_i="16083" txt_f="16084">=</offsets></mml:mo><mml:mn><offsets xml_i="29908" xml_f="29909" txt_i="16084" txt_f="16085">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="29947" xml_f="29948" txt_i="16085" txt_f="16086">M</offsets></mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="30029" xml_f="30030" txt_i="16086" txt_f="16087">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="30068" xml_f="30069" txt_i="16087" txt_f="16088">i</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="30107" xml_f="30108" txt_i="16088" txt_f="16089">v</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="30150" xml_f="30151" txt_i="16089" txt_f="16090">=</offsets></mml:mo><mml:mi><offsets xml_i="30168" xml_f="30169" txt_i="16090" txt_f="16091">χ</offsets></mml:mi><mml:mi><offsets xml_i="30186" xml_f="30187" txt_i="16091" txt_f="16092">Λ</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="30224" xml_f="30225" txt_i="16092" txt_f="16093">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="30263" xml_f="30264" txt_i="16093" txt_f="16094">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="30303" xml_f="30304" txt_i="16094" txt_f="16095">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><offsets xml_i="30379" xml_f="30442" txt_i="16096" txt_f="16159">The final estimate of the group-level PCA space is obtained as </offsets><inline-formula><mml:math id="M10"><mml:mi><offsets xml_i="30485" xml_f="30486" txt_i="16159" txt_f="16160">X</offsets></mml:mi><mml:mo><offsets xml_i="30503" xml_f="30504" txt_i="16160" txt_f="16161">=</offsets></mml:mo><mml:mi><offsets xml_i="30521" xml_f="30522" txt_i="16161" txt_f="16162">χ</offsets></mml:mi><mml:msqrt><mml:mrow><mml:mi><offsets xml_i="30560" xml_f="30561" txt_i="16162" txt_f="16163">v</offsets></mml:mi><mml:mo><offsets xml_i="30578" xml_f="30579" txt_i="16163" txt_f="16164">-</offsets></mml:mo><mml:mn><offsets xml_i="30596" xml_f="30597" txt_i="16164" txt_f="16165">1</offsets></mml:mn></mml:mrow></mml:msqrt></mml:math></inline-formula><offsets xml_i="30657" xml_f="30700" txt_i="16165" txt_f="16208">. Equation (5) gives an upper bound on the </offsets><italic><offsets xml_i="30708" xml_f="30714" txt_i="16208" txt_f="16214">memory</offsets></italic><offsets xml_i="30723" xml_f="30783" txt_i="16214" txt_f="16274"> required for group ICA of fMRI using the EVD method [i.e., </offsets><italic><offsets xml_i="30791" xml_f="30792" txt_i="16274" txt_f="16275">O</offsets></italic><offsets xml_i="30801" xml_f="30802" txt_i="16275" txt_f="16276">(</offsets><italic><offsets xml_i="30810" xml_f="30811" txt_i="16276" txt_f="16277">v</offsets></italic><sup><offsets xml_i="30825" xml_f="30826" txt_i="16277" txt_f="16278">2</offsets></sup><offsets xml_i="30832" xml_f="30868" txt_i="16278" txt_f="16314">) bytes], and note it only requires </offsets><italic><offsets xml_i="30876" xml_f="30877" txt_i="16314" txt_f="16315">M</offsets></italic><offsets xml_i="30886" xml_f="30922" txt_i="16315" txt_f="16351"> dataloads. Particularly, for large-</offsets><italic><offsets xml_i="30930" xml_f="30931" txt_i="16351" txt_f="16352">M</offsets></italic><offsets xml_i="30940" xml_f="30984" txt_i="16352" txt_f="16396"> region of interest- (ROI-) based group ICA </offsets><italic><offsets xml_i="30992" xml_f="30993" txt_i="16396" txt_f="16397">v</offsets></italic><offsets xml_i="31002" xml_f="31005" txt_i="16397" txt_f="16400"> ≪ </offsets><italic><offsets xml_i="31013" xml_f="31015" txt_i="16400" txt_f="16402">Mp</offsets></italic><offsets xml_i="31024" xml_f="31175" txt_i="16402" txt_f="16553">. Thus, the left-hand equality in Equation (5) is highly efficient in memory for big ROI studies and should be the method of choice for computation of </offsets><italic><offsets xml_i="31183" xml_f="31184" txt_i="16553" txt_f="16554">C</offsets></italic><sup><italic><offsets xml_i="31206" xml_f="31207" txt_i="16554" txt_f="16555">v</offsets></italic></sup><offsets xml_i="31222" xml_f="31278" txt_i="16555" txt_f="16611"> rather than stacking the entire data in memory to form </offsets><italic><offsets xml_i="31286" xml_f="31287" txt_i="16611" txt_f="16612">Y</offsets></italic><offsets xml_i="31296" xml_f="31297" txt_i="16612" txt_f="16613">.</offsets></p><p><offsets xml_i="31304" xml_f="31399" txt_i="16614" txt_f="16709">Clearly, trade-offs exist between time, memory, and dataloads depending on the exact values of </offsets><italic><offsets xml_i="31407" xml_f="31408" txt_i="16709" txt_f="16710">v</offsets></italic><offsets xml_i="31417" xml_f="31419" txt_i="16710" txt_f="16712">, </offsets><italic><offsets xml_i="31427" xml_f="31428" txt_i="16712" txt_f="16713">M</offsets></italic><offsets xml_i="31437" xml_f="31443" txt_i="16713" txt_f="16719">, and </offsets><italic><offsets xml_i="31451" xml_f="31452" txt_i="16719" txt_f="16720">p</offsets></italic><offsets xml_i="31461" xml_f="31727" txt_i="16720" txt_f="16986">. In some cases, which we consider later, it might be worth giving up on some computing speed in exchange for a largely disproportional improvement (reduction) in dataloads or memory footprint, and vice-versa, especially if numerical accuracy with respect to EVD of </offsets><italic><offsets xml_i="31735" xml_f="31736" txt_i="16986" txt_f="16987">C</offsets></italic><offsets xml_i="31745" xml_f="31823" txt_i="16987" txt_f="17065"> is not compromised. This will be a recurring theme in the following sections.</offsets></p></sec><sec><title><offsets xml_i="31845" xml_f="31859" txt_i="17067" txt_f="17081">Subsampled PCA</offsets></title><p><offsets xml_i="31870" xml_f="31997" txt_i="17082" txt_f="17209">While EVD is very well-developed and accurate, it still becomes computationally and memory intensive when applied to large data</offsets><xref ref-type="fn" rid="fn0004"><sup><offsets xml_i="32035" xml_f="32036" txt_i="17209" txt_f="17210">4</offsets></sup></xref></p><sec><title><offsets xml_i="32065" xml_f="32091" txt_i="17211" txt_f="17237">Subsampled voxel PCA (SVP)</offsets></title><p><offsets xml_i="32102" xml_f="32169" txt_i="17238" txt_f="17305">Subsampled voxel PCA (SVP) works by setting up a scenario in which </offsets><italic><offsets xml_i="32177" xml_f="32178" txt_i="17305" txt_f="17306">v</offsets></italic><offsets xml_i="32187" xml_f="32193" txt_i="17306" txt_f="17309"> &lt; </offsets><italic><offsets xml_i="32201" xml_f="32203" txt_i="17309" txt_f="17311">Mp</offsets></italic><offsets xml_i="32212" xml_f="32273" txt_i="17311" txt_f="17372"> so that Equation (5) can be used to efficiently approximate </offsets><italic><offsets xml_i="32281" xml_f="32282" txt_i="17372" txt_f="17373">X</offsets></italic><offsets xml_i="32291" xml_f="32667" txt_i="17373" txt_f="17749"> with only a few dataloads per subject. First, consider that, typically, fMRI data is spatially smoothed during the initial preprocessing to improve signal-to-noise ratio (SNR), which introduces data dependency in the spatial domain. Therefore, the actual number of independent and identically distributed (i.i.d.) samples present in the data is less than the voxel dimension </offsets><italic><offsets xml_i="32675" xml_f="32676" txt_i="17749" txt_f="17750">v</offsets></italic><offsets xml_i="32685" xml_f="32698" txt_i="17750" txt_f="17763"> (Li et al., </offsets><xref rid="B24" ref-type="bibr"><offsets xml_i="32730" xml_f="32734" txt_i="17763" txt_f="17767">2007</offsets></xref><offsets xml_i="32741" xml_f="33013" txt_i="17767" txt_f="18039">). A set of approximately independent samples could be obtained by subsampling the data in the voxel dimension. In our experiments, we selected a value of 2 for subsampling depth in x, y, and z directions as this only reduces the number of in-brain voxels by a factor of 2</offsets><sup><offsets xml_i="33018" xml_f="33019" txt_i="18039" txt_f="18040">3</offsets></sup><offsets xml_i="33025" xml_f="33037" txt_i="18040" txt_f="18052"> = 8, i.e., </offsets><italic><offsets xml_i="33045" xml_f="33046" txt_i="18052" txt_f="18053">v</offsets></italic><offsets xml_i="33055" xml_f="33059" txt_i="18053" txt_f="18057">′ = </offsets><italic><offsets xml_i="33067" xml_f="33068" txt_i="18057" txt_f="18058">v</offsets></italic><offsets xml_i="33077" xml_f="33258" txt_i="18058" txt_f="18239">∕8. Thus, the resulting covariance matrix is still fairly approximate to the original one. All-odd and all-even subsampling depths are processed separately to estimate eigenvectors </offsets><italic><offsets xml_i="33266" xml_f="33267" txt_i="18239" txt_f="18240">F</offsets></italic><sub><italic><offsets xml_i="33289" xml_f="33290" txt_i="18240" txt_f="18241">a</offsets></italic></sub><offsets xml_i="33305" xml_f="33310" txt_i="18241" txt_f="18246"> and </offsets><italic><offsets xml_i="33318" xml_f="33319" txt_i="18246" txt_f="18247">F</offsets></italic><sub><italic><offsets xml_i="33341" xml_f="33342" txt_i="18247" txt_f="18248">b</offsets></italic></sub><offsets xml_i="33357" xml_f="33417" txt_i="18248" txt_f="18308">, respectively, using EVD with Equation (5) since typically </offsets><italic><offsets xml_i="33425" xml_f="33426" txt_i="18308" txt_f="18309">v</offsets></italic><offsets xml_i="33435" xml_f="33447" txt_i="18309" txt_f="18315">′ &lt; &lt; </offsets><italic><offsets xml_i="33455" xml_f="33457" txt_i="18315" txt_f="18317">Mp</offsets></italic><offsets xml_i="33466" xml_f="33468" txt_i="18317" txt_f="18319">. </offsets><italic><offsets xml_i="33476" xml_f="33477" txt_i="18319" txt_f="18320">F</offsets></italic><sub><italic><offsets xml_i="33499" xml_f="33500" txt_i="18320" txt_f="18321">a</offsets></italic></sub><offsets xml_i="33515" xml_f="33520" txt_i="18321" txt_f="18326"> and </offsets><italic><offsets xml_i="33528" xml_f="33529" txt_i="18326" txt_f="18327">F</offsets></italic><sub><italic><offsets xml_i="33551" xml_f="33552" txt_i="18327" txt_f="18328">b</offsets></italic></sub><offsets xml_i="33567" xml_f="33596" txt_i="18328" txt_f="18357"> are projected onto the data </offsets><italic><offsets xml_i="33604" xml_f="33605" txt_i="18357" txt_f="18358">Y</offsets></italic><offsets xml_i="33614" xml_f="33646" txt_i="18358" txt_f="18390"> (Equation 8) in order to bring </offsets><italic><offsets xml_i="33654" xml_f="33655" txt_i="18390" txt_f="18391">X</offsets></italic><sub><italic><offsets xml_i="33677" xml_f="33678" txt_i="18391" txt_f="18392">a</offsets></italic></sub><offsets xml_i="33693" xml_f="33698" txt_i="18392" txt_f="18397"> and </offsets><italic><offsets xml_i="33706" xml_f="33707" txt_i="18397" txt_f="18398">X</offsets></italic><sub><italic><offsets xml_i="33729" xml_f="33730" txt_i="18398" txt_f="18399">b</offsets></italic></sub><offsets xml_i="33745" xml_f="33779" txt_i="18399" txt_f="18433">, respectively, back to dimension </offsets><italic><offsets xml_i="33787" xml_f="33788" txt_i="18433" txt_f="18434">v</offsets></italic><offsets xml_i="33797" xml_f="33803" txt_i="18434" txt_f="18440"> from </offsets><italic><offsets xml_i="33811" xml_f="33812" txt_i="18440" txt_f="18441">v</offsets></italic><offsets xml_i="33821" xml_f="33937" txt_i="18441" txt_f="18557">′. These are finally stacked in the time dimension to determine a final (common) PCA subspace (Equations 9 and 10):
</offsets><disp-formula id="E6"><label><offsets xml_i="33966" xml_f="33969" txt_i="18557" txt_f="18560">(6)</offsets></label><mml:math id="M11"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="34106" xml_f="34107" txt_i="18560" txt_f="18561">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34145" xml_f="34146" txt_i="18561" txt_f="18562">a</offsets></mml:mi><mml:mi><offsets xml_i="34163" xml_f="34164" txt_i="18562" txt_f="18563">a</offsets></mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi><offsets xml_i="34222" xml_f="34223" txt_i="18563" txt_f="18564">v</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34261" xml_f="34262" txt_i="18564" txt_f="18565">′</offsets></mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="34326" xml_f="34327" txt_i="18565" txt_f="18566">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="34364" xml_f="34365" txt_i="18566" txt_f="18567">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34403" xml_f="34404" txt_i="18567" txt_f="18568">a</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="34463" xml_f="34464" txt_i="18568" txt_f="18569">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34502" xml_f="34503" txt_i="18569" txt_f="18570">a</offsets></mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="34565" xml_f="34566" txt_i="18570" txt_f="18571">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34604" xml_f="34605" txt_i="18571" txt_f="18572">a</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34643" xml_f="34644" txt_i="18572" txt_f="18573">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="34686" xml_f="34687" txt_i="18573" txt_f="18574">,</offsets></mml:mo><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="34727" xml_f="34728" txt_i="18574" txt_f="18575">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34766" xml_f="34767" txt_i="18575" txt_f="18576">b</offsets></mml:mi><mml:mi><offsets xml_i="34784" xml_f="34785" txt_i="18576" txt_f="18577">b</offsets></mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi><offsets xml_i="34843" xml_f="34844" txt_i="18577" txt_f="18578">v</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="34882" xml_f="34883" txt_i="18578" txt_f="18579">′</offsets></mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="34947" xml_f="34948" txt_i="18579" txt_f="18580">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="34985" xml_f="34986" txt_i="18580" txt_f="18581">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35024" xml_f="35025" txt_i="18581" txt_f="18582">b</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="35084" xml_f="35085" txt_i="18582" txt_f="18583">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35123" xml_f="35124" txt_i="18583" txt_f="18584">b</offsets></mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="35186" xml_f="35187" txt_i="18584" txt_f="18585">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35225" xml_f="35226" txt_i="18585" txt_f="18586">b</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35264" xml_f="35265" txt_i="18586" txt_f="18587">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="35307" xml_f="35308" txt_i="18587" txt_f="18588">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="35376" xml_f="35377" txt_i="18588" txt_f="18589">
</offsets><disp-formula id="E7"><label><offsets xml_i="35406" xml_f="35409" txt_i="18589" txt_f="18592">(7)</offsets></label><mml:math id="M12"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="35543" xml_f="35544" txt_i="18592" txt_f="18593">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35582" xml_f="35583" txt_i="18593" txt_f="18594">a</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="35622" xml_f="35623" txt_i="18594" txt_f="18595">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false"><offsets xml_i="35687" xml_f="35688" txt_i="18595" txt_f="18596">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="35738" xml_f="35739" txt_i="18596" txt_f="18597">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35777" xml_f="35778" txt_i="18597" txt_f="18598">a</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35816" xml_f="35817" txt_i="18598" txt_f="18599">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="35879" xml_f="35880" txt_i="18599" txt_f="18600">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="35918" xml_f="35919" txt_i="18600" txt_f="18601">a</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false"><offsets xml_i="35986" xml_f="35987" txt_i="18601" txt_f="18602">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="36036" xml_f="36037" txt_i="18602" txt_f="18603">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="36076" xml_f="36077" txt_i="18603" txt_f="18604">,</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="36114" xml_f="36115" txt_i="18604" txt_f="18605">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="36153" xml_f="36154" txt_i="18605" txt_f="18606">b</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="36193" xml_f="36194" txt_i="18606" txt_f="18607">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false"><offsets xml_i="36258" xml_f="36259" txt_i="18607" txt_f="18608">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="36309" xml_f="36310" txt_i="18608" txt_f="18609">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="36348" xml_f="36349" txt_i="18609" txt_f="18610">b</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="36387" xml_f="36388" txt_i="18610" txt_f="18611">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="36450" xml_f="36451" txt_i="18611" txt_f="18612">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="36489" xml_f="36490" txt_i="18612" txt_f="18613">b</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false"><offsets xml_i="36557" xml_f="36558" txt_i="18613" txt_f="18614">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="36607" xml_f="36608" txt_i="18614" txt_f="18615">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="36647" xml_f="36648" txt_i="18615" txt_f="18616">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="36716" xml_f="36717" txt_i="18616" txt_f="18617">
</offsets><disp-formula id="E8"><label><offsets xml_i="36746" xml_f="36749" txt_i="18617" txt_f="18620">(8)</offsets></label><mml:math id="M13"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="36883" xml_f="36884" txt_i="18620" txt_f="18621">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="36922" xml_f="36923" txt_i="18621" txt_f="18622">a</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="36962" xml_f="36963" txt_i="18622" txt_f="18623">=</offsets></mml:mo><mml:mi><offsets xml_i="36980" xml_f="36981" txt_i="18623" txt_f="18624">Y</offsets></mml:mi><mml:msub><mml:mrow><mml:mi><offsets xml_i="37018" xml_f="37019" txt_i="18624" txt_f="18625">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="37057" xml_f="37058" txt_i="18625" txt_f="18626">a</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="37097" xml_f="37098" txt_i="18626" txt_f="18627">,</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="37135" xml_f="37136" txt_i="18627" txt_f="18628">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="37174" xml_f="37175" txt_i="18628" txt_f="18629">b</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="37214" xml_f="37215" txt_i="18629" txt_f="18630">=</offsets></mml:mo><mml:mi><offsets xml_i="37232" xml_f="37233" txt_i="18630" txt_f="18631">Y</offsets></mml:mi><mml:msub><mml:mrow><mml:mi><offsets xml_i="37270" xml_f="37271" txt_i="18631" txt_f="18632">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="37309" xml_f="37310" txt_i="18632" txt_f="18633">b</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="37349" xml_f="37350" txt_i="18633" txt_f="18634">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="37418" xml_f="37419" txt_i="18634" txt_f="18635">
</offsets><disp-formula id="E9"><label><offsets xml_i="37448" xml_f="37451" txt_i="18635" txt_f="18638">(9)</offsets></label><mml:math id="M14"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mrow><mml:mo><offsets xml_i="37575" xml_f="37576" txt_i="18638" txt_f="18639">[</offsets></mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="37774" xml_f="37775" txt_i="18639" txt_f="18640">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="37813" xml_f="37814" txt_i="18640" txt_f="18641">a</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="37852" xml_f="37853" txt_i="18641" txt_f="18642">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="37915" xml_f="37916" txt_i="18642" txt_f="18643">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="37954" xml_f="37955" txt_i="18643" txt_f="18644">a</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38015" xml_f="38016" txt_i="18644" txt_f="18645">v</offsets></mml:mi><mml:mo><offsets xml_i="38033" xml_f="38034" txt_i="18645" txt_f="18646">-</offsets></mml:mo><mml:mn><offsets xml_i="38051" xml_f="38052" txt_i="18646" txt_f="18647">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="38155" xml_f="38156" txt_i="18647" txt_f="18648">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38194" xml_f="38195" txt_i="18648" txt_f="18649">a</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38233" xml_f="38234" txt_i="18649" txt_f="18650">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="38296" xml_f="38297" txt_i="18650" txt_f="18651">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38335" xml_f="38336" txt_i="18651" txt_f="18652">b</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38396" xml_f="38397" txt_i="18652" txt_f="18653">v</offsets></mml:mi><mml:mo><offsets xml_i="38414" xml_f="38415" txt_i="18653" txt_f="18654">-</offsets></mml:mo><mml:mn><offsets xml_i="38432" xml_f="38433" txt_i="18654" txt_f="18655">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="38555" xml_f="38556" txt_i="18655" txt_f="18656">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38594" xml_f="38595" txt_i="18656" txt_f="18657">b</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38633" xml_f="38634" txt_i="18657" txt_f="18658">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="38696" xml_f="38697" txt_i="18658" txt_f="18659">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38735" xml_f="38736" txt_i="18659" txt_f="18660">a</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38796" xml_f="38797" txt_i="18660" txt_f="18661">v</offsets></mml:mi><mml:mo><offsets xml_i="38814" xml_f="38815" txt_i="18661" txt_f="18662">-</offsets></mml:mo><mml:mn><offsets xml_i="38832" xml_f="38833" txt_i="18662" txt_f="18663">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="38936" xml_f="38937" txt_i="18663" txt_f="18664">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="38975" xml_f="38976" txt_i="18664" txt_f="18665">b</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="39014" xml_f="39015" txt_i="18665" txt_f="18666">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="39077" xml_f="39078" txt_i="18666" txt_f="18667">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="39116" xml_f="39117" txt_i="18667" txt_f="18668">b</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="39177" xml_f="39178" txt_i="18668" txt_f="18669">v</offsets></mml:mi><mml:mo><offsets xml_i="39195" xml_f="39196" txt_i="18669" txt_f="18670">-</offsets></mml:mo><mml:mn><offsets xml_i="39213" xml_f="39214" txt_i="18670" txt_f="18671">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo><offsets xml_i="39298" xml_f="39299" txt_i="18671" txt_f="18672">]</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="39327" xml_f="39328" txt_i="18672" txt_f="18673">=</offsets></mml:mo><mml:mi><offsets xml_i="39345" xml_f="39346" txt_i="18673" txt_f="18674">W</offsets></mml:mi><mml:mi><offsets xml_i="39363" xml_f="39364" txt_i="18674" txt_f="18675">Λ</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="39401" xml_f="39402" txt_i="18675" txt_f="18676">W</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="39440" xml_f="39441" txt_i="18676" txt_f="18677">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="39480" xml_f="39481" txt_i="18677" txt_f="18678">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="39549" xml_f="39550" txt_i="18678" txt_f="18679">
</offsets><disp-formula id="E10"><label><offsets xml_i="39580" xml_f="39584" txt_i="18679" txt_f="18683">(10)</offsets></label><mml:math id="M15"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="39698" xml_f="39699" txt_i="18683" txt_f="18684">X</offsets></mml:mi><mml:mo><offsets xml_i="39716" xml_f="39717" txt_i="18684" txt_f="18685">=</offsets></mml:mo><mml:mrow><mml:mo><offsets xml_i="39744" xml_f="39745" txt_i="18685" txt_f="18686">[</offsets></mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi><offsets xml_i="39792" xml_f="39793" txt_i="18686" txt_f="18687">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="39831" xml_f="39832" txt_i="18687" txt_f="18688">a</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="39871" xml_f="39872" txt_i="18688" txt_f="18689">,</offsets></mml:mo><mml:mtext><offsets xml_i="39892" xml_f="39893" txt_i="18689" txt_f="18690"> </offsets></mml:mtext><mml:msub><mml:mrow><mml:mi><offsets xml_i="39933" xml_f="39934" txt_i="18690" txt_f="18691">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="39972" xml_f="39973" txt_i="18691" txt_f="18692">b</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="40023" xml_f="40024" txt_i="18692" txt_f="18693">]</offsets></mml:mo></mml:mrow><mml:mi><offsets xml_i="40052" xml_f="40053" txt_i="18693" txt_f="18694">W</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="40090" xml_f="40091" txt_i="18694" txt_f="18695">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mo><offsets xml_i="40129" xml_f="40130" txt_i="18695" txt_f="18696">-</offsets></mml:mo><mml:mn><offsets xml_i="40147" xml_f="40148" txt_i="18696" txt_f="18697">1</offsets></mml:mn><mml:mo><offsets xml_i="40165" xml_f="40166" txt_i="18697" txt_f="18698">∕</offsets></mml:mo><mml:mn><offsets xml_i="40183" xml_f="40184" txt_i="18698" txt_f="18699">2</offsets></mml:mn></mml:mrow></mml:msup><mml:mo><offsets xml_i="40223" xml_f="40224" txt_i="18699" txt_f="18700">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="40292" xml_f="40293" txt_i="18700" txt_f="18701">
</offsets></p><p><italic><offsets xml_i="40308" xml_f="40309" txt_i="18702" txt_f="18703">Y</offsets></italic><sub><italic><offsets xml_i="40331" xml_f="40332" txt_i="18703" txt_f="18704">a</offsets></italic></sub><offsets xml_i="40347" xml_f="40352" txt_i="18704" txt_f="18709"> and </offsets><italic><offsets xml_i="40360" xml_f="40361" txt_i="18709" txt_f="18710">Y</offsets></italic><sub><italic><offsets xml_i="40383" xml_f="40384" txt_i="18710" txt_f="18711">b</offsets></italic></sub><offsets xml_i="40399" xml_f="40470" txt_i="18711" txt_f="18782"> refer to subsampled data in odd- and even-voxel spaces, respectively. </offsets><italic><offsets xml_i="40478" xml_f="40479" txt_i="18782" txt_f="18783">F</offsets></italic><sub><italic><offsets xml_i="40501" xml_f="40502" txt_i="18783" txt_f="18784">a</offsets></italic></sub><offsets xml_i="40517" xml_f="40522" txt_i="18784" txt_f="18789"> and </offsets><italic><offsets xml_i="40530" xml_f="40531" txt_i="18789" txt_f="18790">F</offsets></italic><sub><italic><offsets xml_i="40553" xml_f="40554" txt_i="18790" txt_f="18791">b</offsets></italic></sub><offsets xml_i="40569" xml_f="40604" txt_i="18791" txt_f="18826"> are estimated in subsampled space </offsets><italic><offsets xml_i="40612" xml_f="40613" txt_i="18826" txt_f="18827">v</offsets></italic><offsets xml_i="40622" xml_f="40649" txt_i="18827" txt_f="18854">′ from covariance matrices </offsets><inline-formula><mml:math id="M16"><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="40715" xml_f="40716" txt_i="18854" txt_f="18855">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="40754" xml_f="40755" txt_i="18855" txt_f="18856">a</offsets></mml:mi><mml:mi><offsets xml_i="40772" xml_f="40773" txt_i="18856" txt_f="18857">a</offsets></mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi><offsets xml_i="40831" xml_f="40832" txt_i="18857" txt_f="18858">v</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="40870" xml_f="40871" txt_i="18858" txt_f="18859">′</offsets></mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula><offsets xml_i="40955" xml_f="40960" txt_i="18859" txt_f="18864"> and </offsets><inline-formula><mml:math id="M17"><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="41026" xml_f="41027" txt_i="18864" txt_f="18865">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="41065" xml_f="41066" txt_i="18865" txt_f="18866">b</offsets></mml:mi><mml:mi><offsets xml_i="41083" xml_f="41084" txt_i="18866" txt_f="18867">b</offsets></mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi><offsets xml_i="41142" xml_f="41143" txt_i="18867" txt_f="18868">v</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="41181" xml_f="41182" txt_i="18868" txt_f="18869">′</offsets></mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula><offsets xml_i="41266" xml_f="41368" txt_i="18869" txt_f="18971">, respectively, using Equations (6) and (7). A very high number of components (i.e., much larger than </offsets><italic><offsets xml_i="41376" xml_f="41377" txt_i="18971" txt_f="18972">k</offsets></italic><offsets xml_i="41386" xml_f="41415" txt_i="18972" txt_f="19001">; here, around 500, assuming </offsets><italic><offsets xml_i="41423" xml_f="41424" txt_i="19001" txt_f="19002">k</offsets></italic><offsets xml_i="41433" xml_f="41578" txt_i="19002" txt_f="19147"> ≈ 100) are estimated in this intermediate PCA stage (Equation 6) to minimize error due to approximation. At the end of the estimation, only the </offsets><italic><offsets xml_i="41586" xml_f="41587" txt_i="19147" txt_f="19148">k</offsets></italic><offsets xml_i="41596" xml_f="41636" txt_i="19148" txt_f="19188"> dominant components are extracted from </offsets><italic><offsets xml_i="41644" xml_f="41645" txt_i="19188" txt_f="19189">X</offsets></italic><offsets xml_i="41654" xml_f="41715" txt_i="19189" txt_f="19250"> (Equation 10). Note that the use of Equation (5) for EVD of </offsets><inline-formula><mml:math id="M18"><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="41781" xml_f="41782" txt_i="19250" txt_f="19251">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="41820" xml_f="41821" txt_i="19251" txt_f="19252">a</offsets></mml:mi><mml:mi><offsets xml_i="41838" xml_f="41839" txt_i="19252" txt_f="19253">a</offsets></mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi><offsets xml_i="41897" xml_f="41898" txt_i="19253" txt_f="19254">v</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="41936" xml_f="41937" txt_i="19254" txt_f="19255">′</offsets></mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula><offsets xml_i="42021" xml_f="42026" txt_i="19255" txt_f="19260"> and </offsets><inline-formula><mml:math id="M19"><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="42092" xml_f="42093" txt_i="19260" txt_f="19261">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="42131" xml_f="42132" txt_i="19261" txt_f="19262">b</offsets></mml:mi><mml:mi><offsets xml_i="42149" xml_f="42150" txt_i="19262" txt_f="19263">b</offsets></mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi><offsets xml_i="42208" xml_f="42209" txt_i="19263" txt_f="19264">v</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="42247" xml_f="42248" txt_i="19264" txt_f="19265">′</offsets></mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula><offsets xml_i="42332" xml_f="42461" txt_i="19265" txt_f="19394"> allows SVP to operate in unstacked way (i.e., loading each subject's dataset at a time instead of stacking all datasets to form </offsets><italic><offsets xml_i="42469" xml_f="42470" txt_i="19394" txt_f="19395">Y</offsets></italic><offsets xml_i="42479" xml_f="42590" txt_i="19395" txt_f="19506">) and would require at most two dataloads per subject [one for Equation (6) and one for Equations (7) and (8)].</offsets></p><p><offsets xml_i="42597" xml_f="42882" txt_i="19507" txt_f="19792">SVP is much faster compared to EVD as the voxel dimension is smaller by at least a factor of 8 but only gives an approximate PCA solution. SVP PCA estimates are a great initial solution for any of the randomized PCA methods discussed later, inducing to considerably faster convergence.</offsets></p></sec><sec><title><offsets xml_i="42904" xml_f="42930" txt_i="19794" txt_f="19820">Sub-sampled time PCA (STP)</offsets></title><p><offsets xml_i="42941" xml_f="43052" txt_i="19821" txt_f="19932">The time (stacked) dimension increases as more and more subjects are analyzed in a group PCA framework (Figure </offsets><xref ref-type="fig" rid="F1"><offsets xml_i="43082" xml_f="43083" txt_i="19932" txt_f="19933">1</offsets></xref><offsets xml_i="43090" xml_f="43159" txt_i="19933" txt_f="20002">). By default, initial versions of the GIFT toolbox (Calhoun et al., </offsets><xref rid="B9" ref-type="bibr"><offsets xml_i="43190" xml_f="43194" txt_i="20002" txt_f="20006">2004</offsets></xref><offsets xml_i="43201" xml_f="43352" txt_i="20006" txt_f="20157">) used a three-step data reduction method for large dataset analysis in order to reduce the memory requirements from the group PCA framework of Figure </offsets><xref ref-type="fig" rid="F1"><offsets xml_i="43382" xml_f="43383" txt_i="20157" txt_f="20158">1</offsets></xref><offsets xml_i="43390" xml_f="43485" txt_i="20158" txt_f="20253">. This three-step reduction operated as follows: (1) reduced datasets from the first PCA step (</offsets><italic><offsets xml_i="43493" xml_f="43494" txt_i="20253" txt_f="20254">Y</offsets></italic><sub><italic><offsets xml_i="43516" xml_f="43517" txt_i="20254" txt_f="20255">i</offsets></italic></sub><offsets xml_i="43532" xml_f="43576" txt_i="20255" txt_f="20299">) were randomly organized in groups of size </offsets><italic><offsets xml_i="43584" xml_f="43585" txt_i="20299" txt_f="20300">g</offsets></italic><offsets xml_i="43594" xml_f="43665" txt_i="20300" txt_f="20371"> = 4; (2) PCA was applied on each group separately (including whitening</offsets><xref ref-type="fn" rid="fn0005"><sup><offsets xml_i="43703" xml_f="43704" txt_i="20371" txt_f="20372">5</offsets></sup></xref><offsets xml_i="43717" xml_f="44288" txt_i="20372" txt_f="20943">); (3) reduced group datasets were concatenated and a final PCA step was applied. This approach had the following shortcomings: (1) whitening in the intermediate group PCA (step two above) normalized the variance of components from each group and, therefore, the principal component weights were not correctly reflected in the final PCA step; (2) error of approximation increased if a low number of components was estimated in the intermediate group PCA step; (3) memory overhead increased if higher number of components were estimated in the intermediate group PCA step.</offsets></p><p><offsets xml_i="44295" xml_f="44438" txt_i="20944" txt_f="21087">Here, we present a modified version of this three-step data reduction, which we call sub-sampled time PCA (STP). It estimates the PCA subspace </offsets><italic><offsets xml_i="44446" xml_f="44447" txt_i="21087" txt_f="21088">X</offsets></italic><offsets xml_i="44456" xml_f="44895" txt_i="21088" txt_f="21527"> by incremental updates based on a different group (“sub-sample”) of subjects stacked in time. First, we do not use whitening in the intermediate group PCA (step two above). Second, the final group PCA space is incrementally updated, incorporating the estimates from the previous group PCA before the next group is considered. This reduces the memory overhead incurred by temporal concatenation. Third, a high number of components (around </offsets><italic><offsets xml_i="44903" xml_f="44904" txt_i="21527" txt_f="21528">k</offsets></italic><offsets xml_i="44913" xml_f="45027" txt_i="21528" txt_f="21642">′ = 500) is estimated in every group PCA update. The following equations summarize the proposed STP procedure for </offsets><italic><offsets xml_i="45035" xml_f="45037" txt_i="21642" txt_f="21644">Mp</offsets></italic><offsets xml_i="45046" xml_f="45052" txt_i="21644" txt_f="21647"> &lt; </offsets><italic><offsets xml_i="45060" xml_f="45061" txt_i="21647" txt_f="21648">v</offsets></italic><offsets xml_i="45070" xml_f="45072" txt_i="21648" txt_f="21650">:
</offsets><disp-formula id="E11"><label><offsets xml_i="45102" xml_f="45106" txt_i="21650" txt_f="21654">(11)</offsets></label><mml:math id="M20"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="45240" xml_f="45241" txt_i="21654" txt_f="21655">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="45279" xml_f="45280" txt_i="21655" txt_f="21656">g</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="45319" xml_f="45320" txt_i="21656" txt_f="21657">=</offsets></mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="45381" xml_f="45382" txt_i="21657" txt_f="21658">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="45420" xml_f="45421" txt_i="21658" txt_f="21659">g</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="45459" xml_f="45460" txt_i="21659" txt_f="21660">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="45522" xml_f="45523" txt_i="21660" txt_f="21661">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="45561" xml_f="45562" txt_i="21661" txt_f="21662">g</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="45622" xml_f="45623" txt_i="21662" txt_f="21663">v</offsets></mml:mi><mml:mo><offsets xml_i="45640" xml_f="45641" txt_i="21663" txt_f="21664">-</offsets></mml:mo><mml:mn><offsets xml_i="45658" xml_f="45659" txt_i="21664" txt_f="21665">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="45750" xml_f="45751" txt_i="21665" txt_f="21666">
</offsets><disp-formula id="E12"><label><offsets xml_i="45781" xml_f="45785" txt_i="21666" txt_f="21670">(12)</offsets></label><mml:math id="M21"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="45919" xml_f="45920" txt_i="21670" txt_f="21671">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="45958" xml_f="45959" txt_i="21671" txt_f="21672">g</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="45998" xml_f="45999" txt_i="21672" txt_f="21673">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="46036" xml_f="46037" txt_i="21673" txt_f="21674">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="46075" xml_f="46076" txt_i="21674" txt_f="21675">g</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="46135" xml_f="46136" txt_i="21675" txt_f="21676">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="46174" xml_f="46175" txt_i="21676" txt_f="21677">g</offsets></mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="46237" xml_f="46238" txt_i="21677" txt_f="21678">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="46276" xml_f="46277" txt_i="21678" txt_f="21679">g</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="46315" xml_f="46316" txt_i="21679" txt_f="21680">T</offsets></mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="46409" xml_f="46410" txt_i="21680" txt_f="21681">
</offsets><disp-formula id="E13"><label><offsets xml_i="46440" xml_f="46444" txt_i="21681" txt_f="21685">(13)</offsets></label><mml:math id="M22"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="46578" xml_f="46579" txt_i="21685" txt_f="21686">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="46617" xml_f="46618" txt_i="21686" txt_f="21687">g</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="46657" xml_f="46658" txt_i="21687" txt_f="21688">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="46695" xml_f="46696" txt_i="21688" txt_f="21689">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="46734" xml_f="46735" txt_i="21689" txt_f="21690">g</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="46794" xml_f="46795" txt_i="21690" txt_f="21691">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="46833" xml_f="46834" txt_i="21691" txt_f="21692">g</offsets></mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="46924" xml_f="46925" txt_i="21692" txt_f="21693">
</offsets><disp-formula id="E14"><label><offsets xml_i="46955" xml_f="46959" txt_i="21693" txt_f="21697">(14)</offsets></label><mml:math id="M23"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mrow><mml:mo><offsets xml_i="47083" xml_f="47084" txt_i="21697" txt_f="21698">[</offsets></mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="47282" xml_f="47283" txt_i="21698" txt_f="21699">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47321" xml_f="47322" txt_i="21699" txt_f="21700">g</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47360" xml_f="47361" txt_i="21700" txt_f="21701">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="47423" xml_f="47424" txt_i="21701" txt_f="21702">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47462" xml_f="47463" txt_i="21702" txt_f="21703">g</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47523" xml_f="47524" txt_i="21703" txt_f="21704">v</offsets></mml:mi><mml:mo><offsets xml_i="47541" xml_f="47542" txt_i="21704" txt_f="21705">-</offsets></mml:mo><mml:mn><offsets xml_i="47559" xml_f="47560" txt_i="21705" txt_f="21706">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="47663" xml_f="47664" txt_i="21706" txt_f="21707">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47702" xml_f="47703" txt_i="21707" txt_f="21708">g</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47741" xml_f="47742" txt_i="21708" txt_f="21709">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="47804" xml_f="47805" txt_i="21709" txt_f="21710">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47843" xml_f="47844" txt_i="21710" txt_f="21711">g</offsets></mml:mi><mml:mo><offsets xml_i="47861" xml_f="47862" txt_i="21711" txt_f="21712">+</offsets></mml:mo><mml:mn><offsets xml_i="47879" xml_f="47880" txt_i="21712" txt_f="21713">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="47940" xml_f="47941" txt_i="21713" txt_f="21714">v</offsets></mml:mi><mml:mo><offsets xml_i="47958" xml_f="47959" txt_i="21714" txt_f="21715">-</offsets></mml:mo><mml:mn><offsets xml_i="47976" xml_f="47977" txt_i="21715" txt_f="21716">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="48099" xml_f="48100" txt_i="21716" txt_f="21717">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48138" xml_f="48139" txt_i="21717" txt_f="21718">g</offsets></mml:mi><mml:mo><offsets xml_i="48156" xml_f="48157" txt_i="21718" txt_f="21719">+</offsets></mml:mo><mml:mn><offsets xml_i="48174" xml_f="48175" txt_i="21719" txt_f="21720">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48213" xml_f="48214" txt_i="21720" txt_f="21721">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="48276" xml_f="48277" txt_i="21721" txt_f="21722">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48315" xml_f="48316" txt_i="21722" txt_f="21723">g</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48376" xml_f="48377" txt_i="21723" txt_f="21724">v</offsets></mml:mi><mml:mo><offsets xml_i="48394" xml_f="48395" txt_i="21724" txt_f="21725">-</offsets></mml:mo><mml:mn><offsets xml_i="48412" xml_f="48413" txt_i="21725" txt_f="21726">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="48516" xml_f="48517" txt_i="21726" txt_f="21727">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48555" xml_f="48556" txt_i="21727" txt_f="21728">g</offsets></mml:mi><mml:mo><offsets xml_i="48573" xml_f="48574" txt_i="21728" txt_f="21729">+</offsets></mml:mo><mml:mn><offsets xml_i="48591" xml_f="48592" txt_i="21729" txt_f="21730">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48630" xml_f="48631" txt_i="21730" txt_f="21731">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="48693" xml_f="48694" txt_i="21731" txt_f="21732">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48732" xml_f="48733" txt_i="21732" txt_f="21733">g</offsets></mml:mi><mml:mo><offsets xml_i="48750" xml_f="48751" txt_i="21733" txt_f="21734">+</offsets></mml:mo><mml:mn><offsets xml_i="48768" xml_f="48769" txt_i="21734" txt_f="21735">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="48829" xml_f="48830" txt_i="21735" txt_f="21736">v</offsets></mml:mi><mml:mo><offsets xml_i="48847" xml_f="48848" txt_i="21736" txt_f="21737">-</offsets></mml:mo><mml:mn><offsets xml_i="48865" xml_f="48866" txt_i="21737" txt_f="21738">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo><offsets xml_i="48950" xml_f="48951" txt_i="21738" txt_f="21739">]</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="48979" xml_f="48980" txt_i="21739" txt_f="21740">=</offsets></mml:mo><mml:mi><offsets xml_i="48997" xml_f="48998" txt_i="21740" txt_f="21741">W</offsets></mml:mi><mml:mi><offsets xml_i="49015" xml_f="49016" txt_i="21741" txt_f="21742">Λ</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="49053" xml_f="49054" txt_i="21742" txt_f="21743">W</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="49092" xml_f="49093" txt_i="21743" txt_f="21744">T</offsets></mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="49183" xml_f="49184" txt_i="21744" txt_f="21745">
</offsets><disp-formula id="E15"><label><offsets xml_i="49214" xml_f="49218" txt_i="21745" txt_f="21749">(15)</offsets></label><mml:math id="M24"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="49352" xml_f="49353" txt_i="21749" txt_f="21750">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="49391" xml_f="49392" txt_i="21750" txt_f="21751">g</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="49431" xml_f="49432" txt_i="21751" txt_f="21752">=</offsets></mml:mo><mml:mrow><mml:mo><offsets xml_i="49459" xml_f="49460" txt_i="21752" txt_f="21753">[</offsets></mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi><offsets xml_i="49507" xml_f="49508" txt_i="21753" txt_f="21754">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="49546" xml_f="49547" txt_i="21754" txt_f="21755">g</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="49586" xml_f="49587" txt_i="21755" txt_f="21756">,</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="49624" xml_f="49625" txt_i="21756" txt_f="21757">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="49663" xml_f="49664" txt_i="21757" txt_f="21758">g</offsets></mml:mi><mml:mo><offsets xml_i="49681" xml_f="49682" txt_i="21758" txt_f="21759">+</offsets></mml:mo><mml:mn><offsets xml_i="49699" xml_f="49700" txt_i="21759" txt_f="21760">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="49750" xml_f="49751" txt_i="21760" txt_f="21761">]</offsets></mml:mo></mml:mrow><mml:mi><offsets xml_i="49779" xml_f="49780" txt_i="21761" txt_f="21762">W</offsets></mml:mi><mml:mo><offsets xml_i="49797" xml_f="49798" txt_i="21762" txt_f="21763">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="49866" xml_f="49867" txt_i="21763" txt_f="21764">
</offsets></p><p><italic><offsets xml_i="49882" xml_f="49883" txt_i="21765" txt_f="21766">C</offsets></italic><sub><italic><offsets xml_i="49905" xml_f="49906" txt_i="21766" txt_f="21767">g</offsets></italic></sub><offsets xml_i="49921" xml_f="49926" txt_i="21767" txt_f="21772"> and </offsets><italic><offsets xml_i="49934" xml_f="49935" txt_i="21772" txt_f="21773">F</offsets></italic><sub><italic><offsets xml_i="49957" xml_f="49958" txt_i="21773" txt_f="21774">g</offsets></italic></sub><offsets xml_i="49973" xml_f="49982" txt_i="21774" txt_f="21783"> are the </offsets><italic><offsets xml_i="49990" xml_f="49992" txt_i="21783" txt_f="21785">gp</offsets></italic><offsets xml_i="50001" xml_f="50004" txt_i="21785" txt_f="21788"> × </offsets><italic><offsets xml_i="50012" xml_f="50014" txt_i="21788" txt_f="21790">gp</offsets></italic><offsets xml_i="50023" xml_f="50109" txt_i="21790" txt_f="21876"> covariance matrix in (stacked) time dimension and the eigenvectors of a group's data </offsets><italic><offsets xml_i="50117" xml_f="50118" txt_i="21876" txt_f="21877">Y</offsets></italic><sub><italic><offsets xml_i="50140" xml_f="50141" txt_i="21877" txt_f="21878">g</offsets></italic></sub><offsets xml_i="50156" xml_f="50181" txt_i="21878" txt_f="21903">, respectively. Assuming </offsets><italic><offsets xml_i="50189" xml_f="50191" txt_i="21903" txt_f="21905">gp</offsets></italic><offsets xml_i="50200" xml_f="50203" txt_i="21905" txt_f="21908"> ≥ </offsets><italic><offsets xml_i="50211" xml_f="50212" txt_i="21908" txt_f="21909">k</offsets></italic><offsets xml_i="50221" xml_f="50237" txt_i="21909" txt_f="21925">′, eigenvectors </offsets><italic><offsets xml_i="50245" xml_f="50246" txt_i="21925" txt_f="21926">F</offsets></italic><sub><italic><offsets xml_i="50268" xml_f="50269" txt_i="21926" txt_f="21927">g</offsets></italic></sub><offsets xml_i="50284" xml_f="50313" txt_i="21927" txt_f="21956"> are projected onto the data </offsets><italic><offsets xml_i="50321" xml_f="50322" txt_i="21956" txt_f="21957">Y</offsets></italic><sub><italic><offsets xml_i="50344" xml_f="50345" txt_i="21957" txt_f="21958">g</offsets></italic></sub><offsets xml_i="50360" xml_f="50373" txt_i="21958" txt_f="21971"> to obtain a </offsets><italic><offsets xml_i="50381" xml_f="50382" txt_i="21971" txt_f="21972">v</offsets></italic><offsets xml_i="50391" xml_f="50394" txt_i="21972" txt_f="21975"> × </offsets><italic><offsets xml_i="50402" xml_f="50403" txt_i="21975" txt_f="21976">k</offsets></italic><offsets xml_i="50412" xml_f="50436" txt_i="21976" txt_f="22000">′ subgroup PCA subspace </offsets><italic><offsets xml_i="50444" xml_f="50445" txt_i="22000" txt_f="22001">X</offsets></italic><sub><italic><offsets xml_i="50467" xml_f="50468" txt_i="22001" txt_f="22002">g</offsets></italic></sub><offsets xml_i="50483" xml_f="50585" txt_i="22002" txt_f="22104"> (Equation 13). Equations (11)–(13) are repeated for the next group to compute PCA estimates for data </offsets><italic><offsets xml_i="50593" xml_f="50594" txt_i="22104" txt_f="22105">Y</offsets></italic><sub><italic><offsets xml_i="50616" xml_f="50617" txt_i="22105" txt_f="22106">g</offsets></italic><offsets xml_i="50626" xml_f="50628" txt_i="22106" txt_f="22108">+1</offsets></sub><offsets xml_i="50634" xml_f="50659" txt_i="22108" txt_f="22133">. PCA estimates of group </offsets><italic><offsets xml_i="50667" xml_f="50668" txt_i="22133" txt_f="22134">Y</offsets></italic><sub><italic><offsets xml_i="50690" xml_f="50691" txt_i="22134" txt_f="22135">g</offsets></italic></sub><offsets xml_i="50706" xml_f="50717" txt_i="22135" txt_f="22146"> and group </offsets><italic><offsets xml_i="50725" xml_f="50726" txt_i="22146" txt_f="22147">Y</offsets></italic><sub><italic><offsets xml_i="50748" xml_f="50749" txt_i="22147" txt_f="22148">g</offsets></italic><offsets xml_i="50758" xml_f="50760" txt_i="22148" txt_f="22150">+1</offsets></sub><offsets xml_i="50766" xml_f="50812" txt_i="22150" txt_f="22196"> are stacked in time dimension and the common </offsets><italic><offsets xml_i="50820" xml_f="50821" txt_i="22196" txt_f="22197">v</offsets></italic><offsets xml_i="50830" xml_f="50833" txt_i="22197" txt_f="22200"> × </offsets><italic><offsets xml_i="50841" xml_f="50842" txt_i="22200" txt_f="22201">k</offsets></italic><offsets xml_i="50851" xml_f="50866" txt_i="22201" txt_f="22216">′ PCA subspace </offsets><italic><offsets xml_i="50874" xml_f="50875" txt_i="22216" txt_f="22217">X</offsets></italic><sub><italic><offsets xml_i="50897" xml_f="50898" txt_i="22217" txt_f="22218">g</offsets></italic></sub><offsets xml_i="50913" xml_f="50974" txt_i="22218" txt_f="22279"> is obtained using Equations (14) and (15), since typically 2</offsets><italic><offsets xml_i="50982" xml_f="50983" txt_i="22279" txt_f="22280">k</offsets></italic><offsets xml_i="50992" xml_f="50999" txt_i="22280" txt_f="22284">′ &lt; </offsets><italic><offsets xml_i="51007" xml_f="51008" txt_i="22284" txt_f="22285">v</offsets></italic><offsets xml_i="51017" xml_f="51106" txt_i="22285" txt_f="22374"> in this case. Equations (11)–(15) are repeated until the last group is loaded. Only the </offsets><italic><offsets xml_i="51114" xml_f="51115" txt_i="22374" txt_f="22375">k</offsets></italic><offsets xml_i="51124" xml_f="51184" txt_i="22375" txt_f="22435"> dominant PCA components are retained from the final matrix </offsets><italic><offsets xml_i="51192" xml_f="51193" txt_i="22435" txt_f="22436">X</offsets></italic><sub><italic><offsets xml_i="51215" xml_f="51216" txt_i="22436" txt_f="22437">g</offsets></italic></sub><offsets xml_i="51231" xml_f="51262" txt_i="22437" txt_f="22468"> at the end of the estimation: </offsets><inline-formula><mml:math id="M25"><mml:mi><offsets xml_i="51305" xml_f="51306" txt_i="22468" txt_f="22469">X</offsets></mml:mi><mml:mo><offsets xml_i="51323" xml_f="51324" txt_i="22469" txt_f="22470">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="51361" xml_f="51362" txt_i="22470" txt_f="22471">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="51400" xml_f="51401" txt_i="22471" txt_f="22472">g</offsets></mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi><offsets xml_i="51460" xml_f="51461" txt_i="22472" txt_f="22473">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mo><offsets xml_i="51499" xml_f="51500" txt_i="22473" txt_f="22474">-</offsets></mml:mo><mml:mn><offsets xml_i="51517" xml_f="51518" txt_i="22474" txt_f="22475">1</offsets></mml:mn><mml:mo><offsets xml_i="51535" xml_f="51536" txt_i="22475" txt_f="22476">∕</offsets></mml:mo><mml:mn><offsets xml_i="51553" xml_f="51554" txt_i="22476" txt_f="22477">2</offsets></mml:mn></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="51613" xml_f="51614" txt_i="22477" txt_f="22478">.</offsets></p><p><offsets xml_i="51621" xml_f="52017" txt_i="22479" txt_f="22875">STP requires only a single pass through the data to determine an approximate PCA space and is a very useful method when data loading is a bottleneck. Both the estimation accuracy and memory requirements are proportional to the number of subjects included in each group and number of components estimated in the intermediate group PCA. In this paper, we select number of subjects in each group as </offsets><italic><offsets xml_i="52025" xml_f="52026" txt_i="22875" txt_f="22876">g</offsets></italic><offsets xml_i="52035" xml_f="52279" txt_i="22876" txt_f="23120"> = 20, which has a small memory burden and yet gives a great approximation to the group PCA solution. Of note, STP not only generalizes the original three-step PCA approach but also includes MELODIC's incremental group PCA (MIGP; Smith et al., </offsets><xref rid="B33" ref-type="bibr"><offsets xml_i="52311" xml_f="52315" txt_i="23120" txt_f="23124">2014</offsets></xref><offsets xml_i="52322" xml_f="52347" txt_i="23124" txt_f="23149">) as a special case when </offsets><italic><offsets xml_i="52355" xml_f="52356" txt_i="23149" txt_f="23150">g</offsets></italic><offsets xml_i="52365" xml_f="52370" txt_i="23150" txt_f="23155"> = 1.</offsets></p></sec></sec><sec><title><offsets xml_i="52398" xml_f="52407" txt_i="23158" txt_f="23167">Large PCA</offsets></title><p><offsets xml_i="52418" xml_f="52443" txt_i="23168" txt_f="23193">Large PCA (Halko et al., </offsets><xref rid="B17" ref-type="bibr"><offsets xml_i="52475" xml_f="52480" txt_i="23193" txt_f="23198">2011a</offsets></xref><offsets xml_i="52487" xml_f="52570" txt_i="23198" txt_f="23281">) is a randomized version of the block Lanczos method (Kuczynski and Wozniakowski, </offsets><xref rid="B20" ref-type="bibr"><offsets xml_i="52602" xml_f="52606" txt_i="23281" txt_f="23285">1992</offsets></xref><offsets xml_i="52613" xml_f="52669" txt_i="23285" txt_f="23341">) that estimates a low rank PCA approximation of matrix </offsets><italic><offsets xml_i="52677" xml_f="52678" txt_i="23341" txt_f="23342">F</offsets></italic><offsets xml_i="52687" xml_f="52935" txt_i="23342" txt_f="23590"> [see Equation (2)]. In block Lanczos methods, intermediate subspace estimates from every previous iteration are retained, each forming an additional “block” for the next iteration. This is different from subspace iteration (discussed next), which </offsets><italic><offsets xml_i="52943" xml_f="52950" txt_i="23590" txt_f="23597">updates</offsets></italic><offsets xml_i="52959" xml_f="53101" txt_i="23597" txt_f="23739"> the PCA estimates instead, refining them until convergence is achieved. Similar to subspace iteration, Large PCA also exploits the powers of </offsets><italic><offsets xml_i="53109" xml_f="53111" txt_i="23739" txt_f="23741">YY</offsets></italic><sup><italic><offsets xml_i="53133" xml_f="53134" txt_i="23741" txt_f="23742">T</offsets></italic></sup><offsets xml_i="53149" xml_f="53182" txt_i="23742" txt_f="23775"> to obtain the reduced PCA space </offsets><italic><offsets xml_i="53190" xml_f="53191" txt_i="23775" txt_f="23776">X</offsets></italic><offsets xml_i="53200" xml_f="53246" txt_i="23776" txt_f="23822">. The Large PCA algorithm operates as follows:</offsets></p><p><offsets xml_i="53253" xml_f="53271" txt_i="23823" txt_f="23841">A Krylov subspace </offsets><italic><offsets xml_i="53279" xml_f="53281" txt_i="23841" txt_f="23843">Kr</offsets></italic><offsets xml_i="53290" xml_f="53314" txt_i="23843" txt_f="23867"> based on powers of the </offsets><italic><offsets xml_i="53322" xml_f="53324" txt_i="23867" txt_f="23869">YY</offsets></italic><sup><italic><offsets xml_i="53346" xml_f="53347" txt_i="23869" txt_f="23870">T</offsets></italic></sup><offsets xml_i="53362" xml_f="53443" txt_i="23870" txt_f="23951"> matrix is generated iteratively from an initial standard Gaussian random matrix </offsets><italic><offsets xml_i="53451" xml_f="53452" txt_i="23951" txt_f="23952">F</offsets></italic><sub><offsets xml_i="53466" xml_f="53467" txt_i="23952" txt_f="23953">0</offsets></sub><offsets xml_i="53473" xml_f="53482" txt_i="23953" txt_f="23962"> of size </offsets><italic><offsets xml_i="53490" xml_f="53492" txt_i="23962" txt_f="23964">Mp</offsets></italic><offsets xml_i="53501" xml_f="53504" txt_i="23964" txt_f="23967"> × </offsets><italic><offsets xml_i="53512" xml_f="53513" txt_i="23967" txt_f="23968">b</offsets></italic><offsets xml_i="53522" xml_f="53530" txt_i="23968" txt_f="23976">, where </offsets><italic><offsets xml_i="53538" xml_f="53539" txt_i="23976" txt_f="23977">b</offsets></italic><offsets xml_i="53548" xml_f="53602" txt_i="23977" txt_f="24031"> is the block length (typically, slightly larger than </offsets><italic><offsets xml_i="53610" xml_f="53611" txt_i="24031" txt_f="24032">k</offsets></italic><offsets xml_i="53620" xml_f="53623" txt_i="24032" txt_f="24035">):
</offsets><disp-formula id="E16"><label><offsets xml_i="53653" xml_f="53657" txt_i="24035" txt_f="24039">(16)</offsets></label><mml:math id="M26"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="53741" xml_f="53742" txt_i="24039" txt_f="24040">K</offsets></mml:mi><mml:mi><offsets xml_i="53759" xml_f="53760" txt_i="24040" txt_f="24041">r</offsets></mml:mi><mml:mo><offsets xml_i="53777" xml_f="53778" txt_i="24041" txt_f="24042">=</offsets></mml:mo><mml:mrow><mml:mo><offsets xml_i="53805" xml_f="53806" txt_i="24042" txt_f="24043">[</offsets></mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi><offsets xml_i="53853" xml_f="53854" txt_i="24043" txt_f="24044">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="53892" xml_f="53893" txt_i="24044" txt_f="24045">0</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="53932" xml_f="53933" txt_i="24045" txt_f="24046">,</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="53970" xml_f="53971" txt_i="24046" txt_f="24047">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="54009" xml_f="54010" txt_i="24047" txt_f="24048">1</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="54049" xml_f="54050" txt_i="24048" txt_f="24049">,</offsets></mml:mo><mml:mo><offsets xml_i="54067" xml_f="54068" txt_i="24049" txt_f="24050">…</offsets></mml:mo><mml:mo><offsets xml_i="54085" xml_f="54086" txt_i="24050" txt_f="24051">,</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="54123" xml_f="54124" txt_i="24051" txt_f="24052">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="54162" xml_f="54163" txt_i="24052" txt_f="24053">j</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="54213" xml_f="54214" txt_i="24053" txt_f="24054">]</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="54242" xml_f="54243" txt_i="24054" txt_f="24055">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="54311" xml_f="54318" txt_i="24055" txt_f="24062">
where </offsets><italic><offsets xml_i="54326" xml_f="54327" txt_i="24062" txt_f="24063">F</offsets></italic><sub><offsets xml_i="54341" xml_f="54342" txt_i="24063" txt_f="24064">0</offsets></sub><offsets xml_i="54348" xml_f="54351" txt_i="24064" txt_f="24067"> = </offsets><italic><offsets xml_i="54359" xml_f="54360" txt_i="24067" txt_f="24068">G</offsets></italic><sub><italic><offsets xml_i="54382" xml_f="54384" txt_i="24068" txt_f="24070">Mp</offsets></italic><offsets xml_i="54393" xml_f="54396" txt_i="24070" txt_f="24073"> × </offsets><italic><offsets xml_i="54404" xml_f="54405" txt_i="24073" txt_f="24074">b</offsets></italic></sub><offsets xml_i="54420" xml_f="54422" txt_i="24074" txt_f="24076">, </offsets><italic><offsets xml_i="54430" xml_f="54431" txt_i="24076" txt_f="24077">X</offsets></italic><sub><offsets xml_i="54445" xml_f="54446" txt_i="24077" txt_f="24078">0</offsets></sub><offsets xml_i="54452" xml_f="54455" txt_i="24078" txt_f="24081"> = </offsets><italic><offsets xml_i="54463" xml_f="54465" txt_i="24081" txt_f="24083">YF</offsets></italic><sub><offsets xml_i="54479" xml_f="54480" txt_i="24083" txt_f="24084">0</offsets></sub><offsets xml_i="54486" xml_f="54492" txt_i="24084" txt_f="24090">, and </offsets><inline-formula><mml:math id="M27"><mml:msub><mml:mrow><mml:mi><offsets xml_i="54555" xml_f="54556" txt_i="24090" txt_f="24091">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="54594" xml_f="54595" txt_i="24091" txt_f="24092">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="54634" xml_f="54635" txt_i="24092" txt_f="24093">=</offsets></mml:mo><mml:mi><offsets xml_i="54652" xml_f="54653" txt_i="24093" txt_f="24094">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo><offsets xml_i="54700" xml_f="54701" txt_i="24094" txt_f="24095">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="54751" xml_f="54752" txt_i="24095" txt_f="24096">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="54790" xml_f="54791" txt_i="24096" txt_f="24097">j</offsets></mml:mi><mml:mo><offsets xml_i="54808" xml_f="54809" txt_i="24097" txt_f="24098">-</offsets></mml:mo><mml:mn><offsets xml_i="54826" xml_f="54827" txt_i="24098" txt_f="24099">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="54865" xml_f="54866" txt_i="24099" txt_f="24100">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mi><offsets xml_i="54908" xml_f="54909" txt_i="24100" txt_f="24101">Y</offsets></mml:mi></mml:mrow><mml:mo><offsets xml_i="54937" xml_f="54938" txt_i="24101" txt_f="24102">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="54987" xml_f="54988" txt_i="24102" txt_f="24103">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="55027" xml_f="55028" txt_i="24103" txt_f="24104">=</offsets></mml:mo><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo><offsets xml_i="55106" xml_f="55107" txt_i="24104" txt_f="24105">∑</offsets></mml:mo></mml:mrow><mml:mrow><mml:mi><offsets xml_i="55145" xml_f="55146" txt_i="24105" txt_f="24106">i</offsets></mml:mi><mml:mo><offsets xml_i="55163" xml_f="55164" txt_i="24106" txt_f="24107">=</offsets></mml:mo><mml:mn><offsets xml_i="55181" xml_f="55182" txt_i="24107" txt_f="24108">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="55220" xml_f="55221" txt_i="24108" txt_f="24109">M</offsets></mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi><offsets xml_i="55286" xml_f="55287" txt_i="24109" txt_f="24110">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="55325" xml_f="55326" txt_i="24110" txt_f="24111">i</offsets></mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo><offsets xml_i="55395" xml_f="55396" txt_i="24111" txt_f="24112">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="55446" xml_f="55447" txt_i="24112" txt_f="24113">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="55485" xml_f="55486" txt_i="24113" txt_f="24114">j</offsets></mml:mi><mml:mo><offsets xml_i="55503" xml_f="55504" txt_i="24114" txt_f="24115">-</offsets></mml:mo><mml:mn><offsets xml_i="55521" xml_f="55522" txt_i="24115" txt_f="24116">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="55560" xml_f="55561" txt_i="24116" txt_f="24117">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="55623" xml_f="55624" txt_i="24117" txt_f="24118">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="55662" xml_f="55663" txt_i="24118" txt_f="24119">i</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="55713" xml_f="55714" txt_i="24119" txt_f="24120">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="55763" xml_f="55764" txt_i="24120" txt_f="24121">T</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="55823" xml_f="55824" txt_i="24121" txt_f="24122">.</offsets></p><p><italic><offsets xml_i="55839" xml_f="55841" txt_i="24123" txt_f="24125">Kr</offsets></italic><offsets xml_i="55850" xml_f="55862" txt_i="24125" txt_f="24137"> is of size </offsets><italic><offsets xml_i="55870" xml_f="55871" txt_i="24137" txt_f="24138">v</offsets></italic><offsets xml_i="55880" xml_f="55884" txt_i="24138" txt_f="24142"> × (</offsets><italic><offsets xml_i="55892" xml_f="55893" txt_i="24142" txt_f="24143">j</offsets></italic><offsets xml_i="55902" xml_f="55907" txt_i="24143" txt_f="24148"> + 1)</offsets><italic><offsets xml_i="55915" xml_f="55916" txt_i="24148" txt_f="24149">b</offsets></italic><offsets xml_i="55925" xml_f="55933" txt_i="24149" txt_f="24157">, where </offsets><italic><offsets xml_i="55941" xml_f="55942" txt_i="24157" txt_f="24158">v</offsets></italic><offsets xml_i="55951" xml_f="55981" txt_i="24158" txt_f="24188"> is the number of voxels, and </offsets><italic><offsets xml_i="55989" xml_f="55990" txt_i="24188" txt_f="24189">j</offsets></italic><offsets xml_i="55999" xml_f="56097" txt_i="24189" txt_f="24287"> ≥ 1 is the number of additional blocks required to obtain an accurate solution. The formation of </offsets><italic><offsets xml_i="56105" xml_f="56107" txt_i="24287" txt_f="24289">Kr</offsets></italic><offsets xml_i="56116" xml_f="56127" txt_i="24289" txt_f="24300"> requires (</offsets><italic><offsets xml_i="56135" xml_f="56136" txt_i="24300" txt_f="24301">j</offsets></italic><offsets xml_i="56145" xml_f="56150" txt_i="24301" txt_f="24306"> + 1)</offsets><italic><offsets xml_i="56158" xml_f="56159" txt_i="24306" txt_f="24307">M</offsets></italic><offsets xml_i="56168" xml_f="56241" txt_i="24307" txt_f="24380"> dataloads and only one subject's dataset in memory at a time (unstacked </offsets><italic><offsets xml_i="56249" xml_f="56250" txt_i="24380" txt_f="24381">Y</offsets></italic><offsets xml_i="56259" xml_f="56381" txt_i="24381" txt_f="24503">). Of course, if enough RAM is available in the system to retain all subject's datasets in memory simultaneously (stacked </offsets><italic><offsets xml_i="56389" xml_f="56390" txt_i="24503" txt_f="24504">Y</offsets></italic><offsets xml_i="56399" xml_f="56407" txt_i="24504" txt_f="24512">), then </offsets><italic><offsets xml_i="56415" xml_f="56416" txt_i="24512" txt_f="24513">M</offsets></italic><offsets xml_i="56425" xml_f="56461" txt_i="24513" txt_f="24549"> dataloads would suffice to compute </offsets><italic><offsets xml_i="56469" xml_f="56471" txt_i="24549" txt_f="24551">Kr</offsets></italic><offsets xml_i="56480" xml_f="56516" txt_i="24551" txt_f="24587"> and also Equations (17)–(21) below.</offsets></p><p><offsets xml_i="56523" xml_f="56529" txt_i="24588" txt_f="24594">After </offsets><italic><offsets xml_i="56537" xml_f="56539" txt_i="24594" txt_f="24596">Kr</offsets></italic><offsets xml_i="56548" xml_f="56650" txt_i="24596" txt_f="24698"> is formed, an economy-size QR decomposition is performed on it (the columns of χ are orthonormal and </offsets><italic><offsets xml_i="56658" xml_f="56659" txt_i="24698" txt_f="24699">R</offsets></italic><offsets xml_i="56668" xml_f="56713" txt_i="24699" txt_f="24744"> is an upper triangular real valued matrix):
</offsets><disp-formula id="E17"><label><offsets xml_i="56743" xml_f="56747" txt_i="24744" txt_f="24748">(17)</offsets></label><mml:math id="M28"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="56831" xml_f="56832" txt_i="24748" txt_f="24749">K</offsets></mml:mi><mml:mi><offsets xml_i="56849" xml_f="56850" txt_i="24749" txt_f="24750">r</offsets></mml:mi><mml:mo><offsets xml_i="56867" xml_f="56868" txt_i="24750" txt_f="24751">=</offsets></mml:mo><mml:mi><offsets xml_i="56885" xml_f="56886" txt_i="24751" txt_f="24752">χ</offsets></mml:mi><mml:mi><offsets xml_i="56903" xml_f="56904" txt_i="24752" txt_f="24753">R</offsets></mml:mi><mml:mo><offsets xml_i="56921" xml_f="56922" txt_i="24753" txt_f="24754">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><offsets xml_i="56997" xml_f="57056" txt_i="24755" txt_f="24814">Following, χ is projected onto the data matrix as follows:
</offsets><disp-formula id="E18"><label><offsets xml_i="57086" xml_f="57090" txt_i="24814" txt_f="24818">(18)</offsets></label><mml:math id="M29"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="57205" xml_f="57206" txt_i="24818" txt_f="24819">F</offsets></mml:mi><mml:mo><offsets xml_i="57223" xml_f="57224" txt_i="24819" txt_f="24820">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mi><offsets xml_i="57261" xml_f="57262" txt_i="24820" txt_f="24821">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="57300" xml_f="57301" txt_i="24821" txt_f="24822">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mi><offsets xml_i="57340" xml_f="57341" txt_i="24822" txt_f="24823">χ</offsets></mml:mi><mml:mo><offsets xml_i="57358" xml_f="57359" txt_i="24823" txt_f="24824">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="57427" xml_f="57470" txt_i="24824" txt_f="24867">
and compute an economy-size SVD on matrix </offsets><inline-formula><mml:math id="M30"><mml:mrow><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="57554" xml_f="57555" txt_i="24867" txt_f="24868">F</offsets></mml:mi></mml:mrow></mml:math></inline-formula><offsets xml_i="57603" xml_f="57605" txt_i="24868" txt_f="24870">:
</offsets><disp-formula id="E19"><label><offsets xml_i="57635" xml_f="57639" txt_i="24870" txt_f="24874">(19)</offsets></label><mml:math id="M31"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="57754" xml_f="57755" txt_i="24874" txt_f="24875">F</offsets></mml:mi><mml:mo><offsets xml_i="57772" xml_f="57773" txt_i="24875" txt_f="24876">=</offsets></mml:mo><mml:mi><offsets xml_i="57790" xml_f="57791" txt_i="24876" txt_f="24877">F</offsets></mml:mi><mml:mi><offsets xml_i="57808" xml_f="57809" txt_i="24877" txt_f="24878">S</offsets></mml:mi><mml:mi><offsets xml_i="57826" xml_f="57827" txt_i="24878" txt_f="24879">W</offsets></mml:mi><mml:mo><offsets xml_i="57844" xml_f="57845" txt_i="24879" txt_f="24880">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><offsets xml_i="57920" xml_f="57953" txt_i="24881" txt_f="24914">In order to obtain the PCA space </offsets><italic><offsets xml_i="57961" xml_f="57962" txt_i="24914" txt_f="24915">X</offsets></italic><offsets xml_i="57971" xml_f="58087" txt_i="24915" txt_f="25031">, the matrix product below is more efficient than Equation (3) because it does not require the additional dataload:
</offsets><disp-formula id="E20"><label><offsets xml_i="58117" xml_f="58121" txt_i="25031" txt_f="25035">(20)</offsets></label><mml:math id="M32"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="58205" xml_f="58206" txt_i="25035" txt_f="25036">X</offsets></mml:mi><mml:mo><offsets xml_i="58223" xml_f="58224" txt_i="25036" txt_f="25037">=</offsets></mml:mo><mml:mi><offsets xml_i="58241" xml_f="58242" txt_i="25037" txt_f="25038">χ</offsets></mml:mi><mml:mi><offsets xml_i="58259" xml_f="58260" txt_i="25038" txt_f="25039">W</offsets></mml:mi><mml:msqrt><mml:mrow><mml:mi><offsets xml_i="58298" xml_f="58299" txt_i="25039" txt_f="25040">v</offsets></mml:mi><mml:mo><offsets xml_i="58316" xml_f="58317" txt_i="25040" txt_f="25041">-</offsets></mml:mo><mml:mn><offsets xml_i="58334" xml_f="58335" txt_i="25041" txt_f="25042">1</offsets></mml:mn></mml:mrow></mml:msqrt><mml:mo><offsets xml_i="58375" xml_f="58376" txt_i="25042" txt_f="25043">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><offsets xml_i="58451" xml_f="58484" txt_i="25044" txt_f="25077">Finally, retrieve only the first </offsets><italic><offsets xml_i="58492" xml_f="58493" txt_i="25077" txt_f="25078">k</offsets></italic><offsets xml_i="58502" xml_f="58523" txt_i="25078" txt_f="25099"> dominant columns of </offsets><italic><offsets xml_i="58531" xml_f="58532" txt_i="25099" txt_f="25100">X</offsets></italic><offsets xml_i="58541" xml_f="58546" txt_i="25100" txt_f="25105"> and </offsets><italic><offsets xml_i="58554" xml_f="58555" txt_i="25105" txt_f="25106">F</offsets></italic><offsets xml_i="58564" xml_f="58584" txt_i="25106" txt_f="25126">, and use the first </offsets><italic><offsets xml_i="58592" xml_f="58593" txt_i="25126" txt_f="25127">k</offsets></italic><offsets xml_i="58602" xml_f="58612" txt_i="25127" txt_f="25137"> rows and </offsets><italic><offsets xml_i="58620" xml_f="58621" txt_i="25137" txt_f="25138">k</offsets></italic><offsets xml_i="58630" xml_f="58642" txt_i="25138" txt_f="25150"> columns of </offsets><italic><offsets xml_i="58650" xml_f="58651" txt_i="25150" txt_f="25151">S</offsets></italic><offsets xml_i="58660" xml_f="58695" txt_i="25151" txt_f="25186">. Note that Equation (18) requires </offsets><italic><offsets xml_i="58703" xml_f="58704" txt_i="25186" txt_f="25187">M</offsets></italic><offsets xml_i="58713" xml_f="58752" txt_i="25187" txt_f="25226"> additional dataloads, for a total of (</offsets><italic><offsets xml_i="58760" xml_f="58761" txt_i="25226" txt_f="25227">j</offsets></italic><offsets xml_i="58770" xml_f="58775" txt_i="25227" txt_f="25232"> + 2)</offsets><italic><offsets xml_i="58783" xml_f="58784" txt_i="25232" txt_f="25233">M</offsets></italic><offsets xml_i="58793" xml_f="58828" txt_i="25233" txt_f="25268"> dataloads in Large PCA with fixed </offsets><italic><offsets xml_i="58836" xml_f="58837" txt_i="25268" txt_f="25269">j</offsets></italic><offsets xml_i="58846" xml_f="58858" txt_i="25269" txt_f="25281"> (unstacked </offsets><italic><offsets xml_i="58866" xml_f="58867" txt_i="25281" txt_f="25282">Y</offsets></italic><offsets xml_i="58876" xml_f="58878" txt_i="25282" txt_f="25284">).</offsets></p><p><offsets xml_i="58885" xml_f="58899" txt_i="25285" txt_f="25299">The choice of </offsets><italic><offsets xml_i="58907" xml_f="58908" txt_i="25299" txt_f="25300">j</offsets></italic><offsets xml_i="58917" xml_f="58983" txt_i="25300" txt_f="25366"> is problem dependent: for fixed dataset and datatype, increasing </offsets><italic><offsets xml_i="58991" xml_f="58992" txt_i="25366" txt_f="25367">j</offsets></italic><offsets xml_i="59001" xml_f="59083" txt_i="25367" txt_f="25449"> dictates the attainable accuracy with respect to EVD; on the other hand, a fixed </offsets><italic><offsets xml_i="59091" xml_f="59092" txt_i="25449" txt_f="25450">j</offsets></italic><offsets xml_i="59101" xml_f="59233" txt_i="25450" txt_f="25582"> gives different accuracy for different datasets and datatypes. Thus, the recommendation in the original publication (Halko et al., </offsets><xref rid="B17" ref-type="bibr"><offsets xml_i="59265" xml_f="59270" txt_i="25582" txt_f="25587">2011a</offsets></xref><offsets xml_i="59277" xml_f="59290" txt_i="25587" txt_f="25600">) was to set </offsets><italic><offsets xml_i="59298" xml_f="59299" txt_i="25600" txt_f="25601">b</offsets></italic><offsets xml_i="59308" xml_f="59414" txt_i="25601" txt_f="25707"> to a large-enough value that would guarantee accuracy of the solution for a given data type, using fixed </offsets><italic><offsets xml_i="59422" xml_f="59423" txt_i="25707" txt_f="25708">j</offsets></italic><offsets xml_i="59432" xml_f="59556" txt_i="25708" txt_f="25832"> = 2. While this approach guarantees a small number of dataloads, it does so by increasing the memory burden, due to larger </offsets><italic><offsets xml_i="59564" xml_f="59565" txt_i="25832" txt_f="25833">b</offsets></italic><offsets xml_i="59574" xml_f="59664" txt_i="25833" txt_f="25923">. In our experiments using the recommended settings, we have noticed that the memory usage</offsets><xref ref-type="fn" rid="fn0006"><sup><offsets xml_i="59702" xml_f="59703" txt_i="25923" txt_f="25924">6</offsets></sup></xref><offsets xml_i="59716" xml_f="59993" txt_i="25924" txt_f="26201"> incurred was much undesirable for large fMRI datasets. Moreover, the attained accuracy with respect to EVD seemed inconsistent across different fMRI datasets, suggesting every new dataset would require specific adjustments for better accuracy. Although increasing the size of </offsets><italic><offsets xml_i="60001" xml_f="60003" txt_i="26201" txt_f="26203">Kr</offsets></italic><offsets xml_i="60012" xml_f="60025" txt_i="26203" txt_f="26216"> with larger </offsets><italic><offsets xml_i="60033" xml_f="60034" txt_i="26216" txt_f="26217">b</offsets></italic><offsets xml_i="60043" xml_f="60058" txt_i="26217" txt_f="26232"> and/or larger </offsets><italic><offsets xml_i="60066" xml_f="60067" txt_i="26232" txt_f="26233">j</offsets></italic><offsets xml_i="60076" xml_f="60309" txt_i="26233" txt_f="26466"> improves accuracy, without a direct check for convergence only blind adjustments are possible with the recommended approach. We then noticed that convergence could be assessed by computing the norm of the difference between the top </offsets><italic><offsets xml_i="60317" xml_f="60318" txt_i="26466" txt_f="26467">k</offsets></italic><offsets xml_i="60327" xml_f="60371" txt_i="26467" txt_f="26511"> singular values of sequentially increasing </offsets><italic><offsets xml_i="60379" xml_f="60381" txt_i="26511" txt_f="26513">Kr</offsets></italic><offsets xml_i="60390" xml_f="60558" txt_i="26513" txt_f="26681"> (Equation 16) and verifying that it meets some tolerance δ, as indicated in Equation (21). However, this implies that Equations (17)–(19) need to be computed for each </offsets><italic><offsets xml_i="60566" xml_f="60567" txt_i="26681" txt_f="26682">X</offsets></italic><sub><italic><offsets xml_i="60589" xml_f="60590" txt_i="26682" txt_f="26683">j</offsets></italic></sub><offsets xml_i="60605" xml_f="60619" txt_i="26683" txt_f="26697"> increment to </offsets><italic><offsets xml_i="60627" xml_f="60629" txt_i="26697" txt_f="26699">Kr</offsets></italic><offsets xml_i="60638" xml_f="60729" txt_i="26699" txt_f="26790">. Besides the additional computational burden, this increases the number of dataloads to (2</offsets><italic><offsets xml_i="60737" xml_f="60738" txt_i="26790" txt_f="26791">j</offsets></italic><offsets xml_i="60747" xml_f="60752" txt_i="26791" txt_f="26796"> + 1)</offsets><italic><offsets xml_i="60760" xml_f="60761" txt_i="26796" txt_f="26797">M</offsets></italic><offsets xml_i="60770" xml_f="60922" txt_i="26797" txt_f="26949">. Based on an analysis presented in the Appendix of Supplementary Material (see Section Parameter Selection for Large PCA) a good compromise was to fix </offsets><italic><offsets xml_i="60930" xml_f="60931" txt_i="26949" txt_f="26950">b</offsets></italic><offsets xml_i="60940" xml_f="60969" txt_i="26950" txt_f="26979"> = 170, generate the initial </offsets><italic><offsets xml_i="60977" xml_f="60979" txt_i="26979" txt_f="26981">Kr</offsets></italic><offsets xml_i="60988" xml_f="60994" txt_i="26981" txt_f="26987"> with </offsets><italic><offsets xml_i="61002" xml_f="61003" txt_i="26987" txt_f="26988">j</offsets></italic><sub><offsets xml_i="61017" xml_f="61018" txt_i="26988" txt_f="26989">0</offsets></sub><offsets xml_i="61024" xml_f="61119" txt_i="26989" txt_f="27084"> = 6 before the first estimation of the singular values (Equation 21), and continue to augment </offsets><italic><offsets xml_i="61127" xml_f="61129" txt_i="27084" txt_f="27086">Kr</offsets></italic><offsets xml_i="61138" xml_f="61176" txt_i="27086" txt_f="27124"> and estimate its singular values for </offsets><italic><offsets xml_i="61184" xml_f="61185" txt_i="27124" txt_f="27125">j</offsets></italic><offsets xml_i="61194" xml_f="61200" txt_i="27125" txt_f="27128"> &gt; </offsets><italic><offsets xml_i="61208" xml_f="61209" txt_i="27128" txt_f="27129">j</offsets></italic><sub><offsets xml_i="61223" xml_f="61224" txt_i="27129" txt_f="27130">0</offsets></sub><offsets xml_i="61230" xml_f="61294" txt_i="27130" txt_f="27194"> until convergence was attained. Our approach incurs a total of </offsets><italic><offsets xml_i="61302" xml_f="61303" txt_i="27194" txt_f="27195">j</offsets></italic><offsets xml_i="61312" xml_f="61313" txt_i="27195" txt_f="27196">2</offsets><italic><offsets xml_i="61321" xml_f="61322" txt_i="27196" txt_f="27197">M</offsets></italic><offsets xml_i="61331" xml_f="61334" txt_i="27197" txt_f="27200"> − </offsets><italic><offsets xml_i="61342" xml_f="61343" txt_i="27200" txt_f="27201">j</offsets></italic><sub><offsets xml_i="61357" xml_f="61358" txt_i="27201" txt_f="27202">0</offsets></sub><italic><offsets xml_i="61372" xml_f="61373" txt_i="27202" txt_f="27203">M</offsets></italic><offsets xml_i="61382" xml_f="61386" txt_i="27203" txt_f="27207"> + 2</offsets><italic><offsets xml_i="61394" xml_f="61395" txt_i="27207" txt_f="27208">M</offsets></italic><offsets xml_i="61404" xml_f="61419" txt_i="27208" txt_f="27223"> dataloads for </offsets><italic><offsets xml_i="61427" xml_f="61428" txt_i="27223" txt_f="27224">j</offsets></italic><offsets xml_i="61437" xml_f="61443" txt_i="27224" txt_f="27227"> &gt; </offsets><italic><offsets xml_i="61451" xml_f="61452" txt_i="27227" txt_f="27228">j</offsets></italic><sub><offsets xml_i="61466" xml_f="61467" txt_i="27228" txt_f="27229">0</offsets></sub><offsets xml_i="61473" xml_f="61688" txt_i="27229" txt_f="27444"> and some additional computational effort, but controlled memory usage, all while guaranteeing the accuracy of the solution with respect to EVD. Finally, Equation (20) is computed only after convergence is attained.</offsets></p><disp-formula id="E21"><label><offsets xml_i="61722" xml_f="61726" txt_i="27445" txt_f="27449">(21)</offsets></label><mml:math id="M33"><mml:mrow><mml:mrow><mml:mo><offsets xml_i="61781" xml_f="61782" txt_i="27449" txt_f="27450">‖</offsets></mml:mo><mml:mrow><mml:msub><mml:mi><offsets xml_i="61819" xml_f="61820" txt_i="27450" txt_f="27451">S</offsets></mml:mi><mml:mi><offsets xml_i="61837" xml_f="61838" txt_i="27451" txt_f="27452">j</offsets></mml:mi></mml:msub><mml:mo><offsets xml_i="61866" xml_f="61867" txt_i="27452" txt_f="27453">−</offsets></mml:mo><mml:msub><mml:mi><offsets xml_i="61894" xml_f="61895" txt_i="27453" txt_f="27454">S</offsets></mml:mi><mml:mrow><mml:mi><offsets xml_i="61922" xml_f="61923" txt_i="27454" txt_f="27455">j</offsets></mml:mi><mml:mtext><offsets xml_i="61943" xml_f="61944" txt_i="27455" txt_f="27456"> </offsets></mml:mtext><mml:mo><offsets xml_i="61964" xml_f="61965" txt_i="27456" txt_f="27457">−</offsets></mml:mo><mml:mtext><offsets xml_i="61985" xml_f="61986" txt_i="27457" txt_f="27458"> </offsets></mml:mtext><mml:mn><offsets xml_i="62006" xml_f="62007" txt_i="27458" txt_f="27459">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="62057" xml_f="62058" txt_i="27459" txt_f="27460">‖</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="62086" xml_f="62090" txt_i="27460" txt_f="27461">&lt;</offsets></mml:mo><mml:mtext><offsets xml_i="62110" xml_f="62111" txt_i="27461" txt_f="27462"> </offsets></mml:mtext><mml:mi><offsets xml_i="62131" xml_f="62132" txt_i="27462" txt_f="27463">δ</offsets></mml:mi><mml:mo><offsets xml_i="62149" xml_f="62150" txt_i="27463" txt_f="27464">.</offsets></mml:mo></mml:mrow></mml:math></disp-formula><p><offsets xml_i="62199" xml_f="62241" txt_i="27464" txt_f="27506">The time complexity of large PCA over all </offsets><italic><offsets xml_i="62249" xml_f="62250" txt_i="27506" txt_f="27507">j</offsets></italic><offsets xml_i="62259" xml_f="62265" txt_i="27507" txt_f="27510"> &gt; </offsets><italic><offsets xml_i="62273" xml_f="62274" txt_i="27510" txt_f="27511">j</offsets></italic><sub><offsets xml_i="62288" xml_f="62289" txt_i="27511" txt_f="27512">0</offsets></sub><offsets xml_i="62295" xml_f="62310" txt_i="27512" txt_f="27527"> iterations is </offsets><italic><offsets xml_i="62318" xml_f="62319" txt_i="27527" txt_f="27528">O</offsets></italic><offsets xml_i="62328" xml_f="62329" txt_i="27528" txt_f="27529">(</offsets><italic><offsets xml_i="62337" xml_f="62342" txt_i="27529" txt_f="27534">Mpvbj</offsets></italic><offsets xml_i="62351" xml_f="62630" txt_i="27534" txt_f="27813">). Since both Equations (16) and (18) can be implemented with a “for loop,” only one subject's dataset is required in memory at a time. Thus, Large PCA (un-stacked) would be suitable for large group-level PCA of fMRI data. Finally, when Large PCA is initialized with STP or SVP, </offsets><italic><offsets xml_i="62638" xml_f="62639" txt_i="27813" txt_f="27814">X</offsets></italic><sub><offsets xml_i="62653" xml_f="62654" txt_i="27814" txt_f="27815">0</offsets></sub><offsets xml_i="62660" xml_f="62677" txt_i="27815" txt_f="27832"> is set to equal </offsets><italic><offsets xml_i="62685" xml_f="62686" txt_i="27832" txt_f="27833">X</offsets></italic><sup><italic><offsets xml_i="62708" xml_f="62711" txt_i="27833" txt_f="27836">STP</offsets></italic></sup><offsets xml_i="62726" xml_f="62730" txt_i="27836" txt_f="27840"> or </offsets><italic><offsets xml_i="62738" xml_f="62739" txt_i="27840" txt_f="27841">X</offsets></italic><sup><italic><offsets xml_i="62761" xml_f="62764" txt_i="27841" txt_f="27844">SVP</offsets></italic></sup><offsets xml_i="62779" xml_f="62812" txt_i="27844" txt_f="27877">, respectively, and we recommend </offsets><italic><offsets xml_i="62820" xml_f="62821" txt_i="27877" txt_f="27878">j</offsets></italic><sub><offsets xml_i="62835" xml_f="62836" txt_i="27878" txt_f="27879">0</offsets></sub><offsets xml_i="62842" xml_f="62914" txt_i="27879" txt_f="27951"> = 1 since convergence is attained much faster with this initialization.</offsets></p></sec><sec><title><offsets xml_i="62936" xml_f="62966" txt_i="27953" txt_f="27983">Multi power iteration (MPOWIT)</offsets></title><p><offsets xml_i="62977" xml_f="63062" txt_i="27984" txt_f="28069">Power iteration is an iterative technique which uses powers of the covariance matrix </offsets><inline-formula><mml:math id="M34"><mml:msup><mml:mrow><mml:mi><offsets xml_i="63125" xml_f="63126" txt_i="28069" txt_f="28070">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="63164" xml_f="63165" txt_i="28070" txt_f="28071">v</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="63204" xml_f="63205" txt_i="28071" txt_f="28072">=</offsets></mml:mo><mml:mfrac><mml:mrow><mml:mn><offsets xml_i="63243" xml_f="63244" txt_i="28072" txt_f="28073">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="63282" xml_f="63283" txt_i="28073" txt_f="28074">v</offsets></mml:mi><mml:mo><offsets xml_i="63300" xml_f="63301" txt_i="28074" txt_f="28075">-</offsets></mml:mo><mml:mn><offsets xml_i="63318" xml_f="63319" txt_i="28075" txt_f="28076">1</offsets></mml:mn></mml:mrow></mml:mfrac><mml:mi><offsets xml_i="63359" xml_f="63360" txt_i="28076" txt_f="28077">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="63397" xml_f="63398" txt_i="28077" txt_f="28078">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="63436" xml_f="63437" txt_i="28078" txt_f="28079">T</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="63496" xml_f="63543" txt_i="28079" txt_f="28126"> to estimate one component (a single column of </offsets><italic><offsets xml_i="63551" xml_f="63552" txt_i="28126" txt_f="28127">X</offsets></italic><offsets xml_i="63561" xml_f="63740" txt_i="28127" txt_f="28306">) at a time, with subsequent components determined after removing the variance associated with previous components from the data (known as deflationary mode). While the powers of </offsets><italic><offsets xml_i="63748" xml_f="63749" txt_i="28306" txt_f="28307">C</offsets></italic><sup><italic><offsets xml_i="63771" xml_f="63772" txt_i="28307" txt_f="28308">v</offsets></italic></sup><italic><offsets xml_i="63795" xml_f="63796" txt_i="28308" txt_f="28309">X</offsets></italic><offsets xml_i="63805" xml_f="63810" txt_i="28309" txt_f="28314">) as </offsets><italic><offsets xml_i="63818" xml_f="63819" txt_i="28314" txt_f="28315">C</offsets></italic><sup><italic><offsets xml_i="63841" xml_f="63842" txt_i="28315" txt_f="28316">v</offsets></italic></sup><offsets xml_i="63857" xml_f="64233" txt_i="28316" txt_f="28692"> itself, the largest eigenvalues become more dominant, emphasizing the direction of largest variability. However, power iteration techniques require a normalization step to avoid ill-conditioned situations. Different normalization approaches and the choice of initial PCA subspace mark the key differences among power iteration techniques. In traditional power iteration, the </offsets><italic><offsets xml_i="64241" xml_f="64242" txt_i="28692" txt_f="28693">L</offsets></italic><sub><offsets xml_i="64256" xml_f="64257" txt_i="28693" txt_f="28694">2</offsets></sub><offsets xml_i="64263" xml_f="64351" txt_i="28694" txt_f="28782">-norm of the PCA estimates is used for normalization in each iteration, as shown below:
</offsets><disp-formula id="E22"><label><offsets xml_i="64381" xml_f="64385" txt_i="28782" txt_f="28786">(22)</offsets></label><mml:math id="M35"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="64519" xml_f="64520" txt_i="28786" txt_f="28787">x</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="64558" xml_f="64559" txt_i="28787" txt_f="28788">0</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="64598" xml_f="64599" txt_i="28788" txt_f="28789">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="64636" xml_f="64637" txt_i="28789" txt_f="28790">G</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="64675" xml_f="64676" txt_i="28790" txt_f="28791">v</offsets></mml:mi><mml:mo><offsets xml_i="64693" xml_f="64694" txt_i="28791" txt_f="28792">×</offsets></mml:mo><mml:mn><offsets xml_i="64711" xml_f="64712" txt_i="28792" txt_f="28793">1</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="64751" xml_f="64752" txt_i="28793" txt_f="28794">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="64820" xml_f="64821" txt_i="28794" txt_f="28795">
</offsets><disp-formula id="E23"><label><offsets xml_i="64851" xml_f="64855" txt_i="28795" txt_f="28799">(23)</offsets></label><mml:math id="M36"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="64989" xml_f="64990" txt_i="28799" txt_f="28800">x</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="65028" xml_f="65029" txt_i="28800" txt_f="28801">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="65068" xml_f="65069" txt_i="28801" txt_f="28802">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mi><offsets xml_i="65106" xml_f="65107" txt_i="28802" txt_f="28803">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="65145" xml_f="65146" txt_i="28803" txt_f="28804">v</offsets></mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi><offsets xml_i="65205" xml_f="65206" txt_i="28804" txt_f="28805">x</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="65244" xml_f="65245" txt_i="28805" txt_f="28806">j</offsets></mml:mi><mml:mo><offsets xml_i="65262" xml_f="65263" txt_i="28806" txt_f="28807">-</offsets></mml:mo><mml:mn><offsets xml_i="65280" xml_f="65281" txt_i="28807" txt_f="28808">1</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="65320" xml_f="65321" txt_i="28808" txt_f="28809">,</offsets></mml:mo><mml:mi><offsets xml_i="65338" xml_f="65339" txt_i="28809" txt_f="28810">j</offsets></mml:mi><mml:mo><offsets xml_i="65356" xml_f="65357" txt_i="28810" txt_f="28811">≥</offsets></mml:mo><mml:mn><offsets xml_i="65374" xml_f="65375" txt_i="28811" txt_f="28812">1</offsets></mml:mn><mml:mo><offsets xml_i="65392" xml_f="65393" txt_i="28812" txt_f="28813">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="65461" xml_f="65462" txt_i="28813" txt_f="28814">
</offsets><disp-formula id="E24"><label><offsets xml_i="65492" xml_f="65496" txt_i="28814" txt_f="28818">(24)</offsets></label><mml:math id="M37"><mml:mrow><mml:msub><mml:mi><offsets xml_i="65551" xml_f="65552" txt_i="28818" txt_f="28819">x</offsets></mml:mi><mml:mi><offsets xml_i="65569" xml_f="65570" txt_i="28819" txt_f="28820">j</offsets></mml:mi></mml:msub><mml:mo><offsets xml_i="65598" xml_f="65599" txt_i="28820" txt_f="28821">=</offsets></mml:mo><mml:msub><mml:mi><offsets xml_i="65626" xml_f="65627" txt_i="28821" txt_f="28822">x</offsets></mml:mi><mml:mi><offsets xml_i="65644" xml_f="65645" txt_i="28822" txt_f="28823">j</offsets></mml:mi></mml:msub><mml:mo><offsets xml_i="65673" xml_f="65674" txt_i="28823" txt_f="28824">/</offsets></mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo><offsets xml_i="65721" xml_f="65722" txt_i="28824" txt_f="28825">‖</offsets></mml:mo><mml:mrow><mml:msub><mml:mi><offsets xml_i="65759" xml_f="65760" txt_i="28825" txt_f="28826">x</offsets></mml:mi><mml:mi><offsets xml_i="65777" xml_f="65778" txt_i="28826" txt_f="28827">j</offsets></mml:mi></mml:msub></mml:mrow><mml:mo><offsets xml_i="65817" xml_f="65818" txt_i="28827" txt_f="28828">‖</offsets></mml:mo></mml:mrow></mml:mrow><mml:mn><offsets xml_i="65857" xml_f="65858" txt_i="28828" txt_f="28829">2</offsets></mml:mn></mml:msub><mml:mo><offsets xml_i="65886" xml_f="65887" txt_i="28829" txt_f="28830">.</offsets></mml:mo></mml:mrow></mml:math></disp-formula></p><p><offsets xml_i="65940" xml_f="65973" txt_i="28831" txt_f="28864">Subspace iteration (Rutishauser, </offsets><xref rid="B30" ref-type="bibr"><offsets xml_i="66005" xml_f="66009" txt_i="28864" txt_f="28868">1970</offsets></xref><offsets xml_i="66016" xml_f="66212" txt_i="28868" txt_f="29064">), also known as orthogonal iteration, extends power iteration to estimate multiple components simultaneously from the data (known as symmetric mode). It also uses powers of the covariance matrix </offsets><inline-formula><mml:math id="M38"><mml:msup><mml:mrow><mml:mi><offsets xml_i="66275" xml_f="66276" txt_i="29064" txt_f="29065">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="66314" xml_f="66315" txt_i="29065" txt_f="29066">v</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="66354" xml_f="66355" txt_i="29066" txt_f="29067">=</offsets></mml:mo><mml:mfrac><mml:mrow><mml:mn><offsets xml_i="66393" xml_f="66394" txt_i="29067" txt_f="29068">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="66432" xml_f="66433" txt_i="29068" txt_f="29069">v</offsets></mml:mi><mml:mo><offsets xml_i="66450" xml_f="66451" txt_i="29069" txt_f="29070">-</offsets></mml:mo><mml:mn><offsets xml_i="66468" xml_f="66469" txt_i="29070" txt_f="29071">1</offsets></mml:mn></mml:mrow></mml:mfrac><mml:mi><offsets xml_i="66509" xml_f="66510" txt_i="29071" txt_f="29072">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="66547" xml_f="66548" txt_i="29072" txt_f="29073">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="66586" xml_f="66587" txt_i="29073" txt_f="29074">T</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="66646" xml_f="66715" txt_i="29074" txt_f="29143">, iteratively estimating a subspace projection that contains the top </offsets><italic><offsets xml_i="66723" xml_f="66724" txt_i="29143" txt_f="29144">k</offsets></italic><offsets xml_i="66733" xml_f="66762" txt_i="29144" txt_f="29173"> components of the PCA space </offsets><italic><offsets xml_i="66770" xml_f="66771" txt_i="29173" txt_f="29174">X</offsets></italic><offsets xml_i="66780" xml_f="66829" txt_i="29174" txt_f="29223">. It typically uses QR factorization, instead of </offsets><italic><offsets xml_i="66837" xml_f="66838" txt_i="29223" txt_f="29224">L</offsets></italic><sub><offsets xml_i="66852" xml_f="66853" txt_i="29224" txt_f="29225">2</offsets></sub><offsets xml_i="66859" xml_f="66907" txt_i="29225" txt_f="29273">-norm, to orthonormalize intermediate estimates </offsets><italic><offsets xml_i="66915" xml_f="66916" txt_i="29273" txt_f="29274">X</offsets></italic><sub><italic><offsets xml_i="66938" xml_f="66939" txt_i="29274" txt_f="29275">j</offsets></italic></sub><offsets xml_i="66954" xml_f="67081" txt_i="29275" txt_f="29402"> at each iteration and prevent them from becoming ill-conditioned. The following equations summarize subspace iteration (Saad, </offsets><xref rid="B31" ref-type="bibr"><offsets xml_i="67113" xml_f="67117" txt_i="29402" txt_f="29406">2011</offsets></xref><offsets xml_i="67124" xml_f="67127" txt_i="29406" txt_f="29409">):
</offsets><disp-formula id="E25"><label><offsets xml_i="67157" xml_f="67161" txt_i="29409" txt_f="29413">(25)</offsets></label><mml:math id="M39"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="67295" xml_f="67296" txt_i="29413" txt_f="29414">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="67334" xml_f="67335" txt_i="29414" txt_f="29415">0</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="67374" xml_f="67375" txt_i="29415" txt_f="29416">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="67412" xml_f="67413" txt_i="29416" txt_f="29417">G</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="67451" xml_f="67452" txt_i="29417" txt_f="29418">v</offsets></mml:mi><mml:mo><offsets xml_i="67469" xml_f="67470" txt_i="29418" txt_f="29419">×</offsets></mml:mo><mml:mi><offsets xml_i="67487" xml_f="67488" txt_i="29419" txt_f="29420">k</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="67527" xml_f="67528" txt_i="29420" txt_f="29421">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="67596" xml_f="67597" txt_i="29421" txt_f="29422">
</offsets><disp-formula id="E26"><label><offsets xml_i="67627" xml_f="67631" txt_i="29422" txt_f="29426">(26)</offsets></label><mml:math id="M40"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="67765" xml_f="67766" txt_i="29426" txt_f="29427">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="67804" xml_f="67805" txt_i="29427" txt_f="29428">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="67844" xml_f="67845" txt_i="29428" txt_f="29429">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mi><offsets xml_i="67882" xml_f="67883" txt_i="29429" txt_f="29430">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="67921" xml_f="67922" txt_i="29430" txt_f="29431">v</offsets></mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi><offsets xml_i="67981" xml_f="67982" txt_i="29431" txt_f="29432">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="68020" xml_f="68021" txt_i="29432" txt_f="29433">j</offsets></mml:mi><mml:mo><offsets xml_i="68038" xml_f="68039" txt_i="29433" txt_f="29434">-</offsets></mml:mo><mml:mn><offsets xml_i="68056" xml_f="68057" txt_i="29434" txt_f="29435">1</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="68096" xml_f="68097" txt_i="29435" txt_f="29436">,</offsets></mml:mo><mml:mi><offsets xml_i="68114" xml_f="68115" txt_i="29436" txt_f="29437">j</offsets></mml:mi><mml:mo><offsets xml_i="68132" xml_f="68133" txt_i="29437" txt_f="29438">≥</offsets></mml:mo><mml:mn><offsets xml_i="68150" xml_f="68151" txt_i="29438" txt_f="29439">1</offsets></mml:mn><mml:mo><offsets xml_i="68168" xml_f="68169" txt_i="29439" txt_f="29440">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="68237" xml_f="68238" txt_i="29440" txt_f="29441">
</offsets><disp-formula id="E27"><label><offsets xml_i="68268" xml_f="68272" txt_i="29441" txt_f="29445">(27)</offsets></label><mml:math id="M41"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="68406" xml_f="68407" txt_i="29445" txt_f="29446">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="68445" xml_f="68446" txt_i="29446" txt_f="29447">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="68485" xml_f="68486" txt_i="29447" txt_f="29448">=</offsets></mml:mo><mml:mi><offsets xml_i="68503" xml_f="68504" txt_i="29448" txt_f="29449">o</offsets></mml:mi><mml:mi><offsets xml_i="68521" xml_f="68522" txt_i="29449" txt_f="29450">r</offsets></mml:mi><mml:mi><offsets xml_i="68539" xml_f="68540" txt_i="29450" txt_f="29451">t</offsets></mml:mi><mml:mi><offsets xml_i="68557" xml_f="68558" txt_i="29451" txt_f="29452">h</offsets></mml:mi><mml:mrow><mml:mo><offsets xml_i="68585" xml_f="68586" txt_i="29452" txt_f="29453">(</offsets></mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi><offsets xml_i="68633" xml_f="68634" txt_i="29453" txt_f="29454">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="68672" xml_f="68673" txt_i="29454" txt_f="29455">j</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="68723" xml_f="68724" txt_i="29455" txt_f="29456">)</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="68752" xml_f="68753" txt_i="29456" txt_f="29457">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="68821" xml_f="68828" txt_i="29457" txt_f="29464">
where </offsets><italic><offsets xml_i="68836" xml_f="68840" txt_i="29464" txt_f="29468">orth</offsets></italic><offsets xml_i="68849" xml_f="68970" txt_i="29468" txt_f="29589">(·) is an operation that returns an orthonormal basis, such as the QR factorization. The algorithm is initialized with a </offsets><italic><offsets xml_i="68978" xml_f="68979" txt_i="29589" txt_f="29590">v</offsets></italic><offsets xml_i="68988" xml_f="68991" txt_i="29590" txt_f="29593"> × </offsets><italic><offsets xml_i="68999" xml_f="69000" txt_i="29593" txt_f="29594">k</offsets></italic><offsets xml_i="69009" xml_f="69034" txt_i="29594" txt_f="29619"> Gaussian random matrix. </offsets><italic><offsets xml_i="69042" xml_f="69043" txt_i="29619" txt_f="29620">X</offsets></italic><sub><italic><offsets xml_i="69065" xml_f="69066" txt_i="29620" txt_f="29621">j</offsets></italic></sub><offsets xml_i="69081" xml_f="69114" txt_i="29621" txt_f="29654"> is the subspace estimate at the </offsets><italic><offsets xml_i="69122" xml_f="69123" txt_i="29654" txt_f="29655">j</offsets></italic><sup><italic><offsets xml_i="69145" xml_f="69147" txt_i="29655" txt_f="29657">th</offsets></italic></sup><offsets xml_i="69162" xml_f="69462" txt_i="29657" txt_f="29957"> iteration. Equations (26) and (27) are iterated until convergence. Subspace iteration is straightforward to implement but has slow convergence, especially for the last few eigenvalues, which converge much more slowly. Preconditioning techniques like shift-and-invert and Chebyshev polynomial (Saad, </offsets><xref rid="B31" ref-type="bibr"><offsets xml_i="69494" xml_f="69498" txt_i="29957" txt_f="29961">2011</offsets></xref><offsets xml_i="69505" xml_f="69753" txt_i="29961" txt_f="30209">) have been used on the covariance matrix to accelerate the subspace iteration method. Still, computing the covariance matrix is costly when the data are large. Hence, subspace iteration is not a popular method as compared to block Lanczos methods.</offsets></p><p><offsets xml_i="69760" xml_f="70104" txt_i="30210" txt_f="30554">Here, we introduce a novel method called MPOWIT, which accelerates the subspace iteration method. It relies on making the projecting subspace larger than the desired eigen space in order to overcome the slow convergence associated with the subspace iteration approach. The MPOWIT algorithm starts with a standard Gaussian random matrix of size </offsets><italic><offsets xml_i="70112" xml_f="70113" txt_i="30554" txt_f="30555">v</offsets></italic><offsets xml_i="70122" xml_f="70125" txt_i="30555" txt_f="30558"> × </offsets><italic><offsets xml_i="70133" xml_f="70135" txt_i="30558" txt_f="30560">lk</offsets></italic><offsets xml_i="70144" xml_f="70221" txt_i="30560" txt_f="30637">, following with an initial power iteration and the set of operations below:
</offsets><disp-formula id="E28"><label><offsets xml_i="70251" xml_f="70255" txt_i="30637" txt_f="30641">(28)</offsets></label><mml:math id="M42"><mml:mrow><mml:msub><mml:mi><offsets xml_i="70310" xml_f="70311" txt_i="30641" txt_f="30642">X</offsets></mml:mi><mml:mn><offsets xml_i="70328" xml_f="70329" txt_i="30642" txt_f="30643">0</offsets></mml:mn></mml:msub><mml:mo><offsets xml_i="70357" xml_f="70358" txt_i="30643" txt_f="30644">=</offsets></mml:mo><mml:msub><mml:mi><offsets xml_i="70385" xml_f="70386" txt_i="30644" txt_f="30645">G</offsets></mml:mi><mml:mrow><mml:mi><offsets xml_i="70413" xml_f="70414" txt_i="30645" txt_f="30646">v</offsets></mml:mi><mml:mo><offsets xml_i="70431" xml_f="70432" txt_i="30646" txt_f="30647">×</offsets></mml:mo><mml:mi><offsets xml_i="70449" xml_f="70450" txt_i="30647" txt_f="30648">l</offsets></mml:mi><mml:mi><offsets xml_i="70467" xml_f="70468" txt_i="30648" txt_f="30649">k</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="70507" xml_f="70508" txt_i="30649" txt_f="30650">,</offsets></mml:mo><mml:msub><mml:mi><offsets xml_i="70535" xml_f="70536" txt_i="30650" txt_f="30651">χ</offsets></mml:mi><mml:mn><offsets xml_i="70553" xml_f="70554" txt_i="30651" txt_f="30652">0</offsets></mml:mn></mml:msub><mml:mo><offsets xml_i="70582" xml_f="70583" txt_i="30652" txt_f="30653">=</offsets></mml:mo><mml:mrow><mml:mo><offsets xml_i="70610" xml_f="70611" txt_i="30653" txt_f="30654">(</offsets></mml:mo><mml:mrow><mml:mi><offsets xml_i="70638" xml_f="70639" txt_i="30654" txt_f="30655">Y</offsets></mml:mi><mml:msup><mml:mi><offsets xml_i="70666" xml_f="70667" txt_i="30655" txt_f="30656">Y</offsets></mml:mi><mml:mi><offsets xml_i="70684" xml_f="70685" txt_i="30656" txt_f="30657">T</offsets></mml:mi></mml:msup></mml:mrow><mml:mo><offsets xml_i="70724" xml_f="70725" txt_i="30657" txt_f="30658">)</offsets></mml:mo></mml:mrow><mml:msub><mml:mi><offsets xml_i="70763" xml_f="70764" txt_i="30658" txt_f="30659">X</offsets></mml:mi><mml:mn><offsets xml_i="70781" xml_f="70782" txt_i="30659" txt_f="30660">0</offsets></mml:mn></mml:msub><mml:mo><offsets xml_i="70810" xml_f="70811" txt_i="30660" txt_f="30661">,</offsets></mml:mo><mml:msub><mml:mi><offsets xml_i="70838" xml_f="70839" txt_i="30661" txt_f="30662">Λ</offsets></mml:mi><mml:mn><offsets xml_i="70856" xml_f="70857" txt_i="30662" txt_f="30663">0</offsets></mml:mn></mml:msub><mml:mo><offsets xml_i="70885" xml_f="70886" txt_i="30663" txt_f="30664">=</offsets></mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn><offsets xml_i="70952" xml_f="70953" txt_i="30664" txt_f="30665">0</offsets></mml:mn></mml:mstyle><mml:mo><offsets xml_i="70983" xml_f="70984" txt_i="30665" txt_f="30666">,</offsets></mml:mo></mml:mrow></mml:math></disp-formula><offsets xml_i="71030" xml_f="71031" txt_i="30666" txt_f="30667">
</offsets><disp-formula id="E29"><label><offsets xml_i="71061" xml_f="71065" txt_i="30667" txt_f="30671">(29)</offsets></label><mml:math id="M43"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="71199" xml_f="71200" txt_i="30671" txt_f="30672">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="71238" xml_f="71239" txt_i="30672" txt_f="30673">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="71278" xml_f="71279" txt_i="30673" txt_f="30674">=</offsets></mml:mo><mml:mi><offsets xml_i="71296" xml_f="71297" txt_i="30674" txt_f="30675">o</offsets></mml:mi><mml:mi><offsets xml_i="71314" xml_f="71315" txt_i="30675" txt_f="30676">r</offsets></mml:mi><mml:mi><offsets xml_i="71332" xml_f="71333" txt_i="30676" txt_f="30677">t</offsets></mml:mi><mml:mi><offsets xml_i="71350" xml_f="71351" txt_i="30677" txt_f="30678">h</offsets></mml:mi><mml:mrow><mml:mo><offsets xml_i="71378" xml_f="71379" txt_i="30678" txt_f="30679">(</offsets></mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi><offsets xml_i="71426" xml_f="71427" txt_i="30679" txt_f="30680">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="71465" xml_f="71466" txt_i="30680" txt_f="30681">j</offsets></mml:mi><mml:mo><offsets xml_i="71483" xml_f="71484" txt_i="30681" txt_f="30682">-</offsets></mml:mo><mml:mn><offsets xml_i="71501" xml_f="71502" txt_i="30682" txt_f="30683">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="71552" xml_f="71553" txt_i="30683" txt_f="30684">)</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="71581" xml_f="71582" txt_i="30684" txt_f="30685">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="71619" xml_f="71620" txt_i="30685" txt_f="30686">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="71658" xml_f="71659" txt_i="30686" txt_f="30687">j</offsets></mml:mi><mml:mo><offsets xml_i="71676" xml_f="71677" txt_i="30687" txt_f="30688">-</offsets></mml:mo><mml:mn><offsets xml_i="71694" xml_f="71695" txt_i="30688" txt_f="30689">1</offsets></mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="71765" xml_f="71766" txt_i="30689" txt_f="30690">F</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="71803" xml_f="71804" txt_i="30690" txt_f="30691">L</offsets></mml:mi></mml:mrow><mml:mrow><mml:mo><offsets xml_i="71842" xml_f="71843" txt_i="30691" txt_f="30692">-</offsets></mml:mo><mml:mn><offsets xml_i="71860" xml_f="71861" txt_i="30692" txt_f="30693">1</offsets></mml:mn></mml:mrow></mml:msup><mml:mo><offsets xml_i="71900" xml_f="71901" txt_i="30693" txt_f="30694">,</offsets></mml:mo><mml:mi><offsets xml_i="71918" xml_f="71919" txt_i="30694" txt_f="30695">j</offsets></mml:mi><mml:mo><offsets xml_i="71936" xml_f="71937" txt_i="30695" txt_f="30696">≥</offsets></mml:mo><mml:mn><offsets xml_i="71954" xml_f="71955" txt_i="30696" txt_f="30697">1</offsets></mml:mn><mml:mo><offsets xml_i="71972" xml_f="71973" txt_i="30697" txt_f="30698">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="72041" xml_f="72042" txt_i="30698" txt_f="30699">
</offsets><disp-formula id="E30"><label><offsets xml_i="72072" xml_f="72076" txt_i="30699" txt_f="30703">(30)</offsets></label><mml:math id="M44"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="72210" xml_f="72211" txt_i="30703" txt_f="30704">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="72249" xml_f="72250" txt_i="30704" txt_f="30705">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="72289" xml_f="72290" txt_i="30705" txt_f="30706">=</offsets></mml:mo><mml:mrow><mml:mo><offsets xml_i="72317" xml_f="72318" txt_i="30706" txt_f="30707">(</offsets></mml:mo><mml:mrow><mml:mi><offsets xml_i="72345" xml_f="72346" txt_i="30707" txt_f="30708">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="72383" xml_f="72384" txt_i="30708" txt_f="30709">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="72422" xml_f="72423" txt_i="30709" txt_f="30710">T</offsets></mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo><offsets xml_i="72473" xml_f="72474" txt_i="30710" txt_f="30711">)</offsets></mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi><offsets xml_i="72522" xml_f="72523" txt_i="30711" txt_f="30712">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="72561" xml_f="72562" txt_i="30712" txt_f="30713">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="72601" xml_f="72602" txt_i="30713" txt_f="30714">=</offsets></mml:mo><mml:mi><offsets xml_i="72619" xml_f="72620" txt_i="30714" txt_f="30715">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo><offsets xml_i="72667" xml_f="72668" txt_i="30715" txt_f="30716">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="72718" xml_f="72719" txt_i="30716" txt_f="30717">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="72757" xml_f="72758" txt_i="30717" txt_f="30718">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="72796" xml_f="72797" txt_i="30718" txt_f="30719">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mi><offsets xml_i="72839" xml_f="72840" txt_i="30719" txt_f="30720">Y</offsets></mml:mi></mml:mrow><mml:mo><offsets xml_i="72868" xml_f="72869" txt_i="30720" txt_f="30721">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="72918" xml_f="72919" txt_i="30721" txt_f="30722">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="72958" xml_f="72959" txt_i="30722" txt_f="30723">=</offsets></mml:mo><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo><offsets xml_i="73069" xml_f="73070" txt_i="30723" txt_f="30724">∑</offsets></mml:mo></mml:mrow><mml:mrow><mml:mi><offsets xml_i="73108" xml_f="73109" txt_i="30724" txt_f="30725">i</offsets></mml:mi><mml:mo><offsets xml_i="73126" xml_f="73127" txt_i="30725" txt_f="30726">=</offsets></mml:mo><mml:mn><offsets xml_i="73144" xml_f="73145" txt_i="30726" txt_f="30727">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="73183" xml_f="73184" txt_i="30727" txt_f="30728">M</offsets></mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi><offsets xml_i="73262" xml_f="73263" txt_i="30728" txt_f="30729">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="73301" xml_f="73302" txt_i="30729" txt_f="30730">i</offsets></mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo><offsets xml_i="73371" xml_f="73372" txt_i="30730" txt_f="30731">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="73422" xml_f="73423" txt_i="30731" txt_f="30732">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="73461" xml_f="73462" txt_i="30732" txt_f="30733">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="73500" xml_f="73501" txt_i="30733" txt_f="30734">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="73563" xml_f="73564" txt_i="30734" txt_f="30735">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="73602" xml_f="73603" txt_i="30735" txt_f="30736">i</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="73653" xml_f="73654" txt_i="30736" txt_f="30737">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="73703" xml_f="73704" txt_i="30737" txt_f="30738">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mo><offsets xml_i="73743" xml_f="73744" txt_i="30738" txt_f="30739">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="73812" xml_f="73813" txt_i="30739" txt_f="30740">
</offsets><disp-formula id="E31"><label><offsets xml_i="73843" xml_f="73847" txt_i="30740" txt_f="30744">(31)</offsets></label><mml:math id="M45"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="74005" xml_f="74006" txt_i="30744" txt_f="30745">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74044" xml_f="74045" txt_i="30745" txt_f="30746">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74083" xml_f="74084" txt_i="30746" txt_f="30747">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="74146" xml_f="74147" txt_i="30747" txt_f="30748">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74185" xml_f="74186" txt_i="30748" txt_f="30749">j</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74246" xml_f="74247" txt_i="30749" txt_f="30750">v</offsets></mml:mi><mml:mo><offsets xml_i="74264" xml_f="74265" txt_i="30750" txt_f="30751">-</offsets></mml:mo><mml:mn><offsets xml_i="74282" xml_f="74283" txt_i="30751" txt_f="30752">1</offsets></mml:mn></mml:mrow></mml:mfrac><mml:mo><offsets xml_i="74323" xml_f="74324" txt_i="30752" txt_f="30753">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="74361" xml_f="74362" txt_i="30753" txt_f="30754">W</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74400" xml_f="74401" txt_i="30754" txt_f="30755">j</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="74460" xml_f="74461" txt_i="30755" txt_f="30756">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74499" xml_f="74500" txt_i="30756" txt_f="30757">j</offsets></mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="74562" xml_f="74563" txt_i="30757" txt_f="30758">W</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74601" xml_f="74602" txt_i="30758" txt_f="30759">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="74640" xml_f="74641" txt_i="30759" txt_f="30760">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="74683" xml_f="74684" txt_i="30760" txt_f="30761">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="74752" xml_f="74759" txt_i="30761" txt_f="30768">
where </offsets><italic><offsets xml_i="74767" xml_f="74768" txt_i="30768" txt_f="30769">v</offsets></italic><offsets xml_i="74777" xml_f="74803" txt_i="30769" txt_f="30795"> is the number of voxels, </offsets><italic><offsets xml_i="74811" xml_f="74812" txt_i="30795" txt_f="30796">l</offsets></italic><offsets xml_i="74821" xml_f="74852" txt_i="30796" txt_f="30827"> is an integer multiplier, and </offsets><italic><offsets xml_i="74860" xml_f="74861" txt_i="30827" txt_f="30828">k</offsets></italic><offsets xml_i="74870" xml_f="75138" txt_i="30828" txt_f="31096"> is the number of desired eigenvectors. The main innovation in MPOWIT stems from the realization, through experience, that a small fraction of the top principal components converges much faster than the rest. Thus, a larger subspace leads to fast retrieval of the top </offsets><italic><offsets xml_i="75146" xml_f="75147" txt_i="31096" txt_f="31097">k</offsets></italic><offsets xml_i="75156" xml_f="75173" txt_i="31097" txt_f="31114"> components when </offsets><italic><offsets xml_i="75181" xml_f="75182" txt_i="31114" txt_f="31115">k</offsets></italic><offsets xml_i="75191" xml_f="75289" txt_i="31115" txt_f="31213"> is only a fraction of that subspace's dimensionality. We also propose a faster implementation of </offsets><italic><offsets xml_i="75297" xml_f="75301" txt_i="31213" txt_f="31217">orth</offsets></italic><offsets xml_i="75310" xml_f="75411" txt_i="31217" txt_f="31318">(·) for MPOWIT to return an orthonormal basis for the column space of its operand efficiently. Since </offsets><italic><offsets xml_i="75419" xml_f="75421" txt_i="31318" txt_f="31320">lk</offsets></italic><offsets xml_i="75430" xml_f="75559" txt_i="31320" txt_f="31449"> is typically small compared to the rank of the data, Equations (2) and (3) can be used as follows: first, perform a full EVD of </offsets><inline-formula><mml:math id="M46"><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="75625" xml_f="75626" txt_i="31449" txt_f="31450">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="75664" xml_f="75665" txt_i="31450" txt_f="31451">j</offsets></mml:mi><mml:mo><offsets xml_i="75682" xml_f="75683" txt_i="31451" txt_f="31452">-</offsets></mml:mo><mml:mn><offsets xml_i="75700" xml_f="75701" txt_i="31452" txt_f="31453">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="75739" xml_f="75740" txt_i="31453" txt_f="31454">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="75802" xml_f="75803" txt_i="31454" txt_f="31455">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="75841" xml_f="75842" txt_i="31455" txt_f="31456">j</offsets></mml:mi><mml:mo><offsets xml_i="75859" xml_f="75860" txt_i="31456" txt_f="31457">-</offsets></mml:mo><mml:mn><offsets xml_i="75877" xml_f="75878" txt_i="31457" txt_f="31458">1</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="75917" xml_f="75918" txt_i="31458" txt_f="31459">=</offsets></mml:mo><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="75966" xml_f="75967" txt_i="31459" txt_f="31460">F</offsets></mml:mi><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="76015" xml_f="76016" txt_i="31460" txt_f="31461">D</offsets></mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="76084" xml_f="76085" txt_i="31461" txt_f="31462">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="76123" xml_f="76124" txt_i="31462" txt_f="31463">T</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="76183" xml_f="76194" txt_i="31463" txt_f="31474"> to obtain </offsets><inline-formula><mml:math id="M47"><mml:mrow><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="76278" xml_f="76279" txt_i="31474" txt_f="31475">F</offsets></mml:mi></mml:mrow></mml:math></inline-formula><offsets xml_i="76327" xml_f="76341" txt_i="31475" txt_f="31489">, followed by </offsets><inline-formula><mml:math id="M48"><mml:msub><mml:mrow><mml:mi><offsets xml_i="76404" xml_f="76405" txt_i="31489" txt_f="31490">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="76443" xml_f="76444" txt_i="31490" txt_f="31491">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="76483" xml_f="76484" txt_i="31491" txt_f="31492">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="76521" xml_f="76522" txt_i="31492" txt_f="31493">χ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="76560" xml_f="76561" txt_i="31493" txt_f="31494">j</offsets></mml:mi><mml:mo><offsets xml_i="76578" xml_f="76579" txt_i="31494" txt_f="31495">-</offsets></mml:mo><mml:mn><offsets xml_i="76596" xml_f="76597" txt_i="31495" txt_f="31496">1</offsets></mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="76667" xml_f="76668" txt_i="31496" txt_f="31497">F</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="76705" xml_f="76706" txt_i="31497" txt_f="31498">L</offsets></mml:mi></mml:mrow><mml:mrow><mml:mo><offsets xml_i="76744" xml_f="76745" txt_i="31498" txt_f="31499">-</offsets></mml:mo><mml:mn><offsets xml_i="76762" xml_f="76763" txt_i="31499" txt_f="31500">1</offsets></mml:mn></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="76822" xml_f="76829" txt_i="31500" txt_f="31507"> where </offsets><italic><offsets xml_i="76837" xml_f="76838" txt_i="31507" txt_f="31508">L</offsets></italic><offsets xml_i="76847" xml_f="76884" txt_i="31508" txt_f="31545"> is a diagonal matrix containing the </offsets><italic><offsets xml_i="76892" xml_f="76893" txt_i="31545" txt_f="31546">L</offsets></italic><sub><offsets xml_i="76907" xml_f="76908" txt_i="31546" txt_f="31547">2</offsets></sub><offsets xml_i="76914" xml_f="76939" txt_i="31547" txt_f="31572">-norm of each column of χ</offsets><sub><italic><offsets xml_i="76952" xml_f="76953" txt_i="31572" txt_f="31573">j</offsets></italic><offsets xml_i="76962" xml_f="76964" txt_i="31573" txt_f="31575">−1</offsets></sub><inline-formula><mml:math id="M49"><mml:mrow><mml:mi mathvariant="-tex-caligraphic"><offsets xml_i="77054" xml_f="77055" txt_i="31575" txt_f="31576">F</offsets></mml:mi></mml:mrow></mml:math></inline-formula><offsets xml_i="77103" xml_f="77268" txt_i="31576" txt_f="31741">. This strategy is typically two or three times faster than economy-size QR factorization (based on the default MATLAB implementations) and not memory intensive for </offsets><italic><offsets xml_i="77276" xml_f="77278" txt_i="31741" txt_f="31743">lk</offsets></italic><offsets xml_i="77287" xml_f="77299" txt_i="31743" txt_f="31755"> ≤ 500 (for </offsets><italic><offsets xml_i="77307" xml_f="77308" txt_i="31755" txt_f="31756">k</offsets></italic><offsets xml_i="77317" xml_f="77332" txt_i="31756" txt_f="31771"> = 100, we set </offsets><italic><offsets xml_i="77340" xml_f="77341" txt_i="31771" txt_f="31772">l</offsets></italic><offsets xml_i="77350" xml_f="77441" txt_i="31772" txt_f="31863"> = 5 based on the analysis in Appendix Section How to Select the Projecting Subspace Size (</offsets><italic><offsets xml_i="77449" xml_f="77450" txt_i="31863" txt_f="31864">l</offsets></italic><offsets xml_i="77459" xml_f="77528" txt_i="31864" txt_f="31933">) for MPOWIT?; Supplementary Material). Furthermore, since computing </offsets><italic><offsets xml_i="77536" xml_f="77538" txt_i="31933" txt_f="31935">YY</offsets></italic><sup><italic><offsets xml_i="77560" xml_f="77561" txt_i="31935" txt_f="31936">T</offsets></italic></sup><offsets xml_i="77576" xml_f="77768" txt_i="31936" txt_f="32128"> on large data is inefficient in memory, the associative matrix multiplications shown in the center and right hand side of Equation (30) are used instead. Finally, Equation (31) is the EVD of </offsets><inline-formula><mml:math id="M50"><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="77834" xml_f="77835" txt_i="32128" txt_f="32129">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="77873" xml_f="77874" txt_i="32129" txt_f="32130">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="77912" xml_f="77913" txt_i="32130" txt_f="32131">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mi><offsets xml_i="77955" xml_f="77956" txt_i="32131" txt_f="32132">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo><offsets xml_i="78003" xml_f="78004" txt_i="32132" txt_f="32133">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="78054" xml_f="78055" txt_i="32133" txt_f="32134">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="78093" xml_f="78094" txt_i="32134" txt_f="32135">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="78132" xml_f="78133" txt_i="32135" txt_f="32136">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mi><offsets xml_i="78175" xml_f="78176" txt_i="32136" txt_f="32137">Y</offsets></mml:mi></mml:mrow><mml:mo><offsets xml_i="78204" xml_f="78205" txt_i="32137" txt_f="32138">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="78254" xml_f="78255" txt_i="32138" txt_f="32139">T</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="78314" xml_f="78356" txt_i="32139" txt_f="32181">, which is implemented using the function </offsets><italic><offsets xml_i="78364" xml_f="78368" txt_i="32181" txt_f="32185">eigs</offsets></italic><offsets xml_i="78377" xml_f="78380" txt_i="32185" txt_f="32188">(·)</offsets><xref ref-type="fn" rid="fn0007"><sup><offsets xml_i="78418" xml_f="78419" txt_i="32188" txt_f="32189">7</offsets></sup></xref><offsets xml_i="78432" xml_f="78480" txt_i="32189" txt_f="32237"> in MATLAB to efficiently retrieve only the top </offsets><italic><offsets xml_i="78488" xml_f="78489" txt_i="32237" txt_f="32238">k</offsets></italic><offsets xml_i="78498" xml_f="78512" txt_i="32238" txt_f="32252"> eigenvalues Λ</offsets><sub><italic><offsets xml_i="78525" xml_f="78526" txt_i="32252" txt_f="32253">j</offsets></italic></sub><offsets xml_i="78541" xml_f="78580" txt_i="32253" txt_f="32292">. Equations (29)–(31) are iterated for </offsets><italic><offsets xml_i="78588" xml_f="78589" txt_i="32292" txt_f="32293">j</offsets></italic><offsets xml_i="78598" xml_f="78617" txt_i="32293" txt_f="32312"> ≥ 1 until the top </offsets><italic><offsets xml_i="78625" xml_f="78626" txt_i="32312" txt_f="32313">k</offsets></italic><offsets xml_i="78635" xml_f="78767" txt_i="32313" txt_f="32445"> eigenvalues in the subspace projection converge to within the specified tolerance δ, as shown in Equation (32), and the choice of Λ</offsets><sub><offsets xml_i="78772" xml_f="78773" txt_i="32445" txt_f="32446">0</offsets></sub><offsets xml_i="78779" xml_f="78798" txt_i="32446" txt_f="32465"> = 0, where 0 is a </offsets><italic><offsets xml_i="78806" xml_f="78807" txt_i="32465" txt_f="32466">k</offsets></italic><offsets xml_i="78816" xml_f="78819" txt_i="32466" txt_f="32469"> × </offsets><italic><offsets xml_i="78827" xml_f="78828" txt_i="32469" txt_f="32470">k</offsets></italic><offsets xml_i="78837" xml_f="78901" txt_i="32470" txt_f="32534"> matrix of zeros, guarantees the algorithm will not stop before </offsets><italic><offsets xml_i="78909" xml_f="78910" txt_i="32534" txt_f="32535">j</offsets></italic><offsets xml_i="78919" xml_f="78925" txt_i="32535" txt_f="32541"> = 2:
</offsets><disp-formula id="E32"><label><offsets xml_i="78955" xml_f="78959" txt_i="32541" txt_f="32545">(32)</offsets></label><mml:math id="M51"><mml:mrow><mml:mrow><mml:mo><offsets xml_i="79014" xml_f="79015" txt_i="32545" txt_f="32546">‖</offsets></mml:mo><mml:mrow><mml:msub><mml:mi><offsets xml_i="79052" xml_f="79053" txt_i="32546" txt_f="32547">Λ</offsets></mml:mi><mml:mi><offsets xml_i="79070" xml_f="79071" txt_i="32547" txt_f="32548">j</offsets></mml:mi></mml:msub><mml:mo><offsets xml_i="79099" xml_f="79100" txt_i="32548" txt_f="32549">−</offsets></mml:mo><mml:msub><mml:mi><offsets xml_i="79127" xml_f="79128" txt_i="32549" txt_f="32550">Λ</offsets></mml:mi><mml:mrow><mml:mi><offsets xml_i="79155" xml_f="79156" txt_i="32550" txt_f="32551">j</offsets></mml:mi><mml:mtext><offsets xml_i="79176" xml_f="79177" txt_i="32551" txt_f="32552"> </offsets></mml:mtext><mml:mo><offsets xml_i="79197" xml_f="79198" txt_i="32552" txt_f="32553">−</offsets></mml:mo><mml:mtext><offsets xml_i="79218" xml_f="79219" txt_i="32553" txt_f="32554"> </offsets></mml:mtext><mml:mn><offsets xml_i="79239" xml_f="79240" txt_i="32554" txt_f="32555">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="79290" xml_f="79291" txt_i="32555" txt_f="32556">‖</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="79319" xml_f="79323" txt_i="32556" txt_f="32557">&lt;</offsets></mml:mo><mml:mi><offsets xml_i="79340" xml_f="79341" txt_i="32557" txt_f="32558">δ</offsets></mml:mi><mml:mo><offsets xml_i="79358" xml_f="79359" txt_i="32558" txt_f="32559">.</offsets></mml:mo></mml:mrow></mml:math></disp-formula></p><p><offsets xml_i="79412" xml_f="79453" txt_i="32560" txt_f="32601">After convergence, the reduced PCA space </offsets><italic><offsets xml_i="79461" xml_f="79462" txt_i="32601" txt_f="32602">X</offsets></italic><offsets xml_i="79471" xml_f="79488" txt_i="32602" txt_f="32619"> is obtained as:
</offsets><disp-formula id="E33"><label><offsets xml_i="79518" xml_f="79522" txt_i="32619" txt_f="32623">(33)</offsets></label><mml:math id="M52"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mi><offsets xml_i="79636" xml_f="79637" txt_i="32623" txt_f="32624">X</offsets></mml:mi><mml:mo><offsets xml_i="79654" xml_f="79655" txt_i="32624" txt_f="32625">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="79692" xml_f="79693" txt_i="32625" txt_f="32626">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="79731" xml_f="79732" txt_i="32626" txt_f="32627">j</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="79791" xml_f="79792" txt_i="32627" txt_f="32628">W</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="79830" xml_f="79831" txt_i="32628" txt_f="32629">j</offsets></mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="79893" xml_f="79894" txt_i="32629" txt_f="32630">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="79932" xml_f="79933" txt_i="32630" txt_f="32631">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mo><offsets xml_i="79971" xml_f="79972" txt_i="32631" txt_f="32632">-</offsets></mml:mo><mml:mn><offsets xml_i="79989" xml_f="79990" txt_i="32632" txt_f="32633">1</offsets></mml:mn><mml:mo><offsets xml_i="80007" xml_f="80008" txt_i="32633" txt_f="32634">∕</offsets></mml:mo><mml:mn><offsets xml_i="80025" xml_f="80026" txt_i="32634" txt_f="32635">2</offsets></mml:mn></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="80068" xml_f="80069" txt_i="32635" txt_f="32636">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="80137" xml_f="80159" txt_i="32636" txt_f="32658">
and the eigenvectors </offsets><italic><offsets xml_i="80167" xml_f="80168" txt_i="32658" txt_f="32659">F</offsets></italic><offsets xml_i="80177" xml_f="80190" txt_i="32659" txt_f="32672"> follow from </offsets><italic><offsets xml_i="80198" xml_f="80199" txt_i="32672" txt_f="32673">F</offsets></italic><offsets xml_i="80208" xml_f="80211" txt_i="32673" txt_f="32676"> = </offsets><italic><offsets xml_i="80219" xml_f="80220" txt_i="32676" txt_f="32677">Y</offsets></italic><sup><italic><offsets xml_i="80242" xml_f="80243" txt_i="32677" txt_f="32678">T</offsets></italic></sup><italic><offsets xml_i="80266" xml_f="80267" txt_i="32678" txt_f="32679">X</offsets></italic><offsets xml_i="80276" xml_f="80308" txt_i="32679" txt_f="32711">, then normalize the columns of </offsets><italic><offsets xml_i="80316" xml_f="80317" txt_i="32711" txt_f="32712">F</offsets></italic><offsets xml_i="80326" xml_f="80335" txt_i="32712" txt_f="32721"> to unit </offsets><italic><offsets xml_i="80343" xml_f="80344" txt_i="32721" txt_f="32722">L</offsets></italic><sub><offsets xml_i="80358" xml_f="80359" txt_i="32722" txt_f="32723">2</offsets></sub><offsets xml_i="80365" xml_f="80379" txt_i="32723" txt_f="32737">-norm. A rank </offsets><italic><offsets xml_i="80387" xml_f="80388" txt_i="32737" txt_f="32738">k</offsets></italic><offsets xml_i="80397" xml_f="80452" txt_i="32738" txt_f="32793"> PCA approximation is obtained by retrieving the first </offsets><italic><offsets xml_i="80460" xml_f="80461" txt_i="32793" txt_f="32794">k</offsets></italic><offsets xml_i="80470" xml_f="80504" txt_i="32794" txt_f="32828"> dominant columns of the matrices </offsets><italic><offsets xml_i="80512" xml_f="80513" txt_i="32828" txt_f="32829">X</offsets></italic><offsets xml_i="80522" xml_f="80527" txt_i="32829" txt_f="32834"> and </offsets><italic><offsets xml_i="80535" xml_f="80536" txt_i="32834" txt_f="32835">F</offsets></italic><offsets xml_i="80545" xml_f="80546" txt_i="32835" txt_f="32836">.</offsets></p><p><offsets xml_i="80553" xml_f="80592" txt_i="32837" txt_f="32876">The time complexity of MPOWIT over all </offsets><italic><offsets xml_i="80600" xml_f="80601" txt_i="32876" txt_f="32877">j</offsets></italic><offsets xml_i="80610" xml_f="80625" txt_i="32877" txt_f="32892"> iterations is </offsets><italic><offsets xml_i="80633" xml_f="80634" txt_i="32892" txt_f="32893">O</offsets></italic><offsets xml_i="80643" xml_f="80644" txt_i="32893" txt_f="32894">(</offsets><italic><offsets xml_i="80652" xml_f="80658" txt_i="32894" txt_f="32900">Mpvlkj</offsets></italic><offsets xml_i="80667" xml_f="80722" txt_i="32900" txt_f="32955">). Based on Equation (30), the algorithm requires only </offsets><italic><offsets xml_i="80730" xml_f="80731" txt_i="32955" txt_f="32956">M</offsets></italic><offsets xml_i="80740" xml_f="80781" txt_i="32956" txt_f="32997"> dataloads per iteration and a total of (</offsets><italic><offsets xml_i="80789" xml_f="80790" txt_i="32997" txt_f="32998">j</offsets></italic><offsets xml_i="80799" xml_f="80804" txt_i="32998" txt_f="33003"> + 1)</offsets><italic><offsets xml_i="80812" xml_f="80813" txt_i="33003" txt_f="33004">M</offsets></italic><offsets xml_i="80822" xml_f="80956" txt_i="33004" txt_f="33138"> dataloads, which makes MPOWIT scalable for large data analysis. Furthermore, based on the original power iteration algorithm, simple </offsets><italic><offsets xml_i="80964" xml_f="80965" txt_i="33138" txt_f="33139">L</offsets></italic><sub><offsets xml_i="80979" xml_f="80980" txt_i="33139" txt_f="33140">2</offsets></sub><offsets xml_i="80986" xml_f="81024" txt_i="33140" txt_f="33178">-norm normalization of the columns of </offsets><italic><offsets xml_i="81032" xml_f="81033" txt_i="33178" txt_f="33179">X</offsets></italic><sub><italic><offsets xml_i="81055" xml_f="81056" txt_i="33179" txt_f="33180">j</offsets></italic></sub><offsets xml_i="81071" xml_f="81303" txt_i="33180" txt_f="33412"> without full orthogonalization would suffice in Equation (29). However, in our experiments, we determined that assessments about the convergence of the algorithm are considerably more reliable if they are based on the eigenvalues Λ</offsets><sub><italic><offsets xml_i="81316" xml_f="81317" txt_i="33412" txt_f="33413">j</offsets></italic></sub><offsets xml_i="81332" xml_f="81358" txt_i="33413" txt_f="33439"> obtained from orthogonal </offsets><italic><offsets xml_i="81366" xml_f="81367" txt_i="33439" txt_f="33440">X</offsets></italic><sub><italic><offsets xml_i="81389" xml_f="81390" txt_i="33440" txt_f="33441">j</offsets></italic></sub><offsets xml_i="81405" xml_f="81452" txt_i="33441" txt_f="33488"> instead. Thus, explicit orthonormalization of </offsets><italic><offsets xml_i="81460" xml_f="81461" txt_i="33488" txt_f="33489">X</offsets></italic><sub><italic><offsets xml_i="81483" xml_f="81484" txt_i="33489" txt_f="33490">j</offsets></italic></sub><offsets xml_i="81499" xml_f="81531" txt_i="33490" txt_f="33522"> in each iteration is preferred.</offsets></p><p><offsets xml_i="81538" xml_f="81588" txt_i="33523" txt_f="33573">Also, when MPOWIT is initialized with STP or SVP, </offsets><italic><offsets xml_i="81596" xml_f="81597" txt_i="33573" txt_f="33574">X</offsets></italic><sub><offsets xml_i="81611" xml_f="81612" txt_i="33574" txt_f="33575">0</offsets></sub><offsets xml_i="81618" xml_f="81637" txt_i="33575" txt_f="33594"> is set as the top </offsets><italic><offsets xml_i="81645" xml_f="81647" txt_i="33594" txt_f="33596">lk</offsets></italic><offsets xml_i="81656" xml_f="81671" txt_i="33596" txt_f="33611"> components of </offsets><italic><offsets xml_i="81679" xml_f="81680" txt_i="33611" txt_f="33612">X</offsets></italic><sup><italic><offsets xml_i="81702" xml_f="81705" txt_i="33612" txt_f="33615">STP</offsets></italic></sup><offsets xml_i="81720" xml_f="81724" txt_i="33615" txt_f="33619"> or </offsets><italic><offsets xml_i="81732" xml_f="81733" txt_i="33619" txt_f="33620">X</offsets></italic><sup><italic><offsets xml_i="81755" xml_f="81758" txt_i="33620" txt_f="33623">SVP</offsets></italic></sup><offsets xml_i="81773" xml_f="81793" txt_i="33623" txt_f="33643">, respectively, and </offsets><inline-formula><mml:math id="M53"><mml:msub><mml:mrow><mml:mi><offsets xml_i="81856" xml_f="81857" txt_i="33643" txt_f="33644">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="81895" xml_f="81896" txt_i="33644" txt_f="33645">0</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="81935" xml_f="81936" txt_i="33645" txt_f="33646">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mi><offsets xml_i="81973" xml_f="81974" txt_i="33646" txt_f="33647">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="82012" xml_f="82013" txt_i="33647" txt_f="33648">S</offsets></mml:mi><mml:mi><offsets xml_i="82030" xml_f="82031" txt_i="33648" txt_f="33649">T</offsets></mml:mi><mml:mi><offsets xml_i="82048" xml_f="82049" txt_i="33649" txt_f="33650">P</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="82108" xml_f="82112" txt_i="33650" txt_f="33654"> or </offsets><inline-formula><mml:math id="M54"><mml:msub><mml:mrow><mml:mi><offsets xml_i="82175" xml_f="82176" txt_i="33654" txt_f="33655">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="82214" xml_f="82215" txt_i="33655" txt_f="33656">0</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="82254" xml_f="82255" txt_i="33656" txt_f="33657">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mi><offsets xml_i="82292" xml_f="82293" txt_i="33657" txt_f="33658">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="82331" xml_f="82332" txt_i="33658" txt_f="33659">S</offsets></mml:mi><mml:mi><offsets xml_i="82349" xml_f="82350" txt_i="33659" txt_f="33660">V</offsets></mml:mi><mml:mi><offsets xml_i="82367" xml_f="82368" txt_i="33660" txt_f="33661">P</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="82427" xml_f="82764" txt_i="33661" txt_f="33998"> since all eigenvalues are available at the end of the STP and SVP procedures. Initializing with a sub-sampling technique accelerates convergence and, more importantly, prevents dataloads from becoming a bottleneck in the analysis pipeline. Lastly, note that MPOWIT differs from a block version of subspace iteration (Mitliagkas et al., </offsets><xref rid="B27" ref-type="bibr"><offsets xml_i="82796" xml_f="82800" txt_i="33998" txt_f="34002">2013</offsets></xref><offsets xml_i="82807" xml_f="82997" txt_i="34002" txt_f="34192">) where regular subspace iteration is applied in “online” or “streaming memory” mode, i.e., making a single pass over the data. Although, this approach minimizes the number of dataloads (to </offsets><italic><offsets xml_i="83005" xml_f="83006" txt_i="34192" txt_f="34193">M</offsets></italic><offsets xml_i="83015" xml_f="83142" txt_i="34193" txt_f="34320">), the PCA solution is only approximate with respect to the EVD solution and, thus, not recommended for group ICA of fMRI data.</offsets></p></sec><sec><title><offsets xml_i="83164" xml_f="83212" txt_i="34322" txt_f="34370">MPOWIT and expectation maximization PCA (EM PCA)</offsets></title><p><offsets xml_i="83223" xml_f="83468" txt_i="34371" txt_f="34616">To complete our discussion on methods for PCA of large datasets, we present expectation maximization PCA (EM PCA). Our focus is on the connections and the implications that certain MPOWIT concepts have on this popular technique. EM PCA (Roweis, </offsets><xref rid="B29" ref-type="bibr"><offsets xml_i="83500" xml_f="83504" txt_i="34616" txt_f="34620">1997</offsets></xref><offsets xml_i="83511" xml_f="83619" txt_i="34620" txt_f="34728">) uses expectation and maximization steps to determine the PCA subspace. The algorithm operates as follows:
</offsets><disp-formula id="E34"><label><offsets xml_i="83649" xml_f="83653" txt_i="34728" txt_f="34732">(34)</offsets></label><mml:math id="M55"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="83787" xml_f="83788" txt_i="34732" txt_f="34733">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mn><offsets xml_i="83826" xml_f="83827" txt_i="34733" txt_f="34734">0</offsets></mml:mn></mml:mrow></mml:msub><mml:mo><offsets xml_i="83866" xml_f="83867" txt_i="34734" txt_f="34735">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="83904" xml_f="83905" txt_i="34735" txt_f="34736">G</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="83943" xml_f="83944" txt_i="34736" txt_f="34737">v</offsets></mml:mi><mml:mo><offsets xml_i="83961" xml_f="83962" txt_i="34737" txt_f="34738">×</offsets></mml:mo><mml:mi><offsets xml_i="83979" xml_f="83980" txt_i="34738" txt_f="34739">k</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="84019" xml_f="84020" txt_i="34739" txt_f="34740">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="84088" xml_f="84089" txt_i="34740" txt_f="34741">
</offsets><disp-formula id="E35"><label><offsets xml_i="84119" xml_f="84123" txt_i="34741" txt_f="34745">(35)</offsets></label><mml:math id="M56"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="84260" xml_f="84261" txt_i="34745" txt_f="34746">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="84299" xml_f="84300" txt_i="34746" txt_f="34747">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="84338" xml_f="84339" txt_i="34747" txt_f="34748">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mo><offsets xml_i="84381" xml_f="84382" txt_i="34748" txt_f="34749">=</offsets></mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo><offsets xml_i="84429" xml_f="84430" txt_i="34749" txt_f="34750">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="84480" xml_f="84481" txt_i="34750" txt_f="34751">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="84519" xml_f="84520" txt_i="34751" txt_f="34752">j</offsets></mml:mi><mml:mo><offsets xml_i="84537" xml_f="84538" txt_i="34752" txt_f="34753">-</offsets></mml:mo><mml:mn><offsets xml_i="84555" xml_f="84556" txt_i="34753" txt_f="34754">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="84594" xml_f="84595" txt_i="34754" txt_f="34755">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="84657" xml_f="84658" txt_i="34755" txt_f="34756">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="84696" xml_f="84697" txt_i="34756" txt_f="34757">j</offsets></mml:mi><mml:mo><offsets xml_i="84714" xml_f="84715" txt_i="34757" txt_f="34758">-</offsets></mml:mo><mml:mn><offsets xml_i="84732" xml_f="84733" txt_i="34758" txt_f="34759">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="84783" xml_f="84784" txt_i="34759" txt_f="34760">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo><offsets xml_i="84833" xml_f="84834" txt_i="34760" txt_f="34761">-</offsets></mml:mo><mml:mn><offsets xml_i="84851" xml_f="84852" txt_i="34761" txt_f="34762">1</offsets></mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="84914" xml_f="84915" txt_i="34762" txt_f="34763">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="84953" xml_f="84954" txt_i="34763" txt_f="34764">j</offsets></mml:mi><mml:mo><offsets xml_i="84971" xml_f="84972" txt_i="34764" txt_f="34765">-</offsets></mml:mo><mml:mn><offsets xml_i="84989" xml_f="84990" txt_i="34765" txt_f="34766">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="85028" xml_f="85029" txt_i="34766" txt_f="34767">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:mi><offsets xml_i="85071" xml_f="85072" txt_i="34767" txt_f="34768">Y</offsets></mml:mi><mml:mo><offsets xml_i="85089" xml_f="85090" txt_i="34768" txt_f="34769">,</offsets></mml:mo><mml:mi><offsets xml_i="85107" xml_f="85108" txt_i="34769" txt_f="34770">j</offsets></mml:mi><mml:mo><offsets xml_i="85125" xml_f="85126" txt_i="34770" txt_f="34771">≥</offsets></mml:mo><mml:mn><offsets xml_i="85143" xml_f="85144" txt_i="34771" txt_f="34772">1</offsets></mml:mn><mml:mo><offsets xml_i="85161" xml_f="85162" txt_i="34772" txt_f="34773">,</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><offsets xml_i="85230" xml_f="85231" txt_i="34773" txt_f="34774">
</offsets><disp-formula id="E36"><label><offsets xml_i="85261" xml_f="85265" txt_i="34774" txt_f="34778">(36)</offsets></label><mml:math id="M57"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi><offsets xml_i="85399" xml_f="85400" txt_i="34778" txt_f="34779">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="85438" xml_f="85439" txt_i="34779" txt_f="34780">j</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="85478" xml_f="85479" txt_i="34780" txt_f="34781">=</offsets></mml:mo><mml:mi><offsets xml_i="85496" xml_f="85497" txt_i="34781" txt_f="34782">Y</offsets></mml:mi><mml:msub><mml:mrow><mml:mi><offsets xml_i="85534" xml_f="85535" txt_i="34782" txt_f="34783">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="85573" xml_f="85574" txt_i="34783" txt_f="34784">j</offsets></mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo><offsets xml_i="85643" xml_f="85644" txt_i="34784" txt_f="34785">(</offsets></mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="85694" xml_f="85695" txt_i="34785" txt_f="34786">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="85733" xml_f="85734" txt_i="34786" txt_f="34787">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="85772" xml_f="85773" txt_i="34787" txt_f="34788">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="85835" xml_f="85836" txt_i="34788" txt_f="34789">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="85874" xml_f="85875" txt_i="34789" txt_f="34790">j</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="85925" xml_f="85926" txt_i="34790" txt_f="34791">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo><offsets xml_i="85975" xml_f="85976" txt_i="34791" txt_f="34792">-</offsets></mml:mo><mml:mn><offsets xml_i="85993" xml_f="85994" txt_i="34792" txt_f="34793">1</offsets></mml:mn></mml:mrow></mml:msup><mml:mo><offsets xml_i="86033" xml_f="86034" txt_i="34793" txt_f="34794">.</offsets></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><offsets xml_i="86109" xml_f="86166" txt_i="34795" txt_f="34852">In Equation (34), a Gaussian random matrix of dimensions </offsets><italic><offsets xml_i="86174" xml_f="86175" txt_i="34852" txt_f="34853">v</offsets></italic><offsets xml_i="86184" xml_f="86187" txt_i="34853" txt_f="34856"> × </offsets><italic><offsets xml_i="86195" xml_f="86196" txt_i="34856" txt_f="34857">k</offsets></italic><offsets xml_i="86205" xml_f="86303" txt_i="34857" txt_f="34955"> is selected as the initial PCA subspace. In the expectation step (Equation 35), the PCA subspace </offsets><italic><offsets xml_i="86311" xml_f="86312" txt_i="34955" txt_f="34956">X</offsets></italic><sub><italic><offsets xml_i="86334" xml_f="86335" txt_i="34956" txt_f="34957">j</offsets></italic><offsets xml_i="86344" xml_f="86346" txt_i="34957" txt_f="34959">−1</offsets></sub><offsets xml_i="86352" xml_f="86392" txt_i="34959" txt_f="34999"> is fixed and the transformation matrix </offsets><italic><offsets xml_i="86400" xml_f="86401" txt_i="34999" txt_f="35000">F</offsets></italic><sub><italic><offsets xml_i="86423" xml_f="86424" txt_i="35000" txt_f="35001">j</offsets></italic></sub><offsets xml_i="86439" xml_f="86501" txt_i="35001" txt_f="35063"> is determined, while in the maximization step (Equation 36), </offsets><italic><offsets xml_i="86509" xml_f="86510" txt_i="35063" txt_f="35064">F</offsets></italic><sub><italic><offsets xml_i="86532" xml_f="86533" txt_i="35064" txt_f="35065">j</offsets></italic></sub><offsets xml_i="86548" xml_f="86575" txt_i="35065" txt_f="35092"> is fixed and the subspace </offsets><italic><offsets xml_i="86583" xml_f="86584" txt_i="35092" txt_f="35093">X</offsets></italic><sub><italic><offsets xml_i="86606" xml_f="86607" txt_i="35093" txt_f="35094">j</offsets></italic></sub><offsets xml_i="86622" xml_f="86765" txt_i="35094" txt_f="35237"> is determined. Equations (35) and (36) are iterated until the algorithm converges to within the specified error for tolerance as shown below:
</offsets><disp-formula id="E37"><label><offsets xml_i="86795" xml_f="86799" txt_i="35237" txt_f="35241">(37)</offsets></label><mml:math id="M58"><mml:mrow><mml:mrow><mml:mo><offsets xml_i="86854" xml_f="86855" txt_i="35241" txt_f="35242">‖</offsets></mml:mo><mml:mrow><mml:msub><mml:mi><offsets xml_i="86892" xml_f="86893" txt_i="35242" txt_f="35243">X</offsets></mml:mi><mml:mi><offsets xml_i="86910" xml_f="86911" txt_i="35243" txt_f="35244">j</offsets></mml:mi></mml:msub><mml:mo><offsets xml_i="86939" xml_f="86940" txt_i="35244" txt_f="35245">−</offsets></mml:mo><mml:msub><mml:mi><offsets xml_i="86967" xml_f="86968" txt_i="35245" txt_f="35246">X</offsets></mml:mi><mml:mrow><mml:mi><offsets xml_i="86995" xml_f="86996" txt_i="35246" txt_f="35247">j</offsets></mml:mi><mml:mtext><offsets xml_i="87016" xml_f="87017" txt_i="35247" txt_f="35248"> </offsets></mml:mtext><mml:mo><offsets xml_i="87037" xml_f="87038" txt_i="35248" txt_f="35249">−</offsets></mml:mo><mml:mtext><offsets xml_i="87058" xml_f="87059" txt_i="35249" txt_f="35250"> </offsets></mml:mtext><mml:mn><offsets xml_i="87079" xml_f="87080" txt_i="35250" txt_f="35251">1</offsets></mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="87130" xml_f="87131" txt_i="35251" txt_f="35252">‖</offsets></mml:mo></mml:mrow><mml:mo><offsets xml_i="87159" xml_f="87163" txt_i="35252" txt_f="35253">&lt;</offsets></mml:mo><mml:mi><offsets xml_i="87180" xml_f="87181" txt_i="35253" txt_f="35254">δ</offsets></mml:mi><mml:mo><offsets xml_i="87198" xml_f="87199" txt_i="35254" txt_f="35255">.</offsets></mml:mo></mml:mrow></mml:math></disp-formula></p><p><offsets xml_i="87252" xml_f="87293" txt_i="35256" txt_f="35297">After convergence, the reduced PCA space </offsets><italic><offsets xml_i="87301" xml_f="87302" txt_i="35297" txt_f="35298">X</offsets></italic><offsets xml_i="87311" xml_f="87386" txt_i="35298" txt_f="35373"> is determined using Equations (31) and (33): first, perform a full EVD of </offsets><inline-formula><mml:math id="M59"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="87473" xml_f="87474" txt_i="35373" txt_f="35374">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="87512" xml_f="87513" txt_i="35374" txt_f="35375">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="87551" xml_f="87552" txt_i="35375" txt_f="35376">T</offsets></mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi><offsets xml_i="87614" xml_f="87615" txt_i="35376" txt_f="35377">Y</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="87652" xml_f="87653" txt_i="35377" txt_f="35378">Y</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="87691" xml_f="87692" txt_i="35378" txt_f="35379">T</offsets></mml:mi></mml:mrow></mml:msup><mml:mi><offsets xml_i="87731" xml_f="87732" txt_i="35379" txt_f="35380">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="87770" xml_f="87771" txt_i="35380" txt_f="35381">j</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi><offsets xml_i="87831" xml_f="87832" txt_i="35381" txt_f="35382">v</offsets></mml:mi><mml:mo><offsets xml_i="87849" xml_f="87850" txt_i="35382" txt_f="35383">-</offsets></mml:mo><mml:mn><offsets xml_i="87867" xml_f="87868" txt_i="35383" txt_f="35384">1</offsets></mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula><offsets xml_i="87928" xml_f="87942" txt_i="35384" txt_f="35398">, followed by </offsets><inline-formula><mml:math id="M60"><mml:mi><offsets xml_i="87985" xml_f="87986" txt_i="35398" txt_f="35399">X</offsets></mml:mi><mml:mo><offsets xml_i="88003" xml_f="88004" txt_i="35399" txt_f="35400">=</offsets></mml:mo><mml:msub><mml:mrow><mml:mi><offsets xml_i="88041" xml_f="88042" txt_i="35400" txt_f="35401">X</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="88080" xml_f="88081" txt_i="35401" txt_f="35402">j</offsets></mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi><offsets xml_i="88140" xml_f="88141" txt_i="35402" txt_f="35403">W</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="88179" xml_f="88180" txt_i="35403" txt_f="35404">j</offsets></mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi><offsets xml_i="88242" xml_f="88243" txt_i="35404" txt_f="35405">Λ</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="88281" xml_f="88282" txt_i="35405" txt_f="35406">j</offsets></mml:mi></mml:mrow><mml:mrow><mml:mo><offsets xml_i="88320" xml_f="88321" txt_i="35406" txt_f="35407">-</offsets></mml:mo><mml:mn><offsets xml_i="88338" xml_f="88339" txt_i="35407" txt_f="35408">1</offsets></mml:mn><mml:mo><offsets xml_i="88356" xml_f="88357" txt_i="35408" txt_f="35409">∕</offsets></mml:mo><mml:mn><offsets xml_i="88374" xml_f="88375" txt_i="35409" txt_f="35410">2</offsets></mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula><offsets xml_i="88437" xml_f="88438" txt_i="35410" txt_f="35411">.</offsets></p><p><offsets xml_i="88445" xml_f="88503" txt_i="35412" txt_f="35470">The time complexity of EM PCA over all iterations is only </offsets><italic><offsets xml_i="88511" xml_f="88512" txt_i="35470" txt_f="35471">O</offsets></italic><offsets xml_i="88521" xml_f="88522" txt_i="35471" txt_f="35472">(</offsets><italic><offsets xml_i="88530" xml_f="88535" txt_i="35472" txt_f="35477">Mpvkj</offsets></italic><offsets xml_i="88544" xml_f="88844" txt_i="35477" txt_f="35777">) but it takes a considerably larger number of iterations to converge when compared to Large PCA and MPOWIT methods. This is because EM PCA has the same convergence properties of subspace iteration. In fact, as we prove in Appendix Section Proof: MPOWIT and EM-PCA Converge to the Same PCA Subspace (</offsets><italic><offsets xml_i="88852" xml_f="88853" txt_i="35777" txt_f="35778">X</offsets></italic><offsets xml_i="88862" xml_f="89193" txt_i="35778" txt_f="36109">) (Supplementary Material), EM PCA returns the same subspace estimate as MPOWIT if both run for the same number of iterations and use the same initial guess. Naturally, the acceleration schemes used for subspace iteration [see Section Multi Power Iteration (MPOWIT) and Appendix Section How to Select the Projecting Subspace Size (</offsets><italic><offsets xml_i="89201" xml_f="89202" txt_i="36109" txt_f="36110">l</offsets></italic><offsets xml_i="89211" xml_f="89388" txt_i="36110" txt_f="36287">) for MPOWIT?; Supplementary Material] are equally applicable and useful for EM PCA. However, as seen from Equations (35) and (36), EM PCA requires loading the data into memory </offsets><italic><offsets xml_i="89396" xml_f="89401" txt_i="36287" txt_f="36292">twice</offsets></italic><offsets xml_i="89410" xml_f="89601" txt_i="36292" txt_f="36483"> if un-stacked PCA is performed. Since dataloading is a bottleneck for very large group analyses, EM PCA is still slower than MPOWIT when PCA is carried out on un-stacked data (i.e., on each </offsets><italic><offsets xml_i="89609" xml_f="89610" txt_i="36483" txt_f="36484">Y</offsets></italic><sub><italic><offsets xml_i="89632" xml_f="89633" txt_i="36484" txt_f="36485">i</offsets></italic></sub><offsets xml_i="89648" xml_f="89690" txt_i="36485" txt_f="36527"> at a time rather than the entire stacked </offsets><italic><offsets xml_i="89698" xml_f="89699" txt_i="36527" txt_f="36528">Y</offsets></italic><offsets xml_i="89708" xml_f="89794" txt_i="36528" txt_f="36614"> at once). Therefore, we forgo further comparisons with EM PCA in the Results Section.</offsets></p><p><offsets xml_i="89801" xml_f="89907" txt_i="36615" txt_f="36721">As a final remark on methods, our MPOWIT method relates to normalized power iteration (Martinsson et al., </offsets><xref rid="B26" ref-type="bibr"><offsets xml_i="89939" xml_f="89943" txt_i="36721" txt_f="36725">2010</offsets></xref><offsets xml_i="89950" xml_f="89966" txt_i="36725" txt_f="36741">; Halko et al., </offsets><xref rid="B18" ref-type="bibr"><offsets xml_i="89998" xml_f="90003" txt_i="36741" txt_f="36746">2011b</offsets></xref><offsets xml_i="90010" xml_f="90766" txt_i="36746" txt_f="37502">). However, normalized power iteration is more a variation of the EM PCA method where both the expectation (Equation 35) and maximization (Equation 36) steps are orthonormalized. Hence, the subspaces in each iteration are the same for both EM PCA and normalized power iteration. Also, we note that orthonormalization of the expectation step (Equation 35) in normalized power iteration is redundant and, therefore, normalized power iteration has the same shortcomings as EM PCA in large un-stacked group analyses, i.e., both require two dataloads per iteration. Thus, the time required to solve group PCA is significantly longer than with MPOWIT when a large number of subjects are included [see definition of “large” in Section Sub-Sampled Time PCA (STP)].</offsets></p></sec></sec><sec><title><offsets xml_i="90794" xml_f="90816" txt_i="37505" txt_f="37527">Data and preprocessing</offsets></title><p><offsets xml_i="90827" xml_f="90942" txt_i="37528" txt_f="37643">We use 1600 pre-processed subjects from resting state fMRI data (a superset of the data presented in Allen et al., </offsets><xref rid="B1" ref-type="bibr"><offsets xml_i="90973" xml_f="90977" txt_i="37643" txt_f="37647">2011</offsets></xref><offsets xml_i="90984" xml_f="91088" txt_i="37647" txt_f="37751">) and perform group PCA. Pre-processing steps include image realignment using INRIalign (Freire et al., </offsets><xref rid="B14" ref-type="bibr"><offsets xml_i="91120" xml_f="91124" txt_i="37751" txt_f="37755">2002</offsets></xref><offsets xml_i="91131" xml_f="91234" txt_i="37755" txt_f="37858">), slice-timing correction using the middle slice as reference, spatial normalization (Friston et al., </offsets><xref rid="B15" ref-type="bibr"><offsets xml_i="91266" xml_f="91270" txt_i="37858" txt_f="37862">1995</offsets></xref><offsets xml_i="91277" xml_f="91960" txt_i="37862" txt_f="38545">) and 3D Gaussian smoothing with a kernel size of 10 × 10 × 10 mm. No normalization is done on the BOLD fMRI timeseries (e.g., dividing by the variance or mean as is sometimes done for ICA approaches). Scans from 3 to 150 are included in the analysis to match the same time-points across subjects. A common mask is applied on all subjects to include only in-brain voxels. The common mask for all the subjects is generated by using element-wise multiplication on the individual subject masks. A widely used approach to generate individual subject masks is to eliminate non-brain voxels by keeping voxels with values above or equal to the mean over an entire volume for each timepoint.</offsets></p><p><offsets xml_i="91967" xml_f="92035" txt_i="38546" txt_f="38614">In the initial subject-level PCA step of group ICA (Calhoun et al., </offsets><xref rid="B8" ref-type="bibr"><offsets xml_i="92066" xml_f="92070" txt_i="38614" txt_f="38618">2001</offsets></xref><offsets xml_i="92077" xml_f="92130" txt_i="38618" txt_f="38671">), each individual subject's fMRI data of dimensions </offsets><italic><offsets xml_i="92138" xml_f="92139" txt_i="38671" txt_f="38672">v</offsets></italic><offsets xml_i="92148" xml_f="92151" txt_i="38672" txt_f="38675"> × </offsets><italic><offsets xml_i="92159" xml_f="92160" txt_i="38675" txt_f="38676">t</offsets></italic><offsets xml_i="92169" xml_f="92234" txt_i="38676" txt_f="38741"> is reduced to a few whitened principal components of dimensions </offsets><italic><offsets xml_i="92242" xml_f="92243" txt_i="38741" txt_f="38742">v</offsets></italic><offsets xml_i="92252" xml_f="92255" txt_i="38742" txt_f="38745"> × </offsets><italic><offsets xml_i="92263" xml_f="92264" txt_i="38745" txt_f="38746">p</offsets></italic><offsets xml_i="92273" xml_f="92286" txt_i="38746" txt_f="38759"> (see Figure </offsets><xref ref-type="fig" rid="F1"><offsets xml_i="92316" xml_f="92317" txt_i="38759" txt_f="38760">1</offsets></xref><offsets xml_i="92324" xml_f="92386" txt_i="38760" txt_f="38822">). We use EVD to reduce subject specific fMRI data and retain </offsets><italic><offsets xml_i="92394" xml_f="92395" txt_i="38822" txt_f="38823">p</offsets></italic><offsets xml_i="92404" xml_f="92520" txt_i="38823" txt_f="38939"> = 100 components, capturing near-maximal individual subject variability during the first PCA step (Erhardt et al., </offsets><xref rid="B11" ref-type="bibr"><offsets xml_i="92552" xml_f="92556" txt_i="38939" txt_f="38943">2011</offsets></xref><offsets xml_i="92563" xml_f="93094" txt_i="38943" txt_f="39474">). In the second PCA step (group-level PCA), data from each subject in the first PCA step is stacked along the (reduced) time dimension. As the number of subjects increases, the memory requirement increases since the data is temporally concatenated. On all reported experiments, without loss of generality, we used a typical fMRI acquisition setting where the whole scanning field is sampled at 3 × 3 × 3 mm voxel resolution (resulting in a matrix of dimensions 53 × 63 × 46) and the number of time points is 148 with TR = 2000 ms.</offsets></p></sec><sec><title><offsets xml_i="93116" xml_f="93127" txt_i="39476" txt_f="39487">Experiments</offsets></title><p><offsets xml_i="93138" xml_f="93340" txt_i="39488" txt_f="39690">A number of experiments have been conducted to assess memory usage and computation time for all group PCA methods discussed previously. Firstly, we assessed memory usage for varying number of subjects (</offsets><italic><offsets xml_i="93348" xml_f="93349" txt_i="39690" txt_f="39691">M</offsets></italic><offsets xml_i="93358" xml_f="93502" txt_i="39691" txt_f="39835">). Next, for each group-level PCA algorithm tested in this paper, we conducted 20 different group ICA analyses, varying the number of subjects (</offsets><italic><offsets xml_i="93510" xml_f="93511" txt_i="39835" txt_f="39836">M</offsets></italic><offsets xml_i="93520" xml_f="93607" txt_i="39836" txt_f="39923">) used in each analysis by 100, 200, 400, 800, and 1600, and the number of components (</offsets><italic><offsets xml_i="93615" xml_f="93616" txt_i="39923" txt_f="39924">k</offsets></italic><offsets xml_i="93625" xml_f="93852" txt_i="39924" txt_f="40151">) by 25, 50, 75, and 100. The group-level PCA algorithms considered were EVD, Large PCA, multi power iteration (MPOWIT), subsampled voxel PCA (SVP), and subsampled time PCA (STP). For STP, the number of subjects in each group (</offsets><italic><offsets xml_i="93860" xml_f="93861" txt_i="40151" txt_f="40152">g</offsets></italic><offsets xml_i="93870" xml_f="93964" txt_i="40152" txt_f="40246">) was selected to be as large as possible and such that the analysis did not exceed 4 GB RAM (</offsets><italic><offsets xml_i="93972" xml_f="93973" txt_i="40246" txt_f="40247">g</offsets></italic><offsets xml_i="93982" xml_f="94633" txt_i="40247" txt_f="40898"> = 20 in our analyses). Since, Large PCA and MPOWIT could also be carried out by loading one dataset at a time per PCA iteration, we also included the un-stacked group PCA cases in the analyses. In total, 7 × (5 × 4) = 7 × 20 = 140 different group-level PCA cases were considered for comparison in terms of their accuracy and required computing time. In addition, the number of iterations until convergence was assessed for Large PCA and MPOWIT on each scenario. Finally, we illustrated the total group ICA pipeline computing times attainable using the MPOWIT method (stacked and un-stacked) in the group-level PCA stage (including dataloading times).</offsets></p></sec></sec><sec sec-type="results" id="s3"><title><offsets xml_i="94688" xml_f="94695" txt_i="40901" txt_f="40908">Results</offsets></title><sec><title><offsets xml_i="94715" xml_f="94734" txt_i="40909" txt_f="40928">Memory requirements</offsets></title><p><offsets xml_i="94745" xml_f="94858" txt_i="40929" txt_f="41042">After applying a common binary mask to all time points, there were 66,745 in-brain voxels per time point. Figure </offsets><xref ref-type="fig" rid="F2"><offsets xml_i="94888" xml_f="94889" txt_i="41042" txt_f="41043">2</offsets></xref><offsets xml_i="94896" xml_f="95082" txt_i="41043" txt_f="41229"> summarizes the memory requirements for each group PCA algorithm applied on this data, using the parameters specified in Section Accuracy, Computing Time, and Convergence. Note that for </offsets><italic><offsets xml_i="95090" xml_f="95091" txt_i="41229" txt_f="41230">M</offsets></italic><offsets xml_i="95100" xml_f="95178" txt_i="41230" txt_f="41308"> ≥ 800, the voxel dimension is smaller than the stacked time dimension (i.e., </offsets><italic><offsets xml_i="95186" xml_f="95187" txt_i="41308" txt_f="41309">v</offsets></italic><offsets xml_i="95196" xml_f="95202" txt_i="41309" txt_f="41312"> &lt; </offsets><italic><offsets xml_i="95210" xml_f="95212" txt_i="41312" txt_f="41314">Mp</offsets></italic><offsets xml_i="95221" xml_f="95830" txt_i="41314" txt_f="41923">). Still, we cannot use Equation (5) to compute the covariance matrix in the voxel dimension because the voxel dimension is also very large and it may take several hours to compute if loading each dataset at a time. Thus, EVD is not scalable for large data analysis as both the memory burden to compute the covariance matrix and the computational burden to solve the eigenvalue problem increase exponentially. On the other hand, the un-stacked versions of randomized PCA approaches like Large PCA and MPOWIT are scalable for large datasets, meaning that Large PCA and MPOWIT could load each subject's dataset </offsets><italic><offsets xml_i="95838" xml_f="95839" txt_i="41923" txt_f="41924">Y</offsets></italic><sub><italic><offsets xml_i="95861" xml_f="95862" txt_i="41924" txt_f="41925">i</offsets></italic></sub><offsets xml_i="95877" xml_f="96134" txt_i="41925" txt_f="42182"> at a time for each PCA iteration. Thus, un-stacked versions of these algorithms are also considered. Subsampled EVD methods like SVP and STP are also considered as these have fixed memory requirements and are independent of the number of subjects analyzed.</offsets></p><fig id="F2" position="float"><label><offsets xml_i="96175" xml_f="96183" txt_i="42183" txt_f="42191">Figure 2</offsets></label><caption><p><bold><offsets xml_i="96209" xml_f="96285" txt_i="42191" txt_f="42267">Approximate memory required in gigabytes (GB) to solve the group PCA problem</offsets></bold><offsets xml_i="96292" xml_f="96327" txt_i="42267" txt_f="42302">. The number of in-brain voxels is </offsets><italic><offsets xml_i="96335" xml_f="96336" txt_i="42302" txt_f="42303">v</offsets></italic><offsets xml_i="96345" xml_f="96409" txt_i="42303" txt_f="42367"> = 66745, the number of PCA components in the first PCA step is </offsets><italic><offsets xml_i="96417" xml_f="96418" txt_i="42367" txt_f="42368">p</offsets></italic><offsets xml_i="96427" xml_f="96486" txt_i="42368" txt_f="42427"> = 100, the number of components in the second PCA step is </offsets><italic><offsets xml_i="96494" xml_f="96495" txt_i="42427" txt_f="42428">k</offsets></italic><offsets xml_i="96504" xml_f="96539" txt_i="42428" txt_f="42463"> = 100 and the number of subjects (</offsets><italic><offsets xml_i="96547" xml_f="96548" txt_i="42463" txt_f="42464">M</offsets></italic><offsets xml_i="96557" xml_f="96641" txt_i="42464" txt_f="42548">) selected are 100, 200, 400, 800, and 1600. We used the number of block iterations </offsets><italic><offsets xml_i="96649" xml_f="96650" txt_i="42548" txt_f="42549">j</offsets></italic><offsets xml_i="96659" xml_f="96679" txt_i="42549" txt_f="42569"> = 6 and block size </offsets><italic><offsets xml_i="96687" xml_f="96688" txt_i="42569" txt_f="42570">b</offsets></italic><offsets xml_i="96697" xml_f="96825" txt_i="42570" txt_f="42698"> = 170 for Large PCA (see Appendix Section Parameter Selection for Large PCA; Supplementary Material), and the block multiplier </offsets><italic><offsets xml_i="96833" xml_f="96834" txt_i="42698" txt_f="42699">l</offsets></italic><offsets xml_i="96843" xml_f="96934" txt_i="42699" txt_f="42790"> was set to 5 for MPOWIT (see Appendix Section How to Select the Projecting Subspace Size (</offsets><italic><offsets xml_i="96942" xml_f="96943" txt_i="42790" txt_f="42791">l</offsets></italic><offsets xml_i="96952" xml_f="97030" txt_i="42791" txt_f="42869">) for MPOWIT?; Supplementary Material). The number of subjects in each group (</offsets><italic><offsets xml_i="97038" xml_f="97039" txt_i="42869" txt_f="42870">g</offsets></italic><offsets xml_i="97048" xml_f="97305" txt_i="42870" txt_f="43127">) is set to 20 when estimating PCA using STP. We give the equations used to estimate the memory required by each PCA algorithm discussed in this paper in Appendix Section How Much Memory is Required during the Group-Level PCA Step? (Supplementary Material).</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0002"></graphic></fig><p><offsets xml_i="97381" xml_f="97552" txt_i="43128" txt_f="43299">Some notes about multi-band EPI sequences and subject-level PCA are in order here. First, even if the fMRI data were acquired at a 2 × 2 × 2 mm voxel resolution (roughly, </offsets><italic><offsets xml_i="97560" xml_f="97561" txt_i="43299" txt_f="43300">v</offsets></italic><offsets xml_i="97570" xml_f="97901" txt_i="43300" txt_f="43631"> = 180000 in-brain voxels) and collected for 30 min using multi-band EPI sequences with TR = 200 ms, there would be no significant impact on the memory required to solve the first PCA step, including computations of the subject-specific covariance matrix. This is because the time dimension would still be considered “small” since </offsets><italic><offsets xml_i="97909" xml_f="97910" txt_i="43631" txt_f="43632">t</offsets></italic><offsets xml_i="97919" xml_f="98128" txt_i="43632" txt_f="43838"> = 30 × 60 × 5 = 9000 &lt; 10000. With less than 10,000 time points, the first PCA step could be easily solved by loading the data in blocks along the voxel dimension, summing covariance matrices of dimension </offsets><italic><offsets xml_i="98136" xml_f="98137" txt_i="43838" txt_f="43839">t</offsets></italic><offsets xml_i="98146" xml_f="98149" txt_i="43839" txt_f="43842"> × </offsets><italic><offsets xml_i="98157" xml_f="98158" txt_i="43842" txt_f="43843">t</offsets></italic><offsets xml_i="98167" xml_f="98189" txt_i="43843" txt_f="43865"> across blocks, i.e., </offsets><inline-formula><mml:math id="M61"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo><offsets xml_i="98293" xml_f="98294" txt_i="43865" txt_f="43866">∑</offsets></mml:mo></mml:mrow><mml:mrow><mml:mi><offsets xml_i="98332" xml_f="98333" txt_i="43866" txt_f="43867">n</offsets></mml:mi><mml:mo><offsets xml_i="98350" xml_f="98351" txt_i="43867" txt_f="43868">=</offsets></mml:mo><mml:mn><offsets xml_i="98368" xml_f="98369" txt_i="43868" txt_f="43869">1</offsets></mml:mn></mml:mrow><mml:mrow><mml:mi><offsets xml_i="98407" xml_f="98408" txt_i="43869" txt_f="43870">b</offsets></mml:mi><mml:mi><offsets xml_i="98425" xml_f="98426" txt_i="43870" txt_f="43871">l</offsets></mml:mi><mml:mi><offsets xml_i="98443" xml_f="98444" txt_i="43871" txt_f="43872">o</offsets></mml:mi><mml:mi><offsets xml_i="98461" xml_f="98462" txt_i="43872" txt_f="43873">c</offsets></mml:mi><mml:mi><offsets xml_i="98479" xml_f="98480" txt_i="43873" txt_f="43874">k</offsets></mml:mi><mml:mi><offsets xml_i="98497" xml_f="98498" txt_i="43874" txt_f="43875">s</offsets></mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mrow><mml:mo><offsets xml_i="98573" xml_f="98574" txt_i="43875" txt_f="43876">(</offsets></mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi><offsets xml_i="98621" xml_f="98622" txt_i="43876" txt_f="43877">C</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="98660" xml_f="98661" txt_i="43877" txt_f="43878">t</offsets></mml:mi><mml:mi><offsets xml_i="98678" xml_f="98679" txt_i="43878" txt_f="43879">t</offsets></mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo><offsets xml_i="98729" xml_f="98730" txt_i="43879" txt_f="43880">)</offsets></mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi><offsets xml_i="98779" xml_f="98780" txt_i="43880" txt_f="43881">n</offsets></mml:mi></mml:mrow></mml:msub><mml:mo><offsets xml_i="98819" xml_f="98820" txt_i="43881" txt_f="43882">=</offsets></mml:mo><mml:mi><offsets xml_i="98837" xml_f="98838" txt_i="43882" txt_f="43883">F</offsets></mml:mi><mml:mi><offsets xml_i="98855" xml_f="98856" txt_i="43883" txt_f="43884">Λ</offsets></mml:mi><mml:msup><mml:mrow><mml:mi><offsets xml_i="98893" xml_f="98894" txt_i="43884" txt_f="43885">F</offsets></mml:mi></mml:mrow><mml:mrow><mml:mi><offsets xml_i="98932" xml_f="98933" txt_i="43885" txt_f="43886">T</offsets></mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><offsets xml_i="98992" xml_f="99027" txt_i="43886" txt_f="43921">, and using the EVD (IRAM) method [</offsets><italic><offsets xml_i="99035" xml_f="99039" txt_i="43921" txt_f="43925">eigs</offsets></italic><offsets xml_i="99048" xml_f="99187" txt_i="43925" txt_f="44064">(·) function in MATLAB]. The memory required by EVD to solve the ensuing group-level PCA, however, increases with the number of components </offsets><italic><offsets xml_i="99195" xml_f="99196" txt_i="44064" txt_f="44065">p</offsets></italic><offsets xml_i="99205" xml_f="99261" txt_i="44065" txt_f="44121"> retained in the first PCA step, the number of subjects </offsets><italic><offsets xml_i="99269" xml_f="99270" txt_i="44121" txt_f="44122">M</offsets></italic><offsets xml_i="99279" xml_f="99315" txt_i="44122" txt_f="44158">, and the number of in-brain voxels </offsets><italic><offsets xml_i="99323" xml_f="99324" txt_i="44158" txt_f="44159">v</offsets></italic><offsets xml_i="99333" xml_f="99379" txt_i="44159" txt_f="44205">, assuming the entire temporally stacked data </offsets><italic><offsets xml_i="99387" xml_f="99388" txt_i="44205" txt_f="44206">Y</offsets></italic><offsets xml_i="99397" xml_f="99546" txt_i="44206" txt_f="44355"> is loaded in memory, which makes it unscalable. Unlike EVD, MPOWIT (un-stacked) would solve this group PCA problem using less than 4 GB RAM even if </offsets><italic><offsets xml_i="99554" xml_f="99555" txt_i="44355" txt_f="44356">p</offsets></italic><offsets xml_i="99564" xml_f="99610" txt_i="44356" txt_f="44402"> was increased from 100 to 200 components and </offsets><italic><offsets xml_i="99618" xml_f="99619" txt_i="44402" txt_f="44403">v</offsets></italic><offsets xml_i="99628" xml_f="99690" txt_i="44403" txt_f="44465"> was increased from 60000 to 180000 in-brain voxels, assuming </offsets><italic><offsets xml_i="99698" xml_f="99699" txt_i="44465" txt_f="44466">M</offsets></italic><offsets xml_i="99708" xml_f="99765" txt_i="44466" txt_f="44523"> = 5000 subjects, highlighting its scalability strengths.</offsets></p></sec><sec><title><offsets xml_i="99787" xml_f="99828" txt_i="44525" txt_f="44566">Accuracy, computing time, and convergence</offsets></title><p><offsets xml_i="99839" xml_f="100377" txt_i="44567" txt_f="45105">Here, we present results for the group PCA experiments described in Section Data and Preprocessing. If not specified otherwise, all processes were tested on a server running Linux Centos OS release 6.4 with 512 GB RAM, and MATLAB R2012a. We also note that the files with the results from the initial subject-level PCA step were always saved as uncompressed “.mat” files to later speed up the data loading process during the group PCA step. The parameter settings used to solve the group PCA problem for each algorithm are described below:</offsets></p><list list-type="alpha-lower"><list-item><p><bold><offsets xml_i="100431" xml_f="100434" txt_i="45106" txt_f="45109">EVD</offsets></bold><offsets xml_i="100441" xml_f="100635" txt_i="45109" txt_f="45303">: The covariance matrix is always computed in the smallest dimension of the temporally stacked data. The IRAM algorithm is used to find the desired eigenvectors. In this study, we used MATLAB's </offsets><italic><offsets xml_i="100643" xml_f="100647" txt_i="45303" txt_f="45307">eigs</offsets></italic><offsets xml_i="100656" xml_f="100753" txt_i="45307" txt_f="45404"> (·) function, which is built on the IRAM method. The maximum error tolerance selected was δ = 10</offsets><sup><offsets xml_i="100758" xml_f="100760" txt_i="45404" txt_f="45406">−6</offsets></sup><offsets xml_i="100766" xml_f="100820" txt_i="45406" txt_f="45460"> and the maximum number of iterations was set to 1000.</offsets></p></list-item><list-item><p><bold><offsets xml_i="100856" xml_f="100865" txt_i="45461" txt_f="45470">Large PCA</offsets></bold><offsets xml_i="100872" xml_f="101039" txt_i="45470" txt_f="45637">: Here, we used settings based on the Pareto-optimal study in Appendix Section Parameter Selection for Large PCA (Supplementary Material). The block length was set to </offsets><italic><offsets xml_i="101047" xml_f="101048" txt_i="45637" txt_f="45638">b</offsets></italic><offsets xml_i="101057" xml_f="101104" txt_i="45638" txt_f="45685"> = 170, the number of initial block iterations </offsets><italic><offsets xml_i="101112" xml_f="101113" txt_i="45685" txt_f="45686">j</offsets></italic><sub><offsets xml_i="101127" xml_f="101128" txt_i="45686" txt_f="45687">0</offsets></sub><offsets xml_i="101134" xml_f="101185" txt_i="45687" txt_f="45738"> was set to 6 and the error tolerance was set to 10</offsets><sup><offsets xml_i="101190" xml_f="101192" txt_i="45738" txt_f="45740">−6</offsets></sup><offsets xml_i="101198" xml_f="101245" txt_i="45740" txt_f="45787">. Note that as the number of block iterations (</offsets><italic><offsets xml_i="101253" xml_f="101254" txt_i="45787" txt_f="45788">j</offsets></italic><offsets xml_i="101263" xml_f="101384" txt_i="45788" txt_f="45909">) executed until convergence increases, the performance of the algorithm decreases (i.e., higher memory and lower speed).</offsets></p></list-item><list-item><p><bold><offsets xml_i="101420" xml_f="101426" txt_i="45910" txt_f="45916">MPOWIT</offsets></bold><offsets xml_i="101433" xml_f="101510" txt_i="45916" txt_f="45993">: The maximum error tolerance and maximum number of iterations were set to 10</offsets><sup><offsets xml_i="101515" xml_f="101517" txt_i="45993" txt_f="45995">−6</offsets></sup><offsets xml_i="101523" xml_f="101563" txt_i="45995" txt_f="46035"> and 1000, respectively. The multiplier </offsets><italic><offsets xml_i="101571" xml_f="101572" txt_i="46035" txt_f="46036">l</offsets></italic><offsets xml_i="101581" xml_f="101661" txt_i="46036" txt_f="46116"> was set to 5 (see Appendix Section How to Select the Projecting Subspace Size (</offsets><italic><offsets xml_i="101669" xml_f="101670" txt_i="46116" txt_f="46117">l</offsets></italic><offsets xml_i="101679" xml_f="101770" txt_i="46117" txt_f="46208">) for MPOWIT?; Supplementary Material) to improve the rate of convergence. As the value of </offsets><italic><offsets xml_i="101778" xml_f="101779" txt_i="46208" txt_f="46209">l</offsets></italic><offsets xml_i="101788" xml_f="101946" txt_i="46209" txt_f="46367"> increases, the rate of convergence and accuracy increase but the computational performance decreases (i.e., higher memory and lower speed in each iteration).</offsets></p></list-item><list-item><p><bold><offsets xml_i="101982" xml_f="101985" txt_i="46368" txt_f="46371">SVP</offsets></bold><offsets xml_i="101992" xml_f="102120" txt_i="46371" txt_f="46499">: A value of 2 is selected as subsampling depth and 500 intermediate PCA components are estimated for odd and even voxel spaces.</offsets></p></list-item><list-item><p><bold><offsets xml_i="102156" xml_f="102159" txt_i="46500" txt_f="46503">STP</offsets></bold><offsets xml_i="102166" xml_f="102281" txt_i="46503" txt_f="46618">: The number of subjects included in each group is 20, and 500 intermediate PCA components are estimated per group.</offsets></p></list-item></list><p><offsets xml_i="102307" xml_f="102311" txt_i="46619" txt_f="46623">The </offsets><italic><offsets xml_i="102319" xml_f="102320" txt_i="46623" txt_f="46624">L</offsets></italic><sub><offsets xml_i="102334" xml_f="102335" txt_i="46624" txt_f="46625">2</offsets></sub><offsets xml_i="102341" xml_f="102390" txt_i="46625" txt_f="46674">-norm of the absolute difference between the top </offsets><italic><offsets xml_i="102398" xml_f="102399" txt_i="46674" txt_f="46675">k</offsets></italic><offsets xml_i="102408" xml_f="102643" txt_i="46675" txt_f="46910"> eigenvalues obtained from the randomized PCA methods and those from the EVD (IRAM) method were used to determine the accuracy of the estimated PCA components. The eigenvalues from the EVD method are always considered the ground-truth.</offsets></p><p><offsets xml_i="102650" xml_f="102662" txt_i="46911" txt_f="46923">From Figure </offsets><xref ref-type="fig" rid="F3"><offsets xml_i="102692" xml_f="102693" txt_i="46923" txt_f="46924">3</offsets></xref><offsets xml_i="102700" xml_f="103055" txt_i="46924" txt_f="47279">, it is evident that both the Large PCA and MPOWIT methods give more accurate results than subsampled PCA methods SVP and STP across all model orders. Overall, STP estimates components with higher accuracy than SVP across all model orders. We also note that, generally, Large PCA estimates PCA components with slightly better accuracy than MPOWIT. Figure </offsets><xref ref-type="fig" rid="F4"><offsets xml_i="103085" xml_f="103086" txt_i="47279" txt_f="47280">4</offsets></xref><offsets xml_i="103093" xml_f="103411" txt_i="47280" txt_f="47598"> compares the computing time taken in minutes by each algorithm to solve the group-level PCA problem. Subsampled PCA methods like SVP and STP outperform EVD and unstacked versions of MPOWIT and Large PCA at the cost of accuracy. MPOWIT and Large PCA outperform EVD when the entire data is loaded in memory, i.e., when </offsets><italic><offsets xml_i="103419" xml_f="103420" txt_i="47598" txt_f="47599">Y</offsets></italic><offsets xml_i="103429" xml_f="103579" txt_i="47599" txt_f="47749"> fits in RAM. When the data is not loaded in memory, MPOWIT (un-stacked) marginally outperforms EVD and Large PCA (un-stacked) at lower model orders (</offsets><italic><offsets xml_i="103587" xml_f="103588" txt_i="47749" txt_f="47750">k</offsets></italic><offsets xml_i="103597" xml_f="103607" txt_i="47750" txt_f="47760"> = 25 and </offsets><italic><offsets xml_i="103615" xml_f="103616" txt_i="47760" txt_f="47761">k</offsets></italic><offsets xml_i="103625" xml_f="103664" txt_i="47761" txt_f="47800"> = 50) whereas at higher model orders (</offsets><italic><offsets xml_i="103672" xml_f="103673" txt_i="47800" txt_f="47801">k</offsets></italic><offsets xml_i="103682" xml_f="103692" txt_i="47801" txt_f="47811"> = 75 and </offsets><italic><offsets xml_i="103700" xml_f="103701" txt_i="47811" txt_f="47812">k</offsets></italic><offsets xml_i="103710" xml_f="103815" txt_i="47812" txt_f="47917"> = 100), Large PCA (un-stacked) marginally outperforms both EVD and MPOWIT (un-stacked). Finally, Figure </offsets><xref ref-type="fig" rid="F5"><offsets xml_i="103845" xml_f="103846" txt_i="47917" txt_f="47918">5</offsets></xref><offsets xml_i="103853" xml_f="104043" txt_i="47918" txt_f="48108"> shows the number of iterations MPOWIT and Large PCA take to converge. Overall, Large PCA takes fewer iterations to converge than MPOWIT due to larger block sizes. However, note from Figure </offsets><xref ref-type="fig" rid="F2"><offsets xml_i="104073" xml_f="104074" txt_i="48108" txt_f="48109">2</offsets></xref><offsets xml_i="104081" xml_f="104217" txt_i="48109" txt_f="48245"> that MPOWIT (un-stacked) requires considerably less memory than Large PCA (un-stacked) with fairly sublinear increase in memory use as </offsets><italic><offsets xml_i="104225" xml_f="104226" txt_i="48245" txt_f="48246">M</offsets></italic><offsets xml_i="104235" xml_f="104246" txt_i="48246" txt_f="48257"> increases.</offsets></p><fig id="F3" position="float"><label><offsets xml_i="104287" xml_f="104295" txt_i="48258" txt_f="48266">Figure 3</offsets></label><caption><p><bold><offsets xml_i="104321" xml_f="104363" txt_i="48266" txt_f="48308">Estimation error as compared to EVD (IRAM)</offsets></bold><offsets xml_i="104370" xml_f="104372" txt_i="48308" txt_f="48310">. </offsets><italic><offsets xml_i="104380" xml_f="104381" txt_i="48310" txt_f="48311">L</offsets></italic><sub><offsets xml_i="104395" xml_f="104396" txt_i="48311" txt_f="48312">2</offsets></sub><offsets xml_i="104402" xml_f="104506" txt_i="48312" txt_f="48416">-norm of error is computed between the eigenvalues of each method and the eigenvalues of the EVD method.</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0003"></graphic></fig><fig id="F4" position="float"><label><offsets xml_i="104616" xml_f="104624" txt_i="48417" txt_f="48425">Figure 4</offsets></label><caption><p><bold><offsets xml_i="104650" xml_f="104769" txt_i="48425" txt_f="48544">Computing time (in minutes) taken to solve group-level PCA using EVD (IRAM), Large PCA, MPOWIT, SVP, and STP algorithms</offsets></bold><offsets xml_i="104776" xml_f="104922" txt_i="48544" txt_f="48690">. Using different numbers of subjects and components. The computing time of both Large PCA (un-stacked) and MPOWIT (un-stacked) are also reported.</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0004"></graphic></fig><fig id="F5" position="float"><label><offsets xml_i="105032" xml_f="105040" txt_i="48691" txt_f="48699">Figure 5</offsets></label><caption><p><bold><offsets xml_i="105066" xml_f="105152" txt_i="48699" txt_f="48785">Number of iterations required for convergence for both Large PCA and MPOWIT algorithms</offsets></bold><offsets xml_i="105159" xml_f="105160" txt_i="48785" txt_f="48786">.</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0005"></graphic></fig></sec><sec><title><offsets xml_i="105251" xml_f="105292" txt_i="48788" txt_f="48829">Group ICA and subject back-reconstruction</offsets></title><p><offsets xml_i="105303" xml_f="105477" txt_i="48830" txt_f="49004">Spatial ICA was performed on the final group-level PCA components to determine maximally statistically independent components. The Infomax ICA algorithm (Bell and Sejnowski, </offsets><xref rid="B3" ref-type="bibr"><offsets xml_i="105508" xml_f="105512" txt_i="49004" txt_f="49008">1995</offsets></xref><offsets xml_i="105519" xml_f="105583" txt_i="49008" txt_f="49072">) was repeated 10 times in an ICASSO framework (Himberg et al., </offsets><xref rid="B19" ref-type="bibr"><offsets xml_i="105615" xml_f="105619" txt_i="49072" txt_f="49076">2004</offsets></xref><offsets xml_i="105626" xml_f="105770" txt_i="49076" txt_f="49220">) with a different random initialization at each run. The most stable run estimates were used instead of using centrotype estimates (Ma et al., </offsets><xref rid="B25" ref-type="bibr"><offsets xml_i="105802" xml_f="105806" txt_i="49220" txt_f="49224">2011</offsets></xref><offsets xml_i="105813" xml_f="105878" txt_i="49224" txt_f="49289">). We used the GICA1 back-reconstruction method (Erhardt et al., </offsets><xref rid="B11" ref-type="bibr"><offsets xml_i="105910" xml_f="105914" txt_i="49289" txt_f="49293">2011</offsets></xref><offsets xml_i="105921" xml_f="106097" txt_i="49293" txt_f="49469">) to reconstruct individual subject component maps and timecourses for each analysis. Individual subject component maps and timecourses were then scaled to Z-scores. In Figure </offsets><xref ref-type="fig" rid="F6"><offsets xml_i="106127" xml_f="106128" txt_i="49469" txt_f="49470">6</offsets></xref><offsets xml_i="106135" xml_f="106328" txt_i="49470" txt_f="49663">, we illustrate the total group ICA analysis computing times attainable using the MPOWIT method (stacked and un-stacked, respectively) in the group-level PCA stage (including dataloading time).</offsets></p><fig id="F6" position="float"><label><offsets xml_i="106369" xml_f="106377" txt_i="49664" txt_f="49672">Figure 6</offsets></label><caption><p><bold><offsets xml_i="106403" xml_f="106501" txt_i="49672" txt_f="49770">Total time to solve the entire group ICA analysis using MPOWIT for the group PCA step (in minutes)</offsets></bold><offsets xml_i="106508" xml_f="106650" txt_i="49770" txt_f="49912">. Computing times when all the data is loaded in memory (stacked) and when datasets are loaded each at a time in every iteration (un-stacked).</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0006"></graphic></fig></sec></sec><sec sec-type="discussion" id="s4"><title><offsets xml_i="106777" xml_f="106787" txt_i="49915" txt_f="49925">Discussion</offsets></title><p><offsets xml_i="106798" xml_f="106928" txt_i="49926" txt_f="50056">We demonstrated the entire group ICA process including the group PCA step on a Linux server with 512 GB RAM. We infer from Figure </offsets><xref ref-type="fig" rid="F2"><offsets xml_i="106958" xml_f="106959" txt_i="50056" txt_f="50057">2</offsets></xref><offsets xml_i="106966" xml_f="107398" txt_i="50057" txt_f="50489"> that a large group ICA analysis using un-stacked versions of MPOWIT and Large PCA can be performed on machines with only 4 GB RAM. Moreover, the un-stacked versions of MPOWIT and Large PCA are designed to solve the group PCA problem by loading one dataset at a time. This is a nice feature which makes these algorithms ideal for PCA of “big data” where chunks of data can be extracted one at a time using memory mapping techniques.</offsets></p><p><offsets xml_i="107405" xml_f="107595" txt_i="50490" txt_f="50680">Comparing among EVD, Large PCA, and MPOWIT, we notice that MPOWIT and Large PCA outperform EVD (IRAM) in terms of speed when all datasets are already loaded in memory. As depicted in Figure </offsets><xref ref-type="fig" rid="F4"><offsets xml_i="107625" xml_f="107626" txt_i="50680" txt_f="50681">4</offsets></xref><offsets xml_i="107633" xml_f="107992" txt_i="50681" txt_f="51040">, pink (Large PCA) and light green (MPOWIT) bars are always shorter than the blue (EVD) bar. When only one subject's dataset can be loaded in memory at a time, in which case un-stacked MPOWIT and un-stacked Large PCA have to be used, the computation time increases as showed by the dark green (MPOWIT un-stacked) and red bars (Large PCA un-stacked) in Figure </offsets><xref ref-type="fig" rid="F4"><offsets xml_i="108022" xml_f="108023" txt_i="51040" txt_f="51041">4</offsets></xref><offsets xml_i="108030" xml_f="108890" txt_i="51041" txt_f="51901">. Overall, blue and dark green bars are comparable at various model orders. Dark green bars marginally outperform dark red and blue bars at lower model orders whereas dark red bars marginally outperform dark green and blue bars at higher model orders. We note that un-stacked Large PCA uses larger block size (Krylov subspace) than un-stacked MPOWIT and, therefore, converges faster. In our experiments, both Large PCA and MPOWIT take at least seven iterations to converge to the PCA solution. With increasing datasets, data loading could be a big bottleneck in computational performance. To speed up the process further, we recommend using PCA estimates from subsampled PCA [Section Sub-sampled Time PCA (STP)] instead of random initialization. In our examples, MPOWIT and Large PCA methods provide very similar principal components, differing by less than 10</offsets><sup><offsets xml_i="108895" xml_f="108897" txt_i="51901" txt_f="51903">−6</offsets></sup><offsets xml_i="108903" xml_f="108904" txt_i="51903" txt_f="51904">.</offsets></p><p><offsets xml_i="108911" xml_f="109165" txt_i="51905" txt_f="52159">The PCA methods we discussed in this paper are generic and can be applied to any dataset without any major modifications. However, our goal here is to demonstrate the applicability of these algorithms to real-valued fMRI data in the context of group ICA.</offsets></p><p><offsets xml_i="109172" xml_f="109403" txt_i="52160" txt_f="52391">The execution time for the largest group ICA analysis in this paper, i.e., 1600 subjects and 100 independent components, using the un-stacked version of our MPOWIT PCA algorithm for the group-level PCA, was 329 min (5.48 h, Figure </offsets><xref ref-type="fig" rid="F6"><offsets xml_i="109433" xml_f="109434" txt_i="52391" txt_f="52392">6</offsets></xref><offsets xml_i="109441" xml_f="109964" txt_i="52392" txt_f="52915">). Reloading the datasets from the first PCA step was fast in our Linux server due to the Operating System's “cache effect.” We repeated the 1600 subject group-PCA problem with model orders 50 and 100 on a Windows desktop with 4 GB RAM using both MPOWIT and Large PCA. To speed up the PCA estimation of MPOWIT and Large PCA, 500 PCA components from subsampled PCA [Section Sub-Sampled Time PCA (STP)] were used as the initial PCA subspace. Since components estimated by STP method are more accurate than SVP method (Figure </offsets><xref ref-type="fig" rid="F3"><offsets xml_i="109994" xml_f="109995" txt_i="52915" txt_f="52916">3</offsets></xref><offsets xml_i="110002" xml_f="110057" txt_i="52916" txt_f="52971">), STP components are used as initial subspace. Figure </offsets><xref ref-type="fig" rid="F7"><offsets xml_i="110087" xml_f="110088" txt_i="52971" txt_f="52972">7</offsets></xref><offsets xml_i="110095" xml_f="110576" txt_i="52972" txt_f="53453"> shows that both MPOWIT and Large PCA successfully recovered 100 group PCA components with high accuracy in little more than 3 h (including the STP PCA estimation). MPOWIT required three iterations and large PCA required two block iterations to solve the largest group PCA problem in this paper which would have been an impossible problem to solve using EVD or assuming the entire (reduced) data from the first PCA step had to be loaded in memory during the group PCA step (Figure </offsets><xref ref-type="fig" rid="F2"><offsets xml_i="110606" xml_f="110607" txt_i="53453" txt_f="53454">2</offsets></xref><offsets xml_i="110614" xml_f="110773" txt_i="53454" txt_f="53613">). A significant speedup can still be achieved if the entire group ICA process is run in parallel using, for example, GPU acceleration or distributed clusters.</offsets></p><fig id="F7" position="float"><label><offsets xml_i="110814" xml_f="110822" txt_i="53614" txt_f="53622">Figure 7</offsets></label><caption><p><bold><offsets xml_i="110848" xml_f="110915" txt_i="53622" txt_f="53689">Comparison of MPOWIT and Large PCA on Windows desktop with 4 GB RAM</offsets></bold><offsets xml_i="110922" xml_f="111107" txt_i="53689" txt_f="53874">. Initial PCA subspace is selected from STP method to accelerate algorithms MPOWIT and Large PCA for very large data (both un-stacked). Error in top row second column is computed using </offsets><italic><offsets xml_i="111115" xml_f="111116" txt_i="53874" txt_f="53875">L</offsets></italic><sub><offsets xml_i="111130" xml_f="111131" txt_i="53875" txt_f="53876">2</offsets></sub><offsets xml_i="111137" xml_f="111236" txt_i="53876" txt_f="53975"> - norm of difference between the eigenvalues obtained from the selected PCA method and EVD method.</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0007"></graphic></fig><p><offsets xml_i="111312" xml_f="111868" txt_i="53976" txt_f="54532">Our results also emphasize that EVD is not preferred for large scale analysis as there is an extra cost in storing the covariance matrix in memory. One way to overcome this issue is based on Equation (5). If one dimension of the data is fixed, the covariance matrix can be computed in that dimension and the net covariance matrix could be computed by adding the covariance matrices across subjects. Moreover, since the covariance matrix is symmetric, only the lower or upper triangular portions need to be stored in memory (for eigenvalue problems). LAPACK</offsets><xref ref-type="fn" rid="fn0008"><sup><offsets xml_i="111906" xml_f="111907" txt_i="54532" txt_f="54533">8</offsets></sup></xref><offsets xml_i="111920" xml_f="112167" txt_i="54533" txt_f="54780"> eigen solvers such as DSPEVX or SSPEVX leverage this property and use only lower or upper triangular portions of the matrix. However, the performance (computing speed) of these algorithms will still be poor if the covariance matrix is very large.</offsets></p><p><offsets xml_i="112174" xml_f="112287" txt_i="54781" txt_f="54894">The PCA methods applied for performing spatial group ICA are equally valid for temporal group ICA (Smith et al., </offsets><xref rid="B34" ref-type="bibr"><offsets xml_i="112319" xml_f="112323" txt_i="54894" txt_f="54898">2012</offsets></xref><offsets xml_i="112330" xml_f="112517" txt_i="54898" txt_f="55085">). In temporal group ICA, subject-level PCA is not performed and, thus, group PCA is computed directly on the preprocessed fMRI datasets stacked along the time dimension (Boubela et al., </offsets><xref rid="B5" ref-type="bibr"><offsets xml_i="112548" xml_f="112552" txt_i="55085" txt_f="55089">2013</offsets></xref><offsets xml_i="112559" xml_f="112682" txt_i="55089" txt_f="55212">). The memory requirements for the temporal group PCA step using the PCA methods we presented here could be computed using </offsets><italic><offsets xml_i="112690" xml_f="112691" txt_i="55212" txt_f="55213">p</offsets></italic><offsets xml_i="112700" xml_f="112703" txt_i="55213" txt_f="55216"> = </offsets><italic><offsets xml_i="112711" xml_f="112712" txt_i="55216" txt_f="55217">t</offsets></italic><offsets xml_i="112721" xml_f="113169" txt_i="55217" txt_f="55665"> (See Appendix Section How Much Memory is Required during the Group-Level PCA Step?; Supplementary Material). Notice that loading the full preprocessed fMRI datasets instead of loading just the subject-level PCA results (as in spatial group PCA) is considerably more time-consuming. Conveniently, our MPOWIT PCA approach supports processing un-stacked datasets and, thus, is highly suitable for very large temporal group PCA of multi-band EPI fMRI.</offsets></p><p><offsets xml_i="113176" xml_f="113260" txt_i="55666" txt_f="55750">Based on the methods discussed here, we present our findings in a flowchart (Figure </offsets><xref ref-type="fig" rid="F8"><offsets xml_i="113290" xml_f="113291" txt_i="55750" txt_f="55751">8</offsets></xref><offsets xml_i="113298" xml_f="113416" txt_i="55751" txt_f="55869">), indicating our recommendations for selection of the PCA method given the problem size and its order of complexity (</offsets><italic><offsets xml_i="113424" xml_f="113425" txt_i="55869" txt_f="55870">k</offsets></italic><offsets xml_i="113434" xml_f="113510" txt_i="55870" txt_f="55946">). The EVD (IRAM) method is preferred when the problem size is small (i.e., </offsets><italic><offsets xml_i="113518" xml_f="113519" txt_i="55946" txt_f="55947">v</offsets></italic><offsets xml_i="113528" xml_f="113540" txt_i="55947" txt_f="55959"> ≤ 10000 or </offsets><italic><offsets xml_i="113548" xml_f="113550" txt_i="55959" txt_f="55961">Mp</offsets></italic><offsets xml_i="113559" xml_f="114549" txt_i="55961" txt_f="56951"> ≤ 10000). Note, however, that MPOWIT could also be applied directly on the covariance matrix [Equation (30), left-hand side]. Thus, if the smallest dimension of the data is less than or equal to 10000, applying MPOWIT on the covariance matrix requires only a single dataload per subject and is more computationally efficient than Equation (30) (right-hand side), which requires one dataload per subject in each iteration. Finally, randomized PCA methods such as MPOWIT, EM PCA (accelerated), and Large PCA are efficient in memory and/or speed on large data problems that could be fit in the computer RAM. However, when the data is too large to fit in the computer RAM, un-stacked versions of MPOWIT or Large PCA are preferred with PCA subspace initialization from STP method. At most 2–3 iterations are required to estimate PCA components with high accuracy when the PCA subspace is initialized using STP. However, the performance of Large PCA is highly dependent on the correct tuning of </offsets><italic><offsets xml_i="114557" xml_f="114558" txt_i="56951" txt_f="56952">j</offsets></italic><sub><offsets xml_i="114572" xml_f="114573" txt_i="56952" txt_f="56953">0</offsets></sub><offsets xml_i="114579" xml_f="114584" txt_i="56953" txt_f="56958"> and </offsets><italic><offsets xml_i="114592" xml_f="114593" txt_i="56958" txt_f="56959">b</offsets></italic><offsets xml_i="114602" xml_f="114802" txt_i="56959" txt_f="57159"> (see Appendix Section Parameter Selection for Large PCA; Supplementary Material) for each given dataset. MPOWIT, on the other hand, seems to produce comparable results with little need for tuning of </offsets><italic><offsets xml_i="114810" xml_f="114811" txt_i="57159" txt_f="57160">l</offsets></italic><offsets xml_i="114820" xml_f="114887" txt_i="57160" txt_f="57227"> (see Appendix Section How to Select the Projecting Subspace Size (</offsets><italic><offsets xml_i="114895" xml_f="114896" txt_i="57227" txt_f="57228">l</offsets></italic><offsets xml_i="114905" xml_f="114944" txt_i="57228" txt_f="57267">) for MPOWIT?; Supplementary Material).</offsets></p><fig id="F8" position="float"><label><offsets xml_i="114985" xml_f="114993" txt_i="57268" txt_f="57276">Figure 8</offsets></label><caption><p><bold><offsets xml_i="115019" xml_f="115042" txt_i="57276" txt_f="57299">PCA Algorithm selection</offsets></bold><offsets xml_i="115049" xml_f="115231" txt_i="57299" txt_f="57481">. This flowchart summarizes our recommendations for selection of the most appropriate PCA method given the dimensions of the data (v × Mp), where v is the number of in-brain voxels, </offsets><italic><offsets xml_i="115239" xml_f="115240" txt_i="57481" txt_f="57482">p</offsets></italic><offsets xml_i="115249" xml_f="115314" txt_i="57482" txt_f="57547"> is the number of components retained in the first PCA step, and </offsets><italic><offsets xml_i="115322" xml_f="115323" txt_i="57547" txt_f="57548">M</offsets></italic><offsets xml_i="115332" xml_f="115359" txt_i="57548" txt_f="57575"> is the number of subjects.</offsets></p></caption><graphic xlink:href="fnins-10-00017-g0008"></graphic></fig></sec><sec sec-type="conclusions" id="s5"><title><offsets xml_i="115481" xml_f="115492" txt_i="57577" txt_f="57588">Conclusions</offsets></title><p><offsets xml_i="115503" xml_f="116613" txt_i="57589" txt_f="58699">We presented a new approach for PCA-based data reduction for group ICA called MPOWIT and demonstrated that it can efficiently solve the large scale PCA problem without compromising accuracy. The un-stacked version of MPOWIT takes almost the same time to complete the analysis as compared to EVD but requires much less RAM. We showed that MPOWIT enables group ICA on very large cohorts using standard fMRI acquisition parameters within 4 GB RAM. Computationally efficient data reduction approaches like MPOWIT are becoming more important due to the larger datasets resulting from new studies using high-frequency multi-band EPI sequences and from an increased tendency to share data in the neuroimaging community. Even in such challenging scenarios, un-stacked MPOWIT could realistically solve group-level PCA on virtually any number of subjects, limited only by time, not memory, constraints. Given its high scalability and right fit for parallelism, MPOWIT sets the stage for future groundbreaking developments toward extremely efficient PCA of big data using GPU acceleration and distributed implementations.</offsets></p></sec><sec id="s6"><title><offsets xml_i="116643" xml_f="116663" txt_i="58701" txt_f="58721">Author contributions</offsets></title><p><offsets xml_i="116674" xml_f="116953" txt_i="58722" txt_f="59001">SR—Developed PCA algorithm (MPOWIT) for data reduction. RS—Worked extensively on revising paper and helped with mathematical proofs. JL—Gave good feedback on improving the manuscript. VC—Gave good feedback on improving the manuscript and also work was done under his supervision.</offsets></p></sec><sec><title><offsets xml_i="116975" xml_f="116982" txt_i="59003" txt_f="59010">Funding</offsets></title><p><offsets xml_i="116993" xml_f="117093" txt_i="59011" txt_f="59111">This work was funded by NIH 2R01EB000840, R01EB020407, and COBRE 5P20RR021938/P20GM103472 (Calhoun).</offsets></p><sec><title><offsets xml_i="117109" xml_f="117139" txt_i="59112" txt_f="59142">Conflict of interest statement</offsets></title><p><offsets xml_i="117150" xml_f="117322" txt_i="59143" txt_f="59315">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</offsets></p></sec></sec></body><back><ack><p>The authors would like to thank Tim Mounce, IT administrator at the MIND Research Network, for valuable help installing and mounting software and drives in our Linux servers. We would also like to thank Dr. Tülay Adalı, Professor at the University of Maryland Baltimore County, for giving valuable comments.</p></ack><fn-group><fn id="fn0001"><p><sup>1</sup>The choice of whitening or not the subject-level data changes the final group PCA estimates (Calhoun et al., <xref rid="B10" ref-type="bibr">2015</xref>). However, all it does is preprocess the input data in a different way. While we advocate whitening of subject-level fMRI data for group spatial ICA due to its denoising properties, it remains optional and open for debate (Smith et al., <xref rid="B33" ref-type="bibr">2014</xref>). However, whitening of subject-level data does not alter the correctness of the group PCA methods we present in this paper, which are correct regardless of the choice of preprocessing.</p></fn><fn id="fn0002"><p><sup>2</sup><ext-link ext-link-type="uri" xlink:href="http://mialab.mrn.org/software/gift/">http://mialab.mrn.org/software/gift/</ext-link></p></fn><fn id="fn0003"><p><sup>3</sup>Direct reduction of the largest dimension of subject-level data is <italic>never</italic> recommended as it causes greater loss of information and largely inferior estimates than otherwise. Instead, a better approach is to carry out group PCA in the time dimension as in Figure <xref ref-type="fig" rid="F1">1</xref>, <italic>without</italic> subject-level whitening, followed by <inline-formula><mml:math id="M62"><mml:msup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to perform reduction of the voxel dimension. Group temporal ICA can then be carried out on the temporally concatenated <inline-formula><mml:math id="M63"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></inline-formula></p></fn><fn id="fn0004"><p><sup>4</sup>Large data means <italic>both</italic> dimensions of the data are larger than 10000 (assuming the data is stored using double precision).</p></fn><fn id="fn0005"><p><sup>5</sup>This was in addition to subject-level whitening.</p></fn><fn id="fn0006"><p><sup>6</sup>Notice that the Krylov subspace size increases with larger block sizes, as well as the time to solve economy-size singular value decomposition for each iteration.</p></fn><fn id="fn0007"><p><sup>7</sup>MATLAB's <italic>eigs</italic>(·) function is based on the IRAM method to estimate desired eigenvectors and eigenvalues.</p></fn><fn id="fn0008"><p><sup>8</sup><ext-link ext-link-type="uri" xlink:href="http://www.netlib.org/lapack/">http://www.netlib.org/lapack/</ext-link></p></fn></fn-group><sec sec-type="supplementary-material" id="s7"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnins.2016.00017">http://journal.frontiersin.org/article/10.3389/fnins.2016.00017</ext-link></p><p><supplementary-material content-type="local-data" id="SM1"><media xlink:href="DataSheet1.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></p></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>E. A.</given-names></name><name><surname>Erhardt</surname><given-names>E. B.</given-names></name><name><surname>Damaraju</surname><given-names>E.</given-names></name><name><surname>Gruner</surname><given-names>W.</given-names></name><name><surname>Segall</surname><given-names>J. M.</given-names></name><name><surname>Silva</surname><given-names>R. F.</given-names></name><etal></etal></person-group>. (<year>2011</year>). <article-title>A baseline for the multivariate comparison of resting-state networks</article-title>. <source>Front. Syst. Neurosci.</source>
<volume>5</volume>:<issue>2</issue>. <pub-id pub-id-type="doi">10.3389/fnsys.2011.00002</pub-id><pub-id pub-id-type="pmid">21442040</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name></person-group> (<year>2004</year>). <article-title>Probabilistic independent component analysis for functional magnetic resonance imaging</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>23</volume>, <fpage>137</fpage>–<lpage>152</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2003.822821</pub-id><pub-id pub-id-type="pmid">14964560</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>A. J.</given-names></name><name><surname>Sejnowski</surname><given-names>T. J.</given-names></name></person-group> (<year>1995</year>). <article-title>An information maximisation approach to blind separation and blind deconvolution</article-title>. <source>Neural Comput</source>. <volume>7</volume>, <fpage>1129</fpage>–<lpage>1159</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id><pub-id pub-id-type="pmid">7584893</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswal</surname><given-names>B. B.</given-names></name><name><surname>Mennes</surname><given-names>M.</given-names></name><name><surname>Zuo</surname><given-names>X.-N.</given-names></name><name><surname>Gohel</surname><given-names>S.</given-names></name><name><surname>Kelly</surname><given-names>C.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name><etal></etal></person-group>. (<year>2010</year>). <article-title>Toward discovery science of human brain function</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>107</volume>, <fpage>4734</fpage>–<lpage>4739</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0911855107</pub-id><pub-id pub-id-type="pmid">20176931</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boubela</surname><given-names>R. N.</given-names></name><name><surname>Kalcher</surname><given-names>K.</given-names></name><name><surname>Huf</surname><given-names>W.</given-names></name><name><surname>Kronnerwetter</surname><given-names>C.</given-names></name><name><surname>Filzmoser</surname><given-names>P.</given-names></name><name><surname>Moser</surname><given-names>E.</given-names></name></person-group> (<year>2013</year>). <article-title>Beyond noise: using temporal ICA to extract meaningful information from high-frequency fMRI signal fluctuations during rest</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>7</volume>:<issue>168</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00168</pub-id><pub-id pub-id-type="pmid">23641208</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brand</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>Fast online SVD revisions for lightweight recommender systems</article-title>, in <source>2003 SIAM International Conference on Data Mining</source>, eds <person-group person-group-type="editor"><name><surname>Barbara</surname><given-names>D.</given-names></name><name><surname>Kamath</surname><given-names>C.</given-names></name></person-group> (<publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>SIAM</publisher-name>).</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name></person-group> (<year>2012</year>). <article-title>Multisubject independent component analysis of fMRI: a decade of intrinsic networks, default mode, and neurodiagnostic discovery</article-title>. <source>IEEE Rev. Biomed. Eng.</source>
<volume>5</volume>, <fpage>60</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1109/RBME.2012.2211076</pub-id><pub-id pub-id-type="pmid">23231989</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Pearlson</surname><given-names>G. D.</given-names></name><name><surname>Pekar</surname><given-names>J. J.</given-names></name></person-group> (<year>2001</year>). <article-title>A method for making group inferences from functional MRI data using independent component analysis</article-title>. <source>Hum. Brain Mapp</source>. <volume>14</volume>, <fpage>140</fpage>–<lpage>151</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.1048</pub-id><pub-id pub-id-type="pmid">11559959</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Egolf</surname><given-names>E.</given-names></name><name><surname>Rachakonda</surname><given-names>S.</given-names></name></person-group> (<year>2004</year>). <source>Group ICA of fMRI Toobox</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://mialab.mrn.org/software/gift">http://mialab.mrn.org/software/gift</ext-link></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Silva</surname><given-names>R. F.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Rachakonda</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>Comparison of PCA approaches for very large group ICA</article-title>. <source>Neuroimage</source>
<volume>118</volume>, <fpage>662</fpage>–<lpage>666</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.047</pub-id><pub-id pub-id-type="pmid">26021216</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erhardt</surname><given-names>E. B.</given-names></name><name><surname>Rachakonda</surname><given-names>S.</given-names></name><name><surname>Bedrick</surname><given-names>E. J.</given-names></name><name><surname>Allen</surname><given-names>E. A.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Comparison of multi-subject ICA methods for analysis of fMRI data</article-title>. <source>Hum. Brain Mapp.</source>
<volume>32</volume>, <fpage>2075</fpage>–<lpage>2095</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.21170</pub-id><pub-id pub-id-type="pmid">21162045</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>D. A.</given-names></name><name><surname>Setsompop</surname><given-names>K.</given-names></name></person-group> (<year>2013</year>). <article-title>Ultra-fast MRI of the human brain with simultaneous multi-slice imaging</article-title>. <source>J. Magn. Reson.</source>
<volume>229</volume>, <fpage>90</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmr.2013.02.002</pub-id><pub-id pub-id-type="pmid">23473893</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>D. A.</given-names></name><name><surname>Moeller</surname><given-names>S.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Auerbach</surname><given-names>E.</given-names></name><name><surname>Ramanna</surname><given-names>S.</given-names></name><name><surname>Glasser</surname><given-names>M. F.</given-names></name><etal></etal></person-group>. (<year>2010</year>). <article-title>Multiplexed echo planar imaging for Sub-Second Whole Brain FMRI and fast diffusion imaging</article-title>. <source>PLoS ONE</source>
<volume>5</volume>:<fpage>e15710</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0015710</pub-id><pub-id pub-id-type="pmid">21187930</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freire</surname><given-names>L.</given-names></name><name><surname>Roche</surname><given-names>A.</given-names></name><name><surname>Mangin</surname><given-names>J. F.</given-names></name></person-group> (<year>2002</year>). <article-title>What is the best similarity measure for motion correction in fMRI time series?</article-title>
<source>IEEE Trans. Med. Imaging</source>
<volume>21</volume>, <fpage>470</fpage>–<lpage>484</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2002.1009383</pub-id><pub-id pub-id-type="pmid">12071618</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name><name><surname>Holmes</surname><given-names>A.</given-names></name><name><surname>Worsley</surname><given-names>K. J.</given-names></name><name><surname>Poline</surname><given-names>J. P.</given-names></name><name><surname>Frith</surname><given-names>C. D.</given-names></name><name><surname>Frackowiak</surname><given-names>R. S.</given-names></name></person-group> (<year>1995</year>). <article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title>. <source>Hum. Brain Mapp</source>. <volume>2</volume>, <fpage>189</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Funk</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). <source>Netflix Update: Try This at Home.</source> Available online at: <ext-link ext-link-type="uri" xlink:href="http://sifter.org/~simon/journal/20061211.html">http://sifter.org/~simon/journal/20061211.html</ext-link> (Accessed April 10, 2015).</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halko</surname><given-names>N.</given-names></name><name><surname>Martinsson</surname><given-names>P. G.</given-names></name><name><surname>Shkolnisky</surname><given-names>Y.</given-names></name><name><surname>Tygert</surname><given-names>M.</given-names></name></person-group> (<year>2011a</year>). <article-title>An algorithm for the principal component analysis of large data sets</article-title>. <source>SIAM J. Sci. Comput</source>. <volume>33</volume>, <fpage>2580</fpage>–<lpage>2594</lpage>. <pub-id pub-id-type="doi">10.1137/100804139</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halko</surname><given-names>N.</given-names></name><name><surname>Martinsson</surname><given-names>P. G.</given-names></name><name><surname>Tropp</surname><given-names>J. A.</given-names></name></person-group> (<year>2011b</year>). <article-title>Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions</article-title>. <source>SIAM Rev</source>. <volume>53</volume>, <fpage>217</fpage>–<lpage>288</lpage>. <pub-id pub-id-type="doi">10.1137/090771806</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himberg</surname><given-names>J.</given-names></name><name><surname>Hyvarinen</surname><given-names>A.</given-names></name><name><surname>Esposito</surname><given-names>F.</given-names></name></person-group> (<year>2004</year>). <article-title>Validating the independent components of neuroimaging time series via clustering and visualization</article-title>. <source>Neuroimage</source>
<volume>22</volume>, <fpage>1214</fpage>–<lpage>1222</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.027</pub-id><pub-id pub-id-type="pmid">15219593</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuczynski</surname><given-names>J.</given-names></name><name><surname>Wozniakowski</surname><given-names>H.</given-names></name></person-group> (<year>1992</year>). <article-title>Estimating the largest eigenvalue by the power and lanczos algorithms with a random start</article-title>. <source>SIAM J. Matrix Anal. Appl</source>. <volume>13</volume>, <fpage>1094</fpage>–<lpage>1122</lpage>. <pub-id pub-id-type="doi">10.1137/0613066</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lehoucq</surname><given-names>R.</given-names></name><name><surname>Sorensen</surname><given-names>D.</given-names></name><name><surname>Yang</surname><given-names>C.</given-names></name></person-group> (<year>1998</year>). <source>ARPACK Users' Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods.</source>
<publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>SIAM</publisher-name>.</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehoucq</surname><given-names>R. B.</given-names></name><name><surname>Sorensen</surname><given-names>D. C.</given-names></name></person-group> (<year>1996</year>). <article-title>Deflation techniques for an implicitly restarted Arnoldi iteration</article-title>. <source>SIAM J. Matrix Anal. Appl.</source>
<volume>17</volume>, <fpage>789</fpage>–<lpage>821</lpage>. <pub-id pub-id-type="doi">10.1137/S0895479895281484</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name></person-group> (<year>2004</year>). <article-title>On incremental and robust subspace learning</article-title>. <source>Pattern Recognit.</source>
<volume>37</volume>, <fpage>1509</fpage>–<lpage>1518</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2003.11.010</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.-O.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name></person-group> (<year>2007</year>). <article-title>Estimating the number of independent components for functional magnetic resonance imaging data</article-title>. <source>Hum. Brain Mapp.</source>
<volume>28</volume>, <fpage>1251</fpage>–<lpage>1266</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20359</pub-id><pub-id pub-id-type="pmid">17274023</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>S.</given-names></name><name><surname>Correa</surname><given-names>N. M.</given-names></name><name><surname>Li</surname><given-names>X. L.</given-names></name><name><surname>Eichele</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name></person-group> (<year>2011</year>). <article-title>Automatic identification of functional clusters in fMRI data using spatial dependence</article-title>. <source>IEEE Trans. Biomed. Eng.</source>
<volume>58</volume>, <fpage>3406</fpage>–<lpage>3417</lpage>. <pub-id pub-id-type="doi">10.1109/TBME.2011.2167149</pub-id><pub-id pub-id-type="pmid">21900068</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Martinsson</surname><given-names>P.-G.</given-names></name><name><surname>Szlam</surname><given-names>A.</given-names></name><name><surname>Tygert</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Normalized power iterations for the computation of SVD</article-title>, in <source>Neural Information Processing Systems Workshop 2010</source>. (<publisher-loc>Whistler, BC</publisher-loc>).</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mitliagkas</surname><given-names>I.</given-names></name><name><surname>Caramanis</surname><given-names>C.</given-names></name><name><surname>Jain</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>Memory limited, streaming PCA</article-title>, in <source>Advances in Neural Information Processing Systems 26</source>, eds <person-group person-group-type="editor"><name><surname>Burges</surname><given-names>C. J. C.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name><name><surname>Ghahramani</surname><given-names>Z.</given-names></name><name><surname>Weinberger</surname><given-names>K. Q.</given-names></name></person-group> (<publisher-name>Curran Associates, Inc</publisher-name>), <fpage>2886</fpage>–<lpage>2894</lpage>.</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Recktenwald</surname><given-names>W.</given-names></name></person-group> (<year>2000</year>). <source>Introduction to Numerical Methods and MATLAB: Implementations and Applications</source>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roweis</surname><given-names>S.</given-names></name></person-group> (<year>1997</year>). <article-title>EM Algorithms for PCA and SPCA</article-title>, in <source>Neural Information Processing Systems (NIPS 1997)</source>, eds <person-group person-group-type="editor"><name><surname>Jordan</surname><given-names>M.</given-names></name><name><surname>Kearns</surname><given-names>M.</given-names></name><name><surname>Solla</surname><given-names>S.</given-names></name></person-group> (<publisher-loc>Denver, CO</publisher-loc>: <publisher-name>The MIT Press</publisher-name>).</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutishauser</surname><given-names>H.</given-names></name></person-group> (<year>1970</year>). <article-title>Simultaneous iteration method for symmetric matrices</article-title>. <source>Num. Math.</source>
<volume>16</volume>, <fpage>205</fpage>–<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1007/BF02219773</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Saad</surname><given-names>Y.</given-names></name></person-group> (<year>2011</year>). <source>Numerical Methods for Large Eigenvalue Problems: Revised Edition.</source>
<publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>SIAM</publisher-name>.</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Setsompop</surname><given-names>K.</given-names></name><name><surname>Gagoski</surname><given-names>B. A.</given-names></name><name><surname>Polimeni</surname><given-names>J. R.</given-names></name><name><surname>Witzel</surname><given-names>T.</given-names></name><name><surname>Wedeen</surname><given-names>V. J.</given-names></name><name><surname>Wald</surname><given-names>L. L.</given-names></name></person-group> (<year>2012</year>). <article-title>Blipped-controlled aliasing in parallel imaging for simultaneous multislice echo planar imaging with reduced g-factor penalty</article-title>. <source>Magn. Reson. Med.</source>
<volume>67</volume>, <fpage>1210</fpage>–<lpage>1224</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.23097</pub-id><pub-id pub-id-type="pmid">21858868</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Hyvärinen</surname><given-names>A.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Miller</surname><given-names>K. L.</given-names></name><name><surname>Beckmann</surname><given-names>C. F.</given-names></name></person-group> (<year>2014</year>). <article-title>Group-PCA for very large fMRI datasets</article-title>. <source>Neuroimage</source>
<volume>101</volume>, <fpage>738</fpage>–<lpage>749</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.051</pub-id><pub-id pub-id-type="pmid">25094018</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Miller</surname><given-names>K. L.</given-names></name><name><surname>Moeller</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>J. Q.</given-names></name><name><surname>Auerbach</surname><given-names>E. J.</given-names></name><name><surname>Woolrich</surname><given-names>M. W.</given-names></name><etal></etal></person-group>. (<year>2012</year>). <article-title>Temporally-independent functional modes of spontaneous brain activity</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>109</volume>, <fpage>3131</fpage>–<lpage>3136</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1121329109</pub-id><pub-id pub-id-type="pmid">22323591</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Rao</surname><given-names>H.</given-names></name><name><surname>Detre</surname><given-names>J. A.</given-names></name><name><surname>Childress</surname><given-names>A. R.</given-names></name></person-group> (<year>2006</year>). <article-title>Strategies for reducing large fMRI data sets for ICA</article-title>. <source>Magn. Reson. Imaging</source>
<volume>24</volume>, <fpage>591</fpage>–<lpage>596</lpage>. <pub-id pub-id-type="doi">10.1016/j.mri.2005.12.013</pub-id><pub-id pub-id-type="pmid">16735180</pub-id></mixed-citation></ref></ref-list></back></article>